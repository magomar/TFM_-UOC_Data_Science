Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Hu2014,
abstract = {Semantic matching is of central importance to many natural language tasks [2, 28]. A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them. As a step toward this goal, we propose convolutional neural network models for matching two sentences, by adapting the convolutional strategy in vision and speech. The proposed models not only nicely represent the hierarchical structures of sentences with their layer-by-layer composition and pooling, but also capture the rich matching patterns at different levels. Our models are rather generic, requiring no prior knowledge on language, and can hence be applied to matching tasks of different nature and in different languages. The empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models.},
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {1503.03244v1},
author = {Hu, Baotian and Lu, Zhengdong and Li, Hang and Chen, Qingcai},
booktitle = {Advances in Neural Information Processing Systems 27 (NIPS)},
eprint = {1503.03244v1},
file = {:home/mario/Documents/Mendeley/2014/Hu et al. - 2014 - Convolutional Neural Network Architectures for Matching Natural Language Sentences.pdf:pdf},
title = {{Convolutional Neural Network Architectures for Matching Natural Language Sentences}},
url = {http://www.noahlab.com.hk/technology/Learning2Match.html},
year = {2014}
}
@article{Patterson2014,
abstract = {In this paper we present the first large-scale scene attribute database. First, we perform crowdsourced human studies to find a taxonomy of 102 discriminative attributes. We discover attributes related to materials, surface properties, lighting, affordances, and spatial layout. Next, we build the 'SUN attribute database' on top of the diverse SUN categorical database. We use crowdsourcing to annotate attributes for 14,340 images from 707 scene categories. We perform numerous experiments to study the interplay between scene attributes and scene categories. We train and evaluate attribute classifiers and then study the feasibility of attributes as an intermediate scene representation for scene classification, zero shot learning, automatic image captioning, semantic image search, and parsing natural images. We show that when used as features for these tasks, low dimensional scene attributes can compete with or improve on the state of the art performance. The experiments suggest that scene attributes are an effective low-dimensional feature for capturing high-level context and semantics in scenes. [ABSTRACT FROM AUTHOR]},
author = {Patterson, Genevieve and Xu, Chen and Su, Hang and Hays, James},
doi = {10.1007/s11263-013-0695-z},
file = {:home/mario/Documents/Mendeley/2014/Patterson et al. - 2014 - The SUN Attribute Database Beyond Categories for Deeper Scene Understanding.pdf:pdf},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
keywords = {Attributes,Crowdsourcing,Image captioning,Scene parsing,Scene understanding},
month = {may},
number = {1-2},
pages = {59--81},
title = {{The SUN Attribute Database: Beyond Categories for Deeper Scene Understanding}},
url = {http://link.springer.com/10.1007/s11263-013-0695-z},
volume = {108},
year = {2014}
}
@inproceedings{Yan2015,
abstract = {Image-caption matching을 deep canonical correlation analysis(DCCA)에 기반하여 제안. DCCA를 통해 고차원 이미지와 텍스트 표현이 가능하며, non-trivial complexity 문제와 overfitting issue를 large dataset을 이용하여 해결함},
author = {Yan, Fei and Mikolajczyk, Krystian},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2015.7298966},
file = {:home/mario/Documents/Mendeley/2015/Yan, Mikolajczyk - 2015 - Deep correlation for matching images and text.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {10636919},
month = {jun},
pages = {3441--3450},
publisher = {IEEE},
title = {{Deep correlation for matching images and text}},
url = {http://ieeexplore.ieee.org/document/7298966/},
volume = {07-12-June},
year = {2015}
}
@article{Zhao2019,
abstract = {Deep convolution neural networks connected with the recurrent neural networks are potent models that have achieved excellent performance on image caption task. Although many methods based on the neural network can generate fluent and complete sentences, the image feature vectors extracted by the convolution neural network can only retain a few significant features of the original image, which will lose a lot of useful image information. Moreover, RNNs have a gradient vanishing problem with the growth of RNNs time step, and the generation of sentences will lack the guidance of previous information. In this paper, we introduce a multimodal fusion method for generating descriptions to explain the content of images. Our model consists of four sub-networks: a convolutional neural network for image feature extraction, a ATTssd model for image attributes extraction, a language CNN model CNNm for sentence modeling and a recurrent network (e.g., GRU, LSTM, etc.) for word prediction. Compared with existing methods which predict next word based on one previous word and hidden state, our model uses image attributes information to enhance the image representation and handles all the previous words to modeling the long-term dependencies of history words. The methods are evaluated on the Flickr8k, Flickr30k and MSCOCO datasets. We prove that our model combined with ATTssd and CNNm can significantly enhance the performance, and achieve the competitive results.},
author = {Zhao, Dexin and Chang, Zhi and Guo, Shutao},
doi = {10.1016/j.neucom.2018.11.004},
file = {:home/mario/Documents/Mendeley/2019/Zhao, Chang, Guo - 2019 - A multimodal fusion approach for image captioning.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Deep neural network,Image caption,Multimodal fusion},
pages = {476--485},
publisher = {Elsevier B.V.},
title = {{A multimodal fusion approach for image captioning}},
url = {https://doi.org/10.1016/j.neucom.2018.11.004},
volume = {329},
year = {2019}
}
@inproceedings{Kiros2014a,
abstract = {We introduce two multimodal neural language models: models of natural language that can be conditioned on other modalities. An image-text multimodal neural language model can be used to retrieve images given complex sentence queries, retrieve phrase descriptions given image queries, as well as generate text conditioned on images. We show that in the case of image-text modelling we can jointly learn word representa-tions and image features by training our models together with a convolutional network. Unlike many of the existing methods, our approach can generate sentence descriptions for images with-out the use of templates, structured prediction, and/or syntactic trees. While we focus on image-text modelling, our algorithms can be easily ap-plied to other modalities such as audio.},
address = {Beijing, China},
author = {Kiros, Ryan and Zemel, R and Salakhutdinov, Ruslan},
booktitle = {Proceedings of the 31st International Conference on Machine Learning (ICML 2014)},
file = {:home/mario/Documents/Mendeley/2014/Kiros, Zemel, Salakhutdinov - 2014 - Multimodal Neural Language Models.pdf:pdf},
isbn = {9781634393973},
keywords = {Image Tag Inference},
pages = {595--603},
publisher = {Proceedings of Machine Learning Research},
title = {{Multimodal Neural Language Models}},
url = {http://proceedings.mlr.press/v32/kiros14.pdf},
volume = {32},
year = {2014}
}
@article{Torralba2008,
author = {Torralba, A. and Fergus, R. and Freeman, W.T.},
doi = {10.1109/TPAMI.2008.128},
file = {:home/mario/Documents/Mendeley/2008/Torralba, Fergus, Freeman - 2008 - 80 Million Tiny Images A Large Data Set for Nonparametric Object and Scene Recognition.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = {nov},
number = {11},
pages = {1958--1970},
title = {{80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition}},
url = {http://ieeexplore.ieee.org/document/4531741/},
volume = {30},
year = {2008}
}
@inproceedings{Elliott2014,
abstract = {Image description is a new natural language generation task, where the aim is to generate a human-like description of an image. The evaluation of computer-generated text is a notoriously difficult problem, however , the quality of image descriptions has typically been measured using unigram BLEU and human judgements. The focus of this paper is to determine the correlation of automatic measures with human judgements for this task. We estimate the correlation of unigram and Smoothed BLEU, TER, ROUGE-SU4, and Meteor against human judgements on two data sets. The main finding is that unigram BLEU has a weak correlation, and Meteor has the strongest correlation with human judgements.},
address = {Baltimore, USA},
author = {Elliott, Desmond and Keller, Frank},
booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics},
file = {:home/mario/Documents/Mendeley/2014/Elliott, Keller - 2014 - Comparing Automatic Evaluation Measures for Image Description.pdf:pdf},
pages = {452--457},
publisher = {Association for Computational Linguistics},
title = {{Comparing Automatic Evaluation Measures for Image Description}},
url = {http://www.aclweb.org/anthology/P14-2074},
year = {2014}
}
@inproceedings{Gupta2012,
abstract = {In this paper, we address the problem of automatically generating human-like descriptions for unseen images, given a collection of images and their corresponding human-generated descriptions. Previous attempts for this task mostly rely on visual clues and corpus statistics, but do not take much advantage of the semantic information inherent in the available image descriptions. Here, we present a generic method which benefits from all these three sources (i.e. visual clues, corpus statistics and available descriptions) simultaneously, and is capable of constructing novel descriptions. Our approach works on syntactically and linguistically motivated phrases extracted from the human descriptions. Experimental evaluations demonstrate that our formulation mostly generates lucid and semantically correct descriptions, and significantly outperforms the previous methods on automatic evaluation metrics. One of the significant advantages of our approach is that we can generate multiple interesting descriptions for an image. Unlike any previous work, we also test the applicability of our method on a large dataset containing complex images with rich descriptions.},
address = {Toronto, Ontario, Canada},
author = {Gupta, Ankush and Verma, Yashaswi and Jawahar, C.V. V.},
booktitle = {Proceedings of the Twenty-Sixth (AAAI) Conference on Artificial Intelligence},
file = {:home/mario/Documents/Mendeley/2012/Gupta, Verma, Jawahar - 2012 - Choosing linguistics over vision to describe images.pdf:pdf},
isbn = {9781577355687},
keywords = {Knowledge-Based Information Systems (Main Track)},
pages = {606--612},
publisher = {AAAI Press},
title = {{Choosing linguistics over vision to describe images}},
url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/viewFile/5039/5245},
year = {2012}
}
@article{Tanti2018,
abstract = {When a recurrent neural network language model is used for caption generation, the image information can be fed to the neural network either by directly incorporating it in the RNN -- conditioning the language model by `injecting' image features -- or in a layer following the RNN -- conditioning the language model by `merging' image features. While both options are attested in the literature, there is as yet no systematic comparison between the two. In this paper we empirically show that it is not especially detrimental to performance whether one architecture is used or another. The merge architecture does have practical advantages, as conditioning by merging allows the RNN's hidden state vector to shrink in size by up to four times. Our results suggest that the visual and linguistic modalities for caption generation need not be jointly encoded by the RNN as that yields large, memory-intensive models with few tangible advantages in performance; rather, the multimodal integration should be delayed to a subsequent stage.},
archivePrefix = {arXiv},
arxivId = {1703.09137},
author = {Tanti, Marc and Gatt, Albert and Camilleri, Kenneth P.},
doi = {10.1017/S1351324918000098},
eprint = {1703.09137},
file = {:home/mario/Documents/Mendeley/2017/Tanti, Gatt, Camilleri - 2017 - Where to put the Image in an Image Caption Generator.pdf:pdf},
issn = {1351-3249},
journal = {Natural Language Engineering},
month = {mar},
number = {03},
pages = {467--489},
title = {{Where to put the Image in an Image Caption Generator}},
url = {http://arxiv.org/abs/1703.09137 http://dx.doi.org/10.1017/S1351324918000098 https://www.cambridge.org/core/product/identifier/S1351324918000098/type/journal{\_}article},
volume = {24},
year = {2017}
}
@article{Yang2019,
abstract = {Automatically generating a natural sentence describing the content of an image has been extensively researched in artificial intelligence recently, and it bridges the gap between computer vision and natural language processing communities. Most of existing captioning frameworks rely heavily on the visual content, while rarely being aware of the sentimental information. In this paper, we introduce the affective concepts to enhance the emotion expressibility of text descriptions. We achieve this goal by composing appropriate emotional concepts to sentences, which is calculated from large-scale visual and textual repositories by learning both content and linguistic modules. We extract visual and textual representations respectively, followed by combining the latent codes of the two components into a low-dimensional subspace. After that, we decode the combined latent representations and finally generate the affective image captions. We evaluate our method on the SentiCap dataset, which was established with sentimental adjective noun pairs, and evaluate the emotional descriptions with several qualitative and human inception metrics. The experimental results demonstrate the capability of our method for analyzing the latent emotion of an image and providing the affective description which caters to human cognition.},
author = {Yang, Jufeng and Sun, Yan and Liang, Jie and Ren, Bo and Lai, Shang Hong},
doi = {10.1016/j.neucom.2018.03.078},
file = {:home/mario/Documents/Mendeley/2019/Yang et al. - 2019 - Image captioning by incorporating affective concepts learned from both visual and textual components.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Affective concepts,Emotion recognition,Image captioning},
pages = {56--68},
publisher = {Elsevier B.V.},
title = {{Image captioning by incorporating affective concepts learned from both visual and textual components}},
url = {https://doi.org/10.1016/j.neucom.2018.03.078},
volume = {328},
year = {2019}
}
@article{Yuan2019,
abstract = {It is a big challenge of computer vision to make machine automatically describe the content of an image with a natural language sentence. Previous works have made great progress on this task, but they only use the global or local image feature, which may lose some important subtle or global information of an image. In this paper, we propose a model with 3-gated model which fuses the global and local image features together for the task of image caption generation. The model mainly has three gated structures. (1) Gate for the global image feature, which can adaptively decide when and how much the global image feature should be imported into the sentence generator. (2) The gated recurrent neural network (RNN) is used as the sentence generator. (3) The gated feedback method for stacking RNN is employed to increase the capability of nonlinearity fitting. More specially, the global and local image features are combined together in this paper, which makes full use of the image information. The global image feature is controlled by the first gate and the local image feature is selected by the attention mechanism. With the latter two gates, the relationship between image and text can be well explored, which improves the performance of the language part as well as the multi-modal embedding part. Experimental results show that our proposed method outperforms the state-of-the-art for image caption generation.},
author = {Yuan, Aihong and Li, Xuelong and Lu, Xiaoqiang},
doi = {10.1016/j.neucom.2018.10.059},
file = {:home/mario/Documents/Mendeley/2019/Yuan, Li, Lu - 2019 - 3G structure for image caption generation.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Convolutional neural network,Deep learning,Image caption generation,Multi-modal learning,Recurrent neural network},
pages = {17--28},
publisher = {Elsevier B.V.},
title = {{3G structure for image caption generation}},
url = {https://doi.org/10.1016/j.neucom.2018.10.059},
volume = {330},
year = {2019}
}
@inproceedings{Devlin2015,
abstract = {Two recent approaches have achieved state-of-the-art results in image captioning. The first uses a pipelined process where a set of candidate words is generated by a convolutional neural network (CNN) trained on images, and then a maximum entropy (ME) language model is used to arrange these words into a coherent sentence. The second uses the penultimate activation layer of the CNN as input to a recurrent neural network (RNN) that then generates the caption sequence. In this paper, we compare the merits of these different language modeling approaches for the first time by using the same state-of-the-art CNN as input. We examine issues in the different approaches, including linguistic irregularities, caption repetition, and data set overlap. By combining key aspects of the ME and RNN methods, we achieve a new record performance over previously published results on the benchmark COCO dataset. However, the gains we see in BLEU do not translate to human judgments.},
address = {Stroudsburg, PA, USA},
archivePrefix = {arXiv},
arxivId = {1505.01809},
author = {Devlin, Jacob and Cheng, Hao and Fang, Hao and Gupta, Saurabh and Deng, Li and He, Xiaodong and Zweig, Geoffrey and Mitchell, Margaret},
booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
doi = {10.3115/v1/P15-2017},
eprint = {1505.01809},
file = {:home/mario/Documents/Mendeley/2015/Devlin et al. - 2015 - Language Models for Image Captioning The Quirks and What Works.pdf:pdf},
pages = {100--105},
publisher = {Association for Computational Linguistics},
title = {{Language Models for Image Captioning: The Quirks and What Works}},
url = {http://arxiv.org/abs/1505.01809 http://aclweb.org/anthology/P15-2017},
year = {2015}
}
@inproceedings{Elliott2015,
abstract = {The Visual Dependency Representation (VDR) is an explicit model of the spa-tial relationships between objects in an im-age. In this paper we present an approach to training a VDR Parsing Model without the extensive human supervision used in previous work. Our approach is to find the objects mentioned in a given descrip-tion using a state-of-the-art object detec-tor, and to use successful detections to pro-duce training data. The description of an unseen image is produced by first predict-ing its VDR over automatically detected objects, and then generating the text with a template-based generation model using the predicted VDR. The performance of our approach is comparable to a state-of-the-art multimodal deep neural network in images depicting actions.},
address = {Stroudsburg, PA, USA},
author = {Elliott, Desmond and de Vries, Arjen},
booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
doi = {10.3115/v1/P15-1005},
file = {:home/mario/Documents/Mendeley/2015/Elliott, de Vries - 2015 - Describing Images using Inferred Visual Dependency Representations.pdf:pdf},
pages = {42--52},
publisher = {Association for Computational Linguistics},
title = {{Describing Images using Inferred Visual Dependency Representations}},
url = {http://aclweb.org/anthology/P15-1005},
year = {2015}
}
@inproceedings{Pu2016,
abstract = {A novel variational autoencoder is developed to model images, as well as associated labels or captions. The Deep Generative Deconvolutional Network (DGDN) is used as a decoder of the latent image features, and a deep Convolutional Neural Network (CNN) is used as an image encoder; the CNN is used to approximate a distribution for the latent DGDN features/code. The latent code is also linked to generative models for labels (Bayesian support vector machine) or captions (recurrent neural network). When predicting a label/caption for a new image at test, averaging is performed across the distribution of latent codes; this is computationally efficient as a consequence of the learned CNN-based encoder. Since the framework is capable of modeling the image in the presence/absence of associated labels/captions, a new semi-supervised setting is manifested for CNN learning with images; the framework even allows unsupervised CNN learning, based on images alone.},
address = {Barcelona, Spain},
archivePrefix = {arXiv},
arxivId = {arXiv:1609.08976v1},
author = {Pu, Yunchen and Gan, Zhe and Henao, Ricardo and Yuan, Xin and Li, Chunyuan and Stevens, Andrew and Carin, Lawrence},
booktitle = {NIPS'16 Proceedings of the 30th International Conference on Neural Information Processing Systems},
eprint = {arXiv:1609.08976v1},
file = {:home/mario/Documents/Mendeley/2016/Pu et al. - 2016 - Variational Autoencoder for Deep Learning of Images , Labels and Captions.pdf:pdf},
pages = {2360--2368},
title = {{Variational Autoencoder for Deep Learning of Images , Labels and Captions}},
year = {2016}
}
@inproceedings{Frome2013,
abstract = {Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources-such as text data-both to train visual models and to constrain their predictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recognition challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18{\%} across thousands of novel labels never seen by the visual model.},
author = {Frome, Andrea and Corrado, Greg S and Shlens, Jonathon and {Bengio Jeffrey Dean}, Samy and Ranzato, Aurelio and Mikolov, Tomas},
booktitle = {Advances in Neural Information Processing Systems 26 (NIPS 2013)},
file = {:home/mario/Documents/Mendeley/2013/Frome et al. - 2013 - DeViSE A Deep Visual-Semantic Embedding Model.pdf:pdf},
title = {{DeViSE: A Deep Visual-Semantic Embedding Model}},
url = {https://pdfs.semanticscholar.org/85ac/48c3cc267671fd948ec1ae4c55ccbdd17ed6.pdf},
year = {2013}
}
@inproceedings{Hendricks2016,
abstract = {While recent deep neural network models have achieved promising results on the image captioning task, they rely largely on the availability of corpora with paired im-age and sentence captions to describe objects in context. In this work, we propose the Deep Compositional Cap-tioner (DCC) to address the task of generating descriptions of novel objects which are not present in paired image-sentence datasets. Our method achieves this by leveraging large object recognition datasets and external text corpora and by transferring knowledge between semantically sim-ilar concepts. Current deep caption models can only de-scribe objects contained in paired image-sentence corpora, despite the fact that they are pre-trained with large object recognition datasets, namely ImageNet. In contrast, our model can compose sentences that describe novel objects and their interactions with other objects. We demonstrate our model's ability to describe novel concepts by empir-ically evaluating its performance on MSCOCO and show qualitative results on ImageNet images of objects for which no paired image-sentence data exist. Further, we extend our approach to generate descriptions of objects in video clips. Our results show that DCC has distinct advantages over existing image and video captioning approaches for generating descriptions of new objects in context.},
author = {Hendricks, Lisa Anne and Venugopalan, Subhashini and Rohrbach, Marcus and Mooney, Raymond and Saenko, Kate and Darrell, Trevor},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.8},
file = {:home/mario/Documents/Mendeley/2016/Hendricks et al. - 2016 - Deep Compositional Captioning Describing Novel Object Categories without Paired Training Data.pdf:pdf},
isbn = {978-1-4673-8851-1},
month = {jun},
pages = {1--10},
publisher = {IEEE},
title = {{Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data}},
url = {http://ieeexplore.ieee.org/document/7780377/},
year = {2016}
}
@article{Lecun1998,
author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
doi = {10.1109/5.726791},
file = {:home/mario/Documents/Mendeley/1998/Lecun et al. - 1998 - Gradient-based learning applied to document recognition.pdf:pdf},
issn = {00189219},
journal = {Proceedings of the IEEE},
number = {11},
pages = {2278--2324},
title = {{Gradient-based learning applied to document recognition}},
url = {http://ieeexplore.ieee.org/document/726791/{\#}full-text-section http://ieeexplore.ieee.org/document/726791/},
volume = {86},
year = {1998}
}
@inproceedings{Lebret2015a,
abstract = {Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutional Neural Network) and phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on caption syntax statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results in two popular datasets for the task: Flickr30k and the recently proposed Microsoft COCO.},
archivePrefix = {arXiv},
arxivId = {1502.03671},
author = {Lebret, R{\'{e}}mi and Pinheiro, Pedro O. and Collobert, Ronan},
booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
eprint = {1502.03671},
file = {:home/mario/Documents/Mendeley/2015/Lebret, Pinheiro, Collobert - 2015 - Phrase-based Image Captioning.pdf:pdf},
month = {feb},
title = {{Phrase-based Image Captioning}},
url = {http://arxiv.org/abs/1502.03671},
year = {2015}
}
@inproceedings{Yagcioglu2015,
abstract = {In this paper, we propose a novel query ex-pansion approach for improving transfer-based automatic image captioning. The core idea of our method is to translate the given visual query into a distributional se-mantics based form, which is generated by the average of the sentence vectors ex-tracted from the captions of images visu-ally similar to the input image. Using three image captioning benchmark datasets, we show that our approach provides more ac-curate results compared to the state-of-the-art data-driven methods in terms of both automatic metrics and subjective evalua-tion.},
address = {Stroudsburg, PA, USA},
author = {Yagcioglu, Semih and Erdem, Erkut and Erdem, Aykut and Cakici, Ruket},
booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics},
doi = {10.3115/v1/P15-2018},
file = {:home/mario/Documents/Mendeley/2015/Yagcioglu et al. - 2015 - A Distributed Representation Based Query Expansion Approach for Image Captioning.pdf:pdf},
pages = {106--111},
publisher = {Association for Computational Linguistics},
title = {{A Distributed Representation Based Query Expansion Approach for Image Captioning}},
url = {http://aclweb.org/anthology/P15-2018},
year = {2015}
}
@incollection{Farhadi2010,
abstract = {Humans can prepare concise descriptions of pictures, focus- ing on what they find important.We demonstrate that automatic meth- ods can do so too.We describe a system that can compute a score linking an image to a sentence. This score can be used to attach a descriptive sentence to a given image, or to obtain images that illustrate a given sentence. The score is obtained by comparing an estimate of meaning ob- tained from the image to one obtained from the sentence. Each estimate of meaning comes from a discriminative procedure that is learned us- ing data. We evaluate on a novel dataset consisting of human-annotated images. While our underlying estimate of meaning is impoverished, it is sufficient to produce very good quantitative results, evaluated with a novel score that can account for synecdoche.},
author = {Farhadi, Ali and Hejrati, Mohsen and Sadeghi, Mohammad Amin and Young, Peter and Rashtchian, Cyrus and Hockenmaier, Julia and Forsyth, David},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-15561-1_2},
file = {:home/mario/Documents/Mendeley/2010/Farhadi et al. - 2010 - Every Picture Tells a Story Generating Sentences from Images.pdf:pdf},
isbn = {364215560X},
issn = {03029743},
number = {PART 4},
pages = {15--29},
title = {{Every Picture Tells a Story: Generating Sentences from Images}},
url = {http://link.springer.com/10.1007/978-3-642-15561-1{\_}2},
volume = {6314 LNCS},
year = {2010}
}
@inproceedings{Soh2016,
abstract = {Automatic image caption generation brings together recent advances in natural language processing and computer vision. This work implements a generative CNN-LSTM model that beats human baselines by 2.7 BLEU-4 points and is close to matching (3.8 CIDEr points lower) the current state of the art. Experiments on the MSCOCO dataset set shows that it generates sensible and accurate captions in a majority of cases, and hyperparameter tuning using dropout and number of LSTM layers allow us to alleviate the effects of overfitting. We also demonstrate that semantically-close emitted words (e.g. 'plate' and 'bowl') move the LSTM hidden state in similar ways despite differing previous contexts, and that diver-gences in hidden state occur only upon emission of semantically-distant words (e.g. 'vase' and 'food'). This gives semantic meaning to the interaction between learned word embeddings and the LSTM hidden states. To our knowledge, this is a novel contribution to the literature.},
author = {Soh, Moses},
booktitle = {Thirtieth Conference on Neural Information Processing Systems (NIPS 2016)},
file = {:home/mario/Documents/Mendeley/2016/Soh - 2016 - Learning CNN-LSTM Architectures for Image Caption Generation.pdf:pdf},
title = {{Learning CNN-LSTM Architectures for Image Caption Generation}},
url = {https://cs224d.stanford.edu/reports/msoh.pdf},
year = {2016}
}
@inproceedings{Donahue2013,
abstract = {Models based on deep convolutional networks have dom- inated recent image interpretation tasks; we investigate whether models which are also recurrent, or “temporally deep”, are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image de- scription and retrieval problems, and video narration chal- lenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averag- ing for sequential processing, recurrent convolutional mod- els are “doubly deep” in that they can be compositional in spatial and temporal “layers”. Such models may have advantages when target concepts are complex and/or train- ing data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the net- work state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural lan- guage text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual rep- resentations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized. 1. Introduction Recognition and description of images and videos is a fundamental challenge of computer vision. Dramatic Visual},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.4389v3},
author = {Donahue, Jeff and Hendricks, Lisa Anne and Guadarrama, Sergio and Rohrbach, Marcus and Venugopalan, Subhashini and Darrell, Trevor and Saenko, Kate},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2015.7298878},
eprint = {arXiv:1411.4389v3},
file = {:home/mario/Documents/Mendeley/2015/Donahue et al. - 2015 - Long-term recurrent convolutional networks for visual recognition and description.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {9781467369640},
month = {jun},
number = {3},
pages = {2625--2634},
pmid = {23641138},
publisher = {IEEE},
title = {{Long-term recurrent convolutional networks for visual recognition and description}},
url = {http://ieeexplore.ieee.org/document/7298878/},
volume = {38},
year = {2015}
}
@inproceedings{Karpathy2015,
abstract = {Convolutional neural networks (CNNs) have been extensively applied for image recognition problems giving state-of-the-art results on recognition, detection, segmentation and retrieval. In this work we propose and evaluate several deep neural network architectures to combine image information across a video over longer time periods than previously attempted. We propose two methods capable of handling full length videos. The first method explores various convolutional temporal feature pooling architectures, examining the various design choices which need to be made when adapting a CNN for this task. The second proposed method explicitly models the video as an ordered sequence of frames. For this purpose we employ a recurrent neural network that uses Long Short-Term Memory (LSTM) cells which are connected to the output of the underlying CNN. Our best networks exhibit significant performance improvements over previously published results on the Sports 1 million dataset (73.1{\%} vs. 60.9{\%}) and the UCF-101 datasets with (88.6{\%} vs. 88.0{\%}) and without additional optical flow information (82.6{\%} vs. 72.8{\%}).},
author = {Karpathy, Andrej and Fei-Fei, Li},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2015.7298932},
file = {:home/mario/Documents/Mendeley/2015/Karpathy, Fei-Fei - 2015 - Deep visual-semantic alignments for generating image descriptions.pdf:pdf},
isbn = {978-1-4673-6964-0},
keywords = {Image captioning,deep neural networks,language model,recurrent neural network,visual-semantic embeddings},
month = {jun},
number = {4},
pages = {3128--3137},
publisher = {IEEE},
title = {{Deep visual-semantic alignments for generating image descriptions}},
url = {http://ieeexplore.ieee.org/document/7298932/},
volume = {39},
year = {2015}
}
@inproceedings{Jing2018,
address = {Melbourne, Australia},
author = {Jing, Baoyu and Xie, Pengtao and Xing, Eric P},
booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
doi = {10.18653/v1/P18-1240},
file = {:home/mario/Documents/Mendeley/2018/Jing, Xie, Xing - 2018 - On the Automatic Generation of Medical Imaging Reports.pdf:pdf},
pages = {2577--2586},
title = {{On the Automatic Generation of Medical Imaging Reports}},
year = {2018}
}
@inproceedings{Lin1998,
abstract = {Similarity is an important and widely used concept. Previous definitions of similarity are tied to a particular application or a form of knowledge representation. We present an information-theoretic definition of similarity that is applicable as long as there is a probabilistic model. We demonstrate how our definition can be used to measure the similarity in a number of different domains.},
address = {Madison, USA},
author = {Lin, Dekang},
booktitle = {Proceedings of the Fifteenth International Conference on Machine Learning (ICML 1998)},
file = {:home/mario/Documents/Mendeley/1998/Lin - 1998 - An Information-Theoretic Definition of Similarity.pdf:pdf},
isbn = {1-55860-556-8},
pages = {296--304},
title = {{An Information-Theoretic Definition of Similarity}},
year = {1998}
}
@inproceedings{Le2011,
abstract = {We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting with these learned features, we trained our network to obtain 15.8{\%} accuracy in recognizing 20,000 object categories from ImageNet, a leap of 70{\%} relative improvement over the previous state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1112.6209},
author = {Le, Quoc V. and Ranzato, Marc'Aurelio and Monga, Rajat and Devin, Matthieu and Chen, Kai and Corrado, Greg S. and Dean, Jeff and Ng, Andrew Y.},
booktitle = {Proceedings of the 29 th International Conference on Machine Learning (ICML)},
eprint = {1112.6209},
file = {:home/mario/Documents/Mendeley/2011/Le et al. - 2011 - Building high-level features using large scale unsupervised learning.pdf:pdf},
month = {dec},
title = {{Building high-level features using large scale unsupervised learning}},
url = {http://arxiv.org/abs/1112.6209},
year = {2011}
}
@article{Hodosh2013a,
abstract = {The ability to associate images with natural language sentences that describe what is depicted in them is a hallmark of image understanding, and a prerequisite for applications such as sentence-based image search. In analogy to image search, we propose to frame sentence-based image annotation as the task of ranking a given pool of captions. We introduce a new benchmark collection for sentence-based image description and search, consisting of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. We introduce a number of systems that perform quite well on this task, even though they are only based on features that can be obtained with minimal supervision. Our results clearly indicate the importance of training on multiple captions per image, and of capturing syntactic (word order-based) and semantic features of these captions. We also perform an in-depth comparison of human and automatic evaluation metrics for this task, and propose strategies for collecting human judgments cheaply and on a very large scale, allowing us to augment our collection with additional relevance judgments of which captions describe which image. Our analysis shows that metrics that consider the ranked list of results for each query image or sentence are significantly more robust than metrics that are based on a single response per query. Moreover, our study suggests that the evaluation of ranking-based image description systems may be fully automated.},
author = {Hodosh, Micah and Young, Peter and Hockenmaier, Julia},
doi = {10.1613/jair.3994},
file = {:home/mario/Documents/Mendeley/2013/Hodosh, Young, Hockenmaier - 2013 - Framing Image Description as a Ranking Task Data, Models and Evaluation Metrics.pdf:pdf},
isbn = {9781577357384},
issn = {1076-9757},
journal = {Journal of Artificial Intelligence Research},
month = {aug},
pages = {853--899},
title = {{Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics}},
url = {https://jair.org/index.php/jair/article/view/10833},
volume = {47},
year = {2013}
}
@inproceedings{Ma2016,
author = {Ma, Shubo and Han, Yahong},
booktitle = {2016 IEEE International Conference on Multimedia and Expo (ICME)},
doi = {10.1109/ICME.2016.7552883},
isbn = {978-1-4673-7258-9},
month = {jul},
pages = {1--6},
publisher = {IEEE},
title = {{Describing images by feeding LSTM with structural words}},
url = {http://ieeexplore.ieee.org/document/7552883/},
year = {2016}
}
@inproceedings{Lin,
abstract = {In this paper, we describe an image collection created for the CLEF cross-language image retrieval track (ImageCLEF). This image retrieval benchmark (referred to as the IAPR TC-12 Benchmark) has developed from an initiative started by the Technical Committee 12 (TC-12) of the International Association of Pattern Recognition (IAPR). The collection consists of 20,000 images from a private photographic image collection. The construction and composition of the IAPR TC-12 Benchmark is described, including its associated text captions which are expressed in multiple languages, making the collection well-suited for evaluating the effectiveness of both text-based and visual retrieval methods. We also discuss the current and expected uses of the collection, including its use to benchmark and compare different image retrieval systems in ImageCLEF 2006.},
author = {Grubinger, Michael and Clough, Paul and M{\"{u}}ller, Henning and Deselaers, Thomas},
booktitle = {International Conference on Language Resources and Evaluation},
file = {:home/mario/Documents/Mendeley/2006/Grubinger et al. - 2006 - The IAPR TC-12 Benchmark A New Evaluation Resource for Visual Information Systems.pdf:pdf},
keywords = {IAPTR-TC12},
mendeley-tags = {IAPTR-TC12},
title = {{The IAPR TC-12 Benchmark: A New Evaluation Resource for Visual Information Systems}},
url = {https://pdfs.semanticscholar.org/3819/29a8187010f6db940a23d78731c8e694c56c.pdf},
year = {2006}
}
@inproceedings{Zhou2017,
abstract = {Attention mechanisms have attracted considerable interest in image captioning due to its powerful performance. However, existing methods use only visual content as attention and whether textual context can improve attention in image captioning remains unsolved. To explore this problem, we propose a novel attention mechanism, called $\backslash$textit{\{}text-conditional attention{\}}, which allows the caption generator to focus on certain image features given previously generated text. To obtain text-related image features for our attention model, we adopt the guiding Long Short-Term Memory (gLSTM) captioning architecture with CNN fine-tuning. Our proposed method allows joint learning of the image embedding, text embedding, text-conditional attention and language model with one network architecture in an end-to-end manner. We perform extensive experiments on the MS-COCO dataset. The experimental results show that our method outperforms state-of-the-art captioning methods on various quantitative metrics as well as in human evaluation, which supports the use of our text-conditional attention in image captioning.},
address = {New York, New York, USA},
archivePrefix = {arXiv},
arxivId = {1606.04621},
author = {Zhou, Luowei and Xu, Chenliang and Koch, Parker and Corso, Jason J.},
booktitle = {Proceedings of the on Thematic Workshops of ACM Multimedia 2017 - Thematic Workshops '17},
doi = {10.1145/3126686.3126717},
eprint = {1606.04621},
file = {:home/mario/Documents/Mendeley/2017/Zhou et al. - 2017 - Watch What You Just Said Image Captioning with Text-Conditional Attention.pdf:pdf},
isbn = {9781450354165},
keywords = {-  Computing methodologies  -{\textgreater}  Natural language g,Computer vision representations,Neural networks,image captioning,lstm,multi-modal embedding,neural},
month = {jun},
pages = {305--313},
publisher = {ACM Press},
title = {{Watch What You Just Said: Image Captioning with Text-Conditional Attention}},
url = {http://arxiv.org/abs/1606.04621 http://dl.acm.org/citation.cfm?doid=3126686.3126717},
year = {2017}
}
@article{Young2014,
abstract = {We propose to use the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show to be at least as beneficial as distributional similarities for two tasks that require semantic inference. To compute these denotational similarities, we construct a denotation graph, i.e. a subsump- tion hierarchy over constituents and their de- notations, based on a large corpus of 30K im- ages and 150K descriptive captions.},
author = {Young, M. H. Peter and Lai, Alice and Hockenmaier, J.},
file = {:home/mario/Documents/Mendeley/2014/Young, Lai, Hockenmaier - 2014 - From image descriptions to visual denotations New similarity metrics for semantic inference over event.pdf:pdf},
isbn = {1-58113-905-5},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics},
pages = {67--78},
title = {{From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions}},
volume = {2},
year = {2014}
}
@article{Lowe2004,
author = {Lowe, David G.},
doi = {10.1023/B:VISI.0000029664.99615.94},
file = {:home/mario/Documents/Mendeley/2004/Lowe - 2004 - Distinctive Image Features from Scale-Invariant Keypoints.pdf:pdf},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
month = {nov},
number = {2},
pages = {91--110},
title = {{Distinctive Image Features from Scale-Invariant Keypoints}},
url = {http://link.springer.com/10.1023/B:VISI.0000029664.99615.94},
volume = {60},
year = {2004}
}
@inproceedings{DeMarneffe2006,
abstract = {This paper describes a system for extracting typed dependency parses of English sentences from phrase structure parses. In order to capture inherent relations occurring in corpus texts that can be critical in real-world applications, many NP relations are included in the set of grammatical relations used. We provide a comparison of our system with Minipar and the Link parser. The typed dependency extraction facility described here is integrated in the Stanford Parser, available for download.},
author = {{De Marneffe}, Marie-Catherine and MacCartney, Bill and Manning, Christopher D},
booktitle = {Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC)},
file = {:home/mario/Documents/Mendeley/2006/De Marneffe, MacCartney, Manning - 2006 - Generating Typed Dependency Parses from Phrase Structure Parses.pdf:pdf},
title = {{Generating Typed Dependency Parses from Phrase Structure Parses}},
url = {https://nlp.stanford.edu/{~}wcmac/papers/td-lrec06.pdf},
year = {2006}
}
@inproceedings{Johnson2016,
author = {Johnson, Justin and Karpathy, Andrej and Fei-Fei, Li},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.494},
file = {:home/mario/Documents/Mendeley/2016/Johnson, Karpathy, Fei-Fei - 2016 - DenseCap Fully Convolutional Localization Networks for Dense Captioning.pdf:pdf},
isbn = {978-1-4673-8851-1},
month = {jun},
pages = {4565--4574},
publisher = {IEEE},
title = {{DenseCap: Fully Convolutional Localization Networks for Dense Captioning}},
url = {http://ieeexplore.ieee.org/document/7780863/},
year = {2016}
}
@article{Hossain2019,
abstract = {Generating a description of an image is called image captioning. Image captioning requires to recognize the important objects, their attributes and their relationships in an image. It also needs to generate syntactically and semantically correct sentences. Deep learning-based techniques are capable of handling the complexities and challenges of image captioning. In this survey paper, we aim to present a comprehensive review of existing deep learning-based image captioning techniques. We discuss the foundation of the techniques to analyze their performances, strengths and limitations. We also discuss the datasets and the evaluation metrics popularly used in deep learning based automatic image captioning.},
archivePrefix = {arXiv},
arxivId = {1810.04020v2},
author = {Hossain, MD. Zakir and Sohel, Ferdous and Shiratuddin, Mohd Fairuz and Laga, Hamid},
doi = {10.1145/3295748},
eprint = {1810.04020v2},
file = {:home/mario/Documents/Mendeley/2019/Hossain et al. - 2019 - A Comprehensive Survey of Deep Learning for Image Captioning.pdf:pdf},
issn = {03600300},
journal = {ACM Computing Surveys},
keywords = {Additional Key Words and Phrases: Image Captioning,CCS Concepts: • Computing methodologies → Machine,CNN,Computer Vision,Deep Learning,LSTM,Natural Language Processing,Neural networks,Reinforcement learning,Supervised learning,Unsupervised learning},
month = {feb},
number = {6},
pages = {1--36},
title = {{A Comprehensive Survey of Deep Learning for Image Captioning}},
url = {http://dl.acm.org/citation.cfm?doid=3303862.3295748},
volume = {51},
year = {2019}
}
@inproceedings{Sharma2018,
abstract = {We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and filtering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image cap-tioning models and show that a model architecture based on Inception-ResNet-v2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.},
address = {Melbourne, Australia},
author = {Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics},
file = {:home/mario/Documents/Mendeley/2018/Sharma et al. - 2018 - Conceptual Captions A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning.pdf:pdf},
pages = {2556--2565},
publisher = {Association for Computational Linguistics},
title = {{Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning}},
url = {https://en.wikipedia.org/wiki/Alt},
year = {2018}
}
@inproceedings{Verma2015,
abstract = {{\textcopyright} 2014. The copyright of this document resides with its authors.Building bilateral semantic associations between images and texts is among the fundamental problems in computer vision. In this paper, we study two complementary cross-modal prediction tasks: (i) predicting text(s) given an image ("Im2Text"), and (ii) predicting image(s) given a piece of text ("Text2Im"). We make no assumption on the specific form of text; i.e., it could be either a set of labels, phrases, or even captions. We pose both these tasks in a retrieval framework. For Im2Text, given a query image, our goal is to retrieve a ranked list of semantically relevant texts from an independent textcorpus (i.e., texts with no corresponding images). Similarly, for Text2Im, given a query text, we aim to retrieve a ranked list of semantically relevant images from a collection of unannotated images (i.e., images without any associated textual meta-data). We propose a novel Structural SVM based unified formulation for these two tasks. For both visual and textual data, two types of representations are investigated. These are based on: (1) unimodal probability distributions over topics learned using latent Dirichlet allocation, and (2) explicitly learned multi-modal correlations using canonical correlation analysis. Extensive experiments on three popular datasets (two medium and one web-scale) demonstrate that our framework gives promising results compared to existing models under various settings, thus confirming its efficacy for both the tasks.},
author = {Verma, Yashaswi and Jawahar, C. V.},
booktitle = {Proceedings of the British Machine Vision Conference 2014},
doi = {10.5244/C.28.97},
file = {:home/mario/Documents/Mendeley/2014/Verma, Jawahar - 2014 - Im2Text and Text2Im Associating Images and Texts for Cross-Modal Retrieval.pdf:pdf},
isbn = {1-901725-52-9},
publisher = {British Machine Vision Association},
title = {{Im2Text and Text2Im: Associating Images and Texts for Cross-Modal Retrieval}},
url = {http://www.bmva.org/bmvc/2014/papers/paper089/index.html},
year = {2014}
}
@inproceedings{Fang2015,
abstract = {This paper presents a novel approach for automatically generating image descriptions: visual detectors, language models, and multimodal similarity models learnt directly from a dataset of image captions. We use multiple instance learning to train visual detectors for words that commonly occur in captions, including many different parts of speech such as nouns, verbs, and adjectives. The word detector outputs serve as conditional inputs to a maximum-entropy language model. The language model learns from a set of over 400,000 image descriptions to capture the statistics of word usage. We capture global semantics by re-ranking caption candidates using sentence-level features and a deep multimodal similarity model. Our system is state-of-the-art on the official Microsoft COCO benchmark, producing a BLEU-4 score of 29.1{\%}. When human judges compare the system captions to ones written by other people on our held-out test set, the system captions have equal or better quality 34{\%} of the time.},
archivePrefix = {arXiv},
arxivId = {1411.4952},
author = {Fang, Hao and Gupta, Saurabh and Iandola, Forrest and Srivastava, Rupesh K. and Deng, Li and Dollar, Piotr and Gao, Jianfeng and He, Xiaodong and Mitchell, Margaret and Platt, John C and Zitnick, C Lawrence and Zweig, Geoffrey},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2015.7298754},
eprint = {1411.4952},
file = {:home/mario/Documents/Mendeley/2015/Fang et al. - 2015 - From captions to visual concepts and back.pdf:pdf},
isbn = {978-1-4673-6964-0},
month = {jun},
pages = {1473--1482},
publisher = {IEEE},
title = {{From captions to visual concepts and back}},
url = {http://arxiv.org/abs/1411.4952 http://ieeexplore.ieee.org/document/7298754/},
year = {2015}
}
@inproceedings{Yang2016,
abstract = {We propose a novel extension of the encoder-decoder framework, called a review network. The review network is generic and can enhance any existing encoder- decoder model: in this paper, we consider RNN decoders with both CNN and RNN encoders. The review network performs a number of review steps with attention mechanism on the encoder hidden states, and outputs a thought vector after each review step; the thought vectors are used as the input of the attention mechanism in the decoder. We show that conventional encoder-decoders are a special case of our framework. Empirically, we show that our framework improves over state-of- the-art encoder-decoder systems on the tasks of image captioning and source code captioning.},
archivePrefix = {arXiv},
arxivId = {1605.07912},
author = {Yang, Zhilin and Yuan, Ye and Wu, Yuexin and Salakhutdinov, Ruslan and Cohen, William W.},
booktitle = {Thirtieth Conference on Neural Information Processing Systems (NIPS 2016)},
eprint = {1605.07912},
file = {:home/mario/Documents/Mendeley/2016/Yang et al. - 2016 - Review Networks for Caption Generation.pdf:pdf},
issn = {0034-6748},
month = {may},
number = {6},
pages = {2016--2023},
title = {{Review Networks for Caption Generation}},
url = {http://arxiv.org/abs/1605.07912},
volume = {75},
year = {2016}
}
@article{Fu2017,
abstract = {Recent progress on automatic generation of image captions has shown that it is possible to describe the most salient information conveyed by images with accurate and meaningful sentences. In this paper, we propose an image caption system that exploits the parallel structures between images and sentences. In our model, the process of generating the next word, given the previously generated ones, is aligned with the visual perception experience where the attention shifting among the visual regions imposes a thread of visual ordering. This alignment characterizes the flow of "abstract meaning", encoding what is semantically shared by both the visual scene and the text description. Our system also makes another novel modeling contribution by introducing scene-specific contexts that capture higher-level semantic information encoded in an image. The contexts adapt language models for word generation to specific scene types. We benchmark our system and contrast to published results on several popular datasets. We show that using either region-based attention or scene-specific contexts improves systems without those components. Furthermore, combining these two modeling ingredients attains the state-of-the-art performance.},
archivePrefix = {arXiv},
arxivId = {1506.06272},
author = {Fu, Kun and Jin, Junqi and Cui, Runpeng and Sha, Fei and Zhang, Changshui},
doi = {10.1109/TPAMI.2016.2642953},
eprint = {1506.06272},
file = {:home/mario/Documents/Mendeley/2017/Fu et al. - 2017 - Aligning Where to See and What to Tell Image Captioning with Region-Based Attention and Scene-Specific Contexts.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = {dec},
number = {12},
pages = {2321--2334},
title = {{Aligning Where to See and What to Tell: Image Captioning with Region-Based Attention and Scene-Specific Contexts}},
url = {http://arxiv.org/abs/1506.06272 http://ieeexplore.ieee.org/document/7792748/},
volume = {39},
year = {2017}
}
@inproceedings{Karpathy2014,
author = {Karpathy, Andrej and Joulin, Armand and Fei-Fei, Li},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS 2014)},
file = {:home/mario/Documents/Mendeley/2014/Karpathy, Joulin, Fei-Fei - 2014 - Deep Fragment Embeddings for Bidirectional Image Sentence Mapping.pdf:pdf},
pages = {1889--1897},
publisher = {MIT Press},
title = {{Deep Fragment Embeddings for Bidirectional Image Sentence Mapping}},
url = {https://dl.acm.org/citation.cfm?id=2969038 https://arxiv.org/abs/1406.5679},
year = {2014}
}
@inproceedings{Johnson2016,
abstract = {We introduce the dense captioning task, which requires a computer vision system to both localize and describe salient regions in images in natural language. The dense captioning task generalizes object detection when the descriptions consist of a single word, and Image Captioning when one predicted region covers the full image. To address the localization and description task jointly we propose a Fully Convolutional Localization Network (FCLN) architecture that processes an image with a single, efficient forward pass, requires no external regions proposals, and can be trained end-to-end with a single round of optimization. The architecture is composed of a Convolutional Network, a novel dense localization layer, and Recurrent Neural Network language model that generates the label sequences. We evaluate our network on the Visual Genome dataset, which comprises 94,000 images and 4,100,000 region-grounded captions. We observe both speed and accuracy improvements over baselines based on current state of the art approaches in both generation and retrieval settings.},
address = {Las Vegas, NV, USA},
author = {Johnson, Justin and Karpathy, Andrej and Fei-Fei, Li},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.494},
file = {:home/mario/Documents/Mendeley/2016/Johnson, Karpathy, Fei-Fei - 2016 - DenseCap Fully Convolutional Localization Networks for Dense Captioning.pdf:pdf},
isbn = {978-1-4673-8851-1},
month = {jun},
pages = {4565--4574},
publisher = {IEEE},
title = {{DenseCap: Fully Convolutional Localization Networks for Dense Captioning}},
url = {http://ieeexplore.ieee.org/document/7780863/},
year = {2016}
}
@inproceedings{Girshick2014,
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.81},
file = {:home/mario/Documents/Mendeley/2014/Girshick et al. - 2014 - Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.pdf:pdf},
isbn = {978-1-4799-5118-5},
month = {jun},
pages = {580--587},
publisher = {IEEE},
title = {{Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation}},
url = {http://ieeexplore.ieee.org/document/6909475/},
year = {2014}
}
@inproceedings{Lebret2015b,
abstract = {Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutional Neural Network) and phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on caption syntax statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results on the recently release Microsoft COCO dataset.},
archivePrefix = {arXiv},
arxivId = {1412.8419},
author = {Lebret, Remi and Pinheiro, Pedro O. and Collobert, Ronan},
booktitle = {Proceedings of the 2015 International Conference on Learning Representations},
eprint = {1412.8419},
file = {:home/mario/Documents/Mendeley/2015/Lebret, Pinheiro, Collobert - 2015 - Simple Image Description Generator via a Linear Phrase-Based Approach.pdf:pdf},
title = {{Simple Image Description Generator via a Linear Phrase-Based Approach}},
url = {http://arxiv.org/abs/1412.8419},
year = {2015}
}
@inproceedings{Wang2016,
author = {Wang, Minsi and Song, Li and Yang, Xiaokang and Luo, Chuanfei},
booktitle = {2016 IEEE International Conference on Image Processing (ICIP)},
doi = {10.1109/ICIP.2016.7533201},
file = {:home/mario/Documents/Mendeley/2016/Wang et al. - 2016 - A parallel-fusion RNN-LSTM architecture for image caption generation.pdf:pdf},
isbn = {978-1-4673-9961-6},
month = {sep},
pages = {4448--4452},
publisher = {IEEE},
title = {{A parallel-fusion RNN-LSTM architecture for image caption generation}},
url = {http://ieeexplore.ieee.org/document/7533201/},
year = {2016}
}
@inproceedings{Mitchell2012,
abstract = {This paper introduces a novel generation system that composes humanlike descriptions of images from computer vision detections. By leveraging syntactically informed word co-occurrence statistics, the generator ﬁlters and constrains the noisy detections output from a vision system to generate syntactic trees that detail what the computer vision system sees. Results show that the generation system outperforms state-of-the-art systems, automatically generating some of the most natural image descriptions to date.},
author = {Mitchell, Margaret and Dodge, Jesse and Goyal, Amit and Yamaguchi, Kota and Stratos, Karl and Han, Xufeng and Mensch, Alyssa and Berg, Alexander C. and Berg, Tamara L. and {Daume III}, Hal},
booktitle = {Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics},
file = {:home/mario/Documents/Mendeley/2012/Mitchell et al. - 2012 - Midge Generating Image Descriptions From Computer Vision Detections.pdf:pdf},
isbn = {978-1-937284-19-0},
pages = {747--756},
title = {{Midge: Generating Image Descriptions From Computer Vision Detections}},
year = {2012}
}
@inproceedings{Yatskar2015,
abstract = {This paper studies generation of descriptive sentences from densely annotated images. Previous work studied generation from automatically detected visual information but produced a limited class of sentences, hindered by currently unreliable recognition of activities and attributes. Instead, we collect human annotations of objects, parts, attributes and activities in images. These annotations allow us to build a significantly more comprehensive model of language generation and allow us to study what visual information is required to generate human-like descriptions. Experiments demonstrate high quality output and that activity annotations and relative spatial location of objects contribute most to producing high quality sentences.},
address = {Stroudsburg, PA, USA},
author = {Yatskar, Mark and Galley, Michel and Vanderwende, Lucy and Zettlemoyer, Luke},
booktitle = {Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014)},
doi = {10.3115/v1/S14-1015},
file = {:home/mario/Documents/Mendeley/2014/Yatskar et al. - 2014 - See No Evil, Say No Evil Description Generation from Densely Labeled Images.pdf:pdf},
pages = {110--120},
publisher = {Association for Computational Linguistics and Dublin City University},
title = {{See No Evil, Say No Evil: Description Generation from Densely Labeled Images}},
url = {http://aclweb.org/anthology/S14-1015},
year = {2014}
}
@inproceedings{Vinyals2015,
abstract = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1411.4555},
author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2015.7298935},
eprint = {1411.4555},
file = {:home/mario/Documents/Mendeley/2015/Vinyals et al. - 2015 - Show and tell A neural image caption generator.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {10636919},
month = {jun},
pages = {3156--3164},
pmid = {903},
publisher = {IEEE},
title = {{Show and tell: A neural image caption generator}},
url = {http://ieeexplore.ieee.org/document/7298935/},
volume = {07-12-June},
year = {2015}
}
@inproceedings{Lin2015,
abstract = {This paper proposes a novel framework for generating lingual descriptions of indoor scenes. Whereas substantial efforts have been made to tackle this problem, previous approaches focusing primarily on generating a single sentence for each image, which is not sufficient for describing complex scenes. We attempt to go beyond this, by generating coherent descriptions with multiple sentences. Our approach is distinguished from conventional ones in several aspects: (1) a 3D visual parsing system that jointly infers objects, attributes, and relations; (2) a generative grammar learned automatically from training text; and (3) a text generation algorithm that takes into account the coherence among sentences. Experiments on the augmented NYU-v2 dataset show that our framework can generate natural descriptions with substantially higher ROGUE scores compared to those produced by the baseline.},
archivePrefix = {arXiv},
arxivId = {1503.00064},
author = {Lin, Dahua and Fidler, Sanja and Kong, Chen and Urtasun, Raquel},
booktitle = {Procedings of the British Machine Vision Conference 2015},
doi = {10.5244/C.29.93},
eprint = {1503.00064},
file = {:home/mario/Documents/Mendeley/2015/Lin et al. - 2015 - Generating Multi-sentence Natural Language Descriptions of Indoor Scenes.pdf:pdf},
isbn = {1-901725-53-7},
publisher = {British Machine Vision Association},
title = {{Generating Multi-sentence Natural Language Descriptions of Indoor Scenes}},
url = {http://arxiv.org/abs/1503.00064 http://www.bmva.org/bmvc/2015/papers/paper093/index.html},
year = {2015}
}
@inproceedings{Li2011,
abstract = {Studying natural language, and especially how people describe the world around them can help us better understand the visual world. In turn, it can also help us in the quest to generate natural language that describes this world in a human manner. We present a simple yet effec- tive approach to automatically compose im- age descriptions given computer vision based inputs and using web-scale n-grams. Unlike most previous work that summarizes or re- trieves pre-existing text relevant to an image, our method composes sentences entirely from scratch. Experimental results indicate that it is viable to generate simple textual descriptions that are pertinent to the specific content of an image, while permitting creativity in the de- scription – making for more human-like anno- tations than previous approaches.},
author = {Li, Siming and Kulkarni, Girish and Berg, TL and Berg, AC and Choi, Yejin},
booktitle = {Proceedings of the Fifteenth Conference on Computational Natural Language Learning},
file = {:home/mario/Documents/Mendeley/2011/Li et al. - 2011 - Composing simple image descriptions using web-scale n-grams.pdf:pdf},
isbn = {978-1-932432-92-3},
number = {June},
pages = {220--228},
title = {{Composing simple image descriptions using web-scale n-grams}},
url = {http://dl.acm.org/citation.cfm?id=2018962},
year = {2011}
}
@article{Cao2019,
author = {Cao, Pengfei and Yang, Zhongyi and Sun, Liang and Liang, Yanchun and Yang, Mary Qu and Guan, Renchu},
doi = {10.1007/s11063-018-09973-5},
file = {:home/mario/Documents/Mendeley/2019/Cao et al. - 2019 - Image Captioning with Bidirectional Semantic Attention-Based Guiding of Long Short-Term Memory.pdf:pdf},
isbn = {1106301809973},
issn = {1573773X},
journal = {Neural Processing Letters},
keywords = {Bidirectional guiding LSTM,Convolution neural network,Image captioning,Semantic attention mechanism},
publisher = {Springer US},
title = {{Image Captioning with Bidirectional Semantic Attention-Based Guiding of Long Short-Term Memory}},
url = {https://doi.org/10.1007/s11063-018-09973-5},
year = {2019}
}
@incollection{Srivastava2018,
author = {Srivastava, Gargi and Srivastava, Rajeev},
booktitle = {Communications in Computer and Information Science},
doi = {10.1007/978-981-13-0023-3_8},
file = {:home/mario/Documents/Mendeley/2018/Srivastava, Srivastava - 2018 - A Survey on Automatic Image Captioning.pdf:pdf},
isbn = {9789811300226},
issn = {18650929},
keywords = {Computer vision,Image captioning,Scene analysis},
pages = {74--83},
title = {{A Survey on Automatic Image Captioning}},
url = {http://link.springer.com/10.1007/978-981-13-0023-3{\_}8},
volume = {834},
year = {2018}
}
@article{Kuznetsova2012,
abstract = {We present a holistic data-driven approach to image description generation, exploit- ing the vast amount of (noisy) parallel im- age data and associated natural language descriptions available on the web. More specifically, given a query image, we re- trieve existing human-composed phrases used to describe visually similar images, then selectively combine those phrases to generate a novel description for the query image. We cast the generation pro- cess as constraint optimization problems, collectively incorporating multiple inter- connected aspects of language composition for content planning, surface realization and discourse structure. Evaluation by hu- man annotators indicates that our final system generates more semantically cor- rect and linguistically appealing descrip- tions than two nontrivial baselines.},
author = {Kuznetsova, Polina and Ordonez, Vicente and Berg, Ac},
file = {:home/mario/Documents/Mendeley/2012/Kuznetsova, Ordonez, Berg - 2012 - Collective generation of natural image descriptions.pdf:pdf},
isbn = {9781937284244},
journal = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics},
number = {July},
pages = {359--368},
title = {{Collective generation of natural image descriptions}},
url = {http://dl.acm.org/citation.cfm?id=2390575},
volume = {1},
year = {2012}
}
@inproceedings{Yang2011,
abstract = {We propose a sentence generation strategy that describes images by predicting the most likely nouns, verbs, scenes and prepositions that make up the core sentence structure. The input are initial noisy estimates of the objects and scenes detected in the image using state of the art trained detectors. As predicting actions from still images directly is unreliable, we use a language model trained from the English Gi- gaword corpus to obtain their estimates; to- gether with probabilities of co-located nouns, scenes and prepositions. We use these esti- mates as parameters on a HMM that models the sentence generation process, with hidden nodes as sentence components and image de- tections as the emissions. Experimental re- sults show that our strategy of combining vi- sion and language produces readable and de- scriptive sentences compared to naive strate- gies that use vision alone.},
author = {Yang, Yezhou and Teo, C.L. and {Daum{\'{e}} III}, H. and Aloimonos, Y.},
booktitle = {The 2011 Conference on Empirical Methods in Natural Language Processing},
file = {:home/mario/Documents/Mendeley/2011/Yang et al. - 2011 - Corpus-Guided Sentence Generation of Natural Images.pdf:pdf},
isbn = {978-1-937284-11-4},
issn = {0023-7205},
pages = {444--454},
title = {{Corpus-Guided Sentence Generation of Natural Images}},
url = {http://hal3.name/docs/daume11generation.pdf},
year = {2011}
}
@inproceedings{Xu2015,
abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
address = {Lille, France},
archivePrefix = {arXiv},
arxivId = {1502.03044},
author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
doi = {10.1016/j.exger.2013.05.060},
eprint = {1502.03044},
file = {:home/mario/Documents/Mendeley/2015/Xu et al. - 2015 - Show, Attend and Tell Neural Image Caption Generation with Visual Attention.pdf:pdf},
isbn = {0531-5565},
issn = {05315565},
month = {feb},
pages = {2048--2057},
pmid = {23770107},
title = {{Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}},
url = {http://proceedings.mlr.press/v37/xuc15.pdf http://arxiv.org/abs/1502.03044 https://linkinghub.elsevier.com/retrieve/pii/S0531556513001940},
volume = {37},
year = {2015}
}
@incollection{Oliva2006,
author = {Oliva, Aude and Torralba, Antonio},
booktitle = {Progress of Brain Research},
doi = {10.1016/S0079-6123(06)55002-2},
file = {:home/mario/Documents/Mendeley/2006/Oliva, Torralba - 2006 - Building the gist of a scene the role of global image features in recognition.pdf:pdf},
pages = {23--36},
title = {{Building the gist of a scene: the role of global image features in recognition}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0079612306550022},
volume = {155},
year = {2006}
}
@inproceedings{Kulkarni2011,
abstract = {We posit that visually descriptive language offers com- puter vision researchers both information about the world, and information about how people describe the world. The potential benefit from this source is made more significant due to the enormous amount of language data easily avail- able today. We present a system to automatically gener- ate natural language descriptions from images that exploits both statistics gleaned from parsing large quantities of text data and recognition algorithms from computer vision. The system is very effective at producing relevant sentences for images. It also generates descriptions that are notably more true to the specific image content than previous work.},
author = {Kulkarni, Girish and Premraj, Visruth and Dhar, Sagnik and Li, Siming and Choi, Yejin and Berg, Alexander C and Berg, Tamara L},
booktitle = {CVPR 2011},
doi = {10.1109/CVPR.2011.5995466},
file = {:home/mario/Documents/Mendeley/2011/Kulkarni et al. - 2011 - Baby talk Understanding and generating simple image descriptions.pdf:pdf},
isbn = {978-1-4577-0394-2},
issn = {0018-019X},
month = {jun},
number = {1},
pages = {1601--1608},
pmid = {22848128},
publisher = {IEEE},
title = {{Baby talk: Understanding and generating simple image descriptions}},
url = {http://doi.wiley.com/10.1002/hlca.19350180143 http://ieeexplore.ieee.org/document/5995466/},
volume = {18},
year = {2011}
}
@inproceedings{Wu2015,
abstract = {Much of the recent progress in Vision-to-Language (V2L) problems has been achieved through a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). This approach does not explicitly represent high-level semantic concepts, but rather seeks to progress directly from image features to text. We propose here a method of incorporating high-level concepts into the very successful CNN-RNN approach, and show that it achieves a significant improvement on the state-of-the-art performance in both image captioning and visual question answering. We also show that the same mechanism can be used to introduce external semantic information and that doing so further improves performance. In doing so we provide an analysis of the value of high level semantic information in V2L problems.},
archivePrefix = {arXiv},
arxivId = {1506.01144},
author = {Wu, Qi and Shen, Chunhua and Liu, Lingqiao and Dick, Anthony and van den Hengel, Anton},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.29},
eprint = {1506.01144},
file = {:home/mario/Documents/Mendeley/2016/Wu et al. - 2016 - What Value Do Explicit High Level Concepts Have in Vision to Language Problems.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10636919},
month = {jun},
pages = {203--212},
publisher = {IEEE},
title = {{What Value Do Explicit High Level Concepts Have in Vision to Language Problems?}},
url = {http://arxiv.org/abs/1506.01144 http://ieeexplore.ieee.org/document/7780398/},
year = {2016}
}
@article{Bernardi2017,
abstract = {Automatic description generation from natural images is a challenging problem that has recently received a large amount of interest from the computer vision and natural language processing communities. In this survey, we classify the existing approaches based on how they conceptualize this problem, viz., models that cast description as either generation problem or as a retrieval problem over a visual or multimodal representational space. We provide a detailed review of existing models, highlighting their advantages and disadvantages. Moreover, we give an overview of the benchmark image datasets and the evaluation measures that have been developed to assess the quality of machine-generated image descriptions. Finally we extrapolate future directions in the area of automatic image description generation.},
archivePrefix = {arXiv},
arxivId = {1601.03896},
author = {Bernardi, Raffaella and Cakici, Ruket and Elliott, Desmond and Erdem, Aykut and Erdem, Erkut and Ikizler-Cinbis, Nazli and Keller, Frank and Muscat, Adrian and Plank, Barbara},
doi = {10.1613/jair.4900},
eprint = {1601.03896},
file = {:home/mario/Documents/Mendeley/2017/Bernardi et al. - 2017 - Automatic description generation from images A survey of models, datasets, and evaluation measures.pdf:pdf},
isbn = {9780999241103},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
pages = {4970--4974},
pmid = {11329188},
title = {{Automatic description generation from images: A survey of models, datasets, and evaluation measures}},
volume = {55},
year = {2017}
}
@inproceedings{Hodosh2013b,
abstract = {Associating photographs with complete sentences that describe what is depicted in them is a challenging problem. This paper examines how an approach that is inspired by image tagging techniques which can scale to very large data sets performs on this much harder task, and examines some of the linguistic difficulties that this bag-of-words model faces.},
author = {Hodosh, Micah and Hockenmaier, Julia},
booktitle = {2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2013.51},
file = {:home/mario/Documents/Mendeley/2013/Hodosh, Hockenmaier - 2013 - Sentence-Based Image Description with Scalable, Explicit Models.pdf:pdf},
isbn = {978-0-7695-4990-3},
month = {jun},
pages = {294--300},
publisher = {IEEE},
title = {{Sentence-Based Image Description with Scalable, Explicit Models}},
url = {http://openaccess.thecvf.com/content{\_}cvpr{\_}workshops{\_}2013/W06/papers/Hodosh{\_}Sentence-Based{\_}Image{\_}Description{\_}2013{\_}CVPR{\_}paper.pdf http://ieeexplore.ieee.org/document/6595890/},
year = {2013}
}
@inproceedings{Jia2015,
address = {Santiago, Chile},
author = {Jia, Xu and Gavves, Efstratios and Fernando, Basura and Tuytelaars, Tinne},
booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.277},
file = {:home/mario/Documents/Mendeley/2015/Jia et al. - 2015 - Guiding the Long-Short Term Memory Model for Image Caption Generation.pdf:pdf},
isbn = {978-1-4673-8391-2},
month = {dec},
pages = {2407--2415},
publisher = {IEEE},
title = {{Guiding the Long-Short Term Memory Model for Image Caption Generation}},
url = {http://ieeexplore.ieee.org/document/7410634/},
year = {2015}
}
@inproceedings{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-ferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make train-ing faster, we used non-saturating neurons and a very efficient GPU implemen-tation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called " dropout " that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
address = {Lake Tahoe, USA},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
booktitle = {Proceedings of the 25 th International Conference on Neural Information Processing Systems},
doi = {10.1145/3065386},
file = {:home/mario/Documents/Mendeley/2012/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet classification with deep convolutional neural networks.pdf:pdf},
issn = {00010782},
month = {may},
pages = {1097--1105},
title = {{ImageNet classification with deep convolutional neural networks}},
url = {http://ascelibrary.org/doi/10.1061/{\%}28ASCE{\%}29GT.1943-5606.0001284 http://dl.acm.org/citation.cfm?doid=3098997.3065386},
year = {2012}
}
@inproceedings{Oruganti2016,
author = {Oruganti, Ram Manohar and Sah, Shagan and Pillai, Suhas and Ptucha, Raymond},
booktitle = {2016 IEEE International Conference on Image Processing (ICIP)},
doi = {10.1109/ICIP.2016.7533033},
file = {:home/mario/Documents/Mendeley/2016/Oruganti et al. - 2016 - Image description through fusion based recurrent multi-modal learning.pdf:pdf},
isbn = {978-1-4673-9961-6},
month = {sep},
pages = {3613--3617},
publisher = {IEEE},
title = {{Image description through fusion based recurrent multi-modal learning}},
url = {http://ieeexplore.ieee.org/document/7533033/},
year = {2016}
}
@incollection{Lin2014,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
archivePrefix = {arXiv},
arxivId = {1405.0312},
author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C. Lawrence},
booktitle = {Lecture Notes in Computer Science (Proceedings of ECCV 2014)},
doi = {10.1007/978-3-319-10602-1_48},
eprint = {1405.0312},
file = {:home/mario/Documents/Mendeley/2014/Lin et al. - 2014 - Microsoft COCO Common Objects in Context.pdf:pdf},
month = {may},
pages = {740--755},
title = {{Microsoft COCO: Common Objects in Context}},
url = {https://arxiv.org/pdf/1405.0312.pdf http://link.springer.com/10.1007/978-3-319-10602-1{\_}48 http://arxiv.org/abs/1405.0312},
volume = {8693},
year = {2014}
}
@inproceedings{Bach2003,
author = {Bach, F.R. and Jordan, M.I.},
booktitle = {2003 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2003)},
doi = {10.1109/ICASSP.2003.1202783},
file = {:home/mario/Documents/Mendeley/2003/Bach, Jordan - 2003 - Kernel independent component analysis.pdf:pdf},
isbn = {0-7803-7663-3},
pages = {IV--876--9},
publisher = {IEEE},
title = {{Kernel independent component analysis}},
url = {http://ieeexplore.ieee.org/document/1202783/},
volume = {4},
year = {2003}
}
@article{Ordonez2011,
abstract = {We develop and demonstrate automatic image description methods using a large captioned photo collection. One contribution is our technique for the automatic collection of this new dataset -- performing a huge number of Flickr queries and then filtering the noisy results down to 1 million images with associated visually relevant captions. Such a collection allows us to approach the extremely challenging problem of description generation using relatively simple non-parametric methods and produces surprisingly effective results. We also develop methods incorporating many state of the art, but fairly noisy, estimates of image content to produce even more pleasing results. Finally we introduce a new objective performance measure for image captioning.},
author = {Ordonez, V and Kulkarni, G and Berg, Tl},
file = {:home/mario/Documents/Mendeley/2011/Ordonez, Kulkarni, Berg - 2011 - Im2text Describing Images Using 1 Million Captioned Photographs.pdf:pdf},
isbn = {9781618395993},
journal = {Advances in Neural Information Processing Systems (NIPS)},
pages = {1143--1151},
title = {{Im2text: Describing Images Using 1 Million Captioned Photographs}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2011{\_}0671.pdf},
year = {2011}
}
@article{Mao2015a,
archivePrefix = {arXiv},
arxivId = {https://arxiv.org/pdf/1410.1090},
author = {Mao, Junhua and Yuille, Alan},
eprint = {/arxiv.org/pdf/1410.1090},
file = {:home/mario/Documents/Mendeley/2015/Mao, Yuille - 2015 - Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN).pdf:pdf},
journal = {2015 International Conference on Learning Representations (ICLR 2015)},
number = {2014},
pages = {1--17},
primaryClass = {https:},
title = {{Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)}},
volume = {1090},
year = {2015}
}
@inproceedings{Ushiku2012,
address = {New York, New York, USA},
author = {Ushiku, Yoshitaka and Harada, Tatsuya and Kuniyoshi, Yasuo},
booktitle = {Proceedings of the 20th ACM international conference on Multimedia - MM '12},
doi = {10.1145/2393347.2393424},
file = {:home/mario/Documents/Mendeley/2012/Ushiku, Harada, Kuniyoshi - 2012 - Efficient image annotation for automatic sentence generation.pdf:pdf},
isbn = {9781450310895},
pages = {549},
publisher = {ACM Press},
title = {{Efficient image annotation for automatic sentence generation}},
url = {http://dl.acm.org/citation.cfm?doid=2393347.2393424},
year = {2012}
}
@incollection{Masotti2017,
abstract = {Recent advancements in Deep Learning show that the combination of Convolutional Neural Networks and Recurrent Neural Networks enables the definition of very effective methods for the automatic captioning of images. Unfortunately, this straightforward result requires the existence of large-scale corpora and they are not available for many languages. This paper describes a simple methodology to automatically acquire a large-scale corpus of 600 thousand image/sentences pairs in Italian. At the best of our knowledge, this corpus has been used to train one of the first neural systems for the same language. The experimental evaluation over a subset of validated image/captions pairs suggests that results comparable with the English counterpart can be achieved.},
author = {Masotti, Caterina and Croce, Danilo and Basili, Roberto},
booktitle = {Proceedings of the Fourth Italian Conference on Computational Linguistics CLiC-it 2017},
doi = {10.4000/books.aaccademia.2425},
file = {:home/mario/Documents/Mendeley/2017/Masotti, Croce, Basili - 2017 - Deep Learning for Automatic Image Captioning in poor Training Conditions.pdf:pdf},
issn = {16130073},
pages = {207--211},
publisher = {Accademia University Press},
title = {{Deep Learning for Automatic Image Captioning in poor Training Conditions}},
url = {http://books.openedition.org/aaccademia/2425},
volume = {2006},
year = {2017}
}
@misc{Hrga,
abstract = {As a problem that resides at the intersection of Computer Vision and Natural Language Processing, image captioning has witnessed a rapid progress in a very short time, from initial template-based models to the current ones, based on deep neural networks. This paper gives an overview of current issues and recent research on image captioning, with a special emphasis on models employing deep encoder-decoder architectures. We discuss the advantages and disadvantages of different approaches, along with reviewing some of the most commonly used datasets and evaluation metrics. We point out to some open questions and conclude with directions for future research.},
author = {Hrga, Ingrid},
file = {:home/mario/Documents/Mendeley/Unknown/Hrga - Unknown - Deep Image Captioning Models, Data and Evaluation.pdf:pdf},
keywords = {attention mechanism,deep,encoder-decoder framework,image captioning,neural networks},
title = {{Deep Image Captioning: Models, Data and Evaluation}},
url = {http://www.inf.uniri.hr/files/studiji/poslijediplomski/kvalifikacijski/Hrga{\_}Ingrid{\_}Kvalifikacijski{\_}rad.pdf}
}
@inproceedings{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
address = {San Diego, USA},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
booktitle = {Proceedings of the 3rd International Conference on Learning Representations (ICLR)},
eprint = {1409.1556},
file = {:home/mario/Documents/Mendeley/2015/Simonyan, Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:pdf},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2015}
}
@inproceedings{Kiros2014b,
abstract = {Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* - "blue" + "red" is near images of red cars. Sample captions generated for 800 images are made available for comparison.},
archivePrefix = {arXiv},
arxivId = {1411.2539},
author = {Kiros, Ryan and Salakhutdinov, Ruslan and Zemel, Richard S.},
booktitle = {Advances in Neural Information Processing Systems Deep Learning Workshop},
eprint = {1411.2539},
file = {:home/mario/Documents/Mendeley/2014/Kiros, Salakhutdinov, Zemel - 2014 - Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models.pdf:pdf},
title = {{Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models}},
url = {http://arxiv.org/abs/1411.2539},
year = {2014}
}
@article{Plummer2017,
abstract = {The Flickr30k dataset has become a standard benchmark for sentence-based image description. This paper presents Flickr30k Entities, which augments the 158k captions from Flickr30k with 244k coreference chains, linking mentions of the same entities across different captions for the same image, and associating them with 276k manually annotated bounding boxes. Such annotations are essential for continued progress in automatic image description and grounded language understanding. They enable us to define a new benchmark for localization of textual entity mentions in an image. We present a strong baseline for this task that combines an image-text embedding, detectors for common objects, a color classifier, and a bias towards selecting larger objects. While our baseline rivals in accuracy more complex state-of-the-art models, we show that its gains cannot be easily parlayed into improvements on such tasks as image-sentence retrieval, thus underlining the limitations of current methods and the need for further research.},
archivePrefix = {arXiv},
arxivId = {1505.04870},
author = {Plummer, Bryan A. and Wang, Liwei and Cervantes, Chris M. and Caicedo, Juan C. and Hockenmaier, Julia and Lazebnik, Svetlana},
doi = {10.1007/s11263-016-0965-7},
eprint = {1505.04870},
file = {:home/mario/Documents/Mendeley/2017/Plummer et al. - 2017 - Flickr30k Entities Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models.pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Computer vision,Crowdsourcing,Datasets,Language,Region phrase correspondence},
number = {1},
pages = {74--93},
title = {{Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models}},
volume = {123},
year = {2017}
}
@inproceedings{You2016,
abstract = {A one-pot “green” synthesis of water soluble and pH-responsive natural silk fibroin (SF)-stabilized fluorescent copper nanoclusters (CuNCs) was reported without using any additional reducing agents.},
author = {You, Quanzeng and Jin, Hailin and Wang, Zhaowen and Fang, Chen and Luo, Jiebo},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.503},
file = {:home/mario/Documents/Mendeley/2016/You et al. - 2016 - Image Captioning with Semantic Attention.pdf:pdf},
isbn = {978-1-4673-8851-1},
month = {jun},
number = {16},
pages = {4651--4659},
publisher = {IEEE},
title = {{Image Captioning with Semantic Attention}},
url = {http://ieeexplore.ieee.org/document/7780872/},
volume = {4},
year = {2016}
}
@inproceedings{Mason2015,
abstract = {We present a nonparametric density esti-mation technique for image caption gener-ation. Data-driven matching methods have shown to be effective for a variety of com-plex problems in Computer Vision. These methods reduce an inference problem for an unknown image to finding an exist-ing labeled image which is semantically similar. However, related approaches for image caption generation (Ordonez et al., 2011; Kuznetsova et al., 2012) are ham-pered by noisy estimations of visual con-tent and poor alignment between images and human-written captions. Our work addresses this challenge by estimating a word frequency representation of the vi-sual content of a query image. This al-lows us to cast caption generation as an extractive summarization problem. Our model strongly outperforms two state-of-the-art caption extraction systems accord-ing to human judgments of caption rele-vance.},
address = {Stroudsburg, PA, USA},
author = {Mason, Rebecca and Charniak, Eugene},
booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics},
doi = {10.3115/v1/P14-2097},
file = {:home/mario/Documents/Mendeley/2014/Mason, Charniak - 2014 - Nonparametric Method for Data-driven Image Captioning.pdf:pdf},
pages = {592--598},
publisher = {Association for Computational Linguistics},
title = {{Nonparametric Method for Data-driven Image Captioning}},
url = {http://aclweb.org/anthology/P14-2097},
year = {2014}
}
@inproceedings{Tanti2017,
abstract = {In neural image captioning systems, a recurrent neural network (RNN) is typically viewed as the primary `generation' component. This view suggests that the image features should be `injected' into the RNN. This is in fact the dominant view in the literature. Alternatively, the RNN can instead be viewed as only encoding the previously generated words. This view suggests that the RNN should only be used to encode linguistic features and that only the final representation should be `merged' with the image features at a later stage. This paper compares these two architectures. We find that, in general, late merging outperforms injection, suggesting that RNNs are better viewed as encoders, rather than generators.},
address = {Stroudsburg, PA, USA},
archivePrefix = {arXiv},
arxivId = {1708.02043},
author = {Tanti, Marc and Gatt, Albert and Camilleri, Kenneth},
booktitle = {Proceedings of the 10th International Conference on Natural Language Generation},
doi = {10.18653/v1/W17-3506},
eprint = {1708.02043},
file = {:home/mario/Documents/Mendeley/2017/Tanti, Gatt, Camilleri - 2017 - What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator.pdf:pdf},
month = {aug},
pages = {51--60},
publisher = {Association for Computational Linguistics},
title = {{What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator?}},
url = {http://arxiv.org/abs/1708.02043 http://aclweb.org/anthology/W17-3506},
year = {2017}
}
@inproceedings{Mao2014,
abstract = {In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel sentence descriptions to explain the content of images. It directly models the probability distribution of generating a word given previous words and the image. Image descriptions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on three benchmark datasets (IAPR TC-12, Flickr 8K, and Flickr 30K). Our model outperforms the state-of-the-art generative method. In addition, the m-RNN model can be applied to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval.},
archivePrefix = {arXiv},
arxivId = {1410.1090},
author = {Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Yuille, Alan L.},
booktitle = {NIPS 2014 Deep Learning Workshop},
eprint = {1410.1090},
file = {:home/mario/Documents/Mendeley/2014/Mao et al. - 2014 - Explain Images with Multimodal Recurrent Neural Networks.pdf:pdf},
month = {oct},
title = {{Explain Images with Multimodal Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1410.1090},
year = {2014}
}
@inproceedings{Cui2018,
abstract = {Evaluation metrics for image captioning face two challenges. Firstly, commonly used metrics such as CIDEr, METEOR, ROUGE and BLEU often do not correlate well with human judgments. Secondly, each metric has well known blind spots to pathological caption constructions, and rule-based metrics lack provisions to repair such blind spots once identified. For example, the newly proposed SPICE correlates well with human judgments, but fails to capture the syntactic structure of a sentence. To address these two challenges, we propose a novel learning based discriminative evaluation metric that is directly trained to distinguish between human and machine-generated captions. In addition, we further propose a data augmentation scheme to explicitly incorporate pathological transformations as negative examples during training. The proposed metric is evaluated with three kinds of robustness tests and its correlation with human judgments. Extensive experiments show that the proposed data augmentation scheme not only makes our metric more robust toward several pathological transformations, but also improves its correlation with human judgments. Our metric outperforms other metrics on both caption level human correlation in Flickr 8k and system level human correlation in COCO. The proposed approach could be served as a learning based evaluation metric that is complementary to existing rule-based metrics.},
archivePrefix = {arXiv},
arxivId = {1806.06422},
author = {Cui, Yin and Yang, Guandao and Veit, Andreas and Huang, Xun and Belongie, Serge},
booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00608},
eprint = {1806.06422},
file = {:home/mario/Documents/Mendeley/2018/Cui et al. - 2018 - Learning to Evaluate Image Captioning.pdf:pdf},
isbn = {978-1-5386-6420-9},
issn = {0265-203X},
month = {jun},
pages = {5804--5812},
publisher = {IEEE},
title = {{Learning to Evaluate Image Captioning}},
url = {http://arxiv.org/abs/1806.06422 https://ieeexplore.ieee.org/document/8578706/},
year = {2018}
}
@article{Bai2018,
abstract = {Image captioning means automatically generating a caption for an image. As a recently emerged research area, it is attracting more and more attention. To achieve the goal of image captioning, semantic information of images needs to be captured and expressed in natural languages. Connecting both research communities of computer vision and natural language processing, image captioning is a quite challenging task. Various approaches have been proposed to solve this problem. In this paper, we present a survey on advances in image captioning research. Based on the technique adopted, we classify image captioning approaches into different categories. Representative methods in each category are summarized, and their strengths and limitations are talked about. In this paper, we first discuss methods used in early work which are mainly retrieval and template based. Then, we focus our main attention on neural network based methods, which give state of the art results. Neural network based methods are further divided into subcategories based on the specific framework they use. Each subcategory of neural network based methods are discussed in detail. After that, state of the art methods are compared on benchmark datasets. Following that, discussions on future research directions are presented.},
author = {Bai, Shuang and An, Shan},
doi = {10.1016/j.neucom.2018.05.080},
file = {:home/mario/Documents/Mendeley/2018/Bai, An - 2018 - A survey on automatic image caption generation.pdf:pdf},
isbn = {9789811300226},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Attention mechanism,Deep neural networks,Encoder–decoder framework,Image captioning,Multimodal embedding,Sentence template},
pages = {291--304},
publisher = {Elsevier B.V.},
title = {{A survey on automatic image caption generation}},
volume = {311},
year = {2018}
}
@article{Zheng2017,
abstract = {Matching images and sentences demands a fine understanding of both modalities. In this paper, we propose a new system to discriminatively embed the image and text to a shared visual-textual space. In this field, most existing works apply the ranking loss to pull the positive image / text pairs close and push the negative pairs apart from each other. However, directly deploying the ranking loss is hard for network learning, since it starts from the two heterogeneous features to build inter-modal relationship. To address this problem, we propose the instance loss which explicitly considers the intra-modal data distribution. It is based on an unsupervised assumption that each image / text group can be viewed as a class. So the network can learn the fine granularity from every image/text group. The experiment shows that the instance loss offers better weight initialization for the ranking loss, so that more discriminative embeddings can be learned. Besides, existing works usually apply the off-the-shelf features, i.e., word2vec and fixed visual feature. So in a minor contribution, this paper constructs an end-to-end dual-path convolutional network to learn the image and text representations. End-to-end learning allows the system to directly learn from the data and fully utilize the supervision. On two generic retrieval datasets (Flickr30k and MSCOCO), experiments demonstrate that our method yields competitive accuracy compared to state-of-the-art methods. Moreover, in language based person retrieval, we improve the state of the art by a large margin. The code has been made publicly available.},
archivePrefix = {arXiv},
arxivId = {1711.05535},
author = {Zheng, Zhedong and Zheng, Liang and Garrett, Michael and Yang, Yi and Shen, Yi-Dong},
eprint = {1711.05535},
file = {:home/mario/Documents/Mendeley/2017/Zheng et al. - 2017 - Dual-Path Convolutional Image-Text Embedding with Instance Loss.pdf:pdf},
month = {nov},
number = {8},
pages = {1--15},
title = {{Dual-Path Convolutional Image-Text Embedding with Instance Loss}},
url = {http://arxiv.org/abs/1711.05535},
volume = {14},
year = {2017}
}
@inproceedings{Ma2015,
abstract = {In this paper, we propose multimodal convolutional neu-ral networks (m-CNNs) for matching image and sentence. Our m-CNN provides an end-to-end framework with convo-lutional architectures to exploit image representation, word composition, and the matching relations between the two modalities. More specifically, it consists of one image CNN encoding the image content, and one matching CNN learn-ing the joint representation of image and sentence. The matching CNN composes words to different semantic frag-ments and learns the inter-modal relations between image and the composed fragments at different levels, thus fully exploit the matching relations between image and sentence. Experimental results on benchmark databases of bidirec-tional image and sentence retrieval demonstrate that the proposed m-CNNs can effectively capture the information necessary for image and sentence matching. Specifically, our proposed m-CNNs for bidirectional image and sentence retrieval on Flickr8K and Flickr30K databases significantly outperform the state-of-the-art approaches.},
author = {Ma, Lin and Lu, Zhengdong and Shang, Lifeng and Li, Hang},
booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.301},
file = {:home/mario/Documents/Mendeley/2015/Ma et al. - 2015 - Multimodal Convolutional Neural Networks for Matching Image and Sentence.pdf:pdf},
isbn = {978-1-4673-8391-2},
keywords = {Administrative health data,Lung cancer,Palliative care,Survival time},
month = {dec},
number = {1},
pages = {2623--2631},
publisher = {IEEE},
title = {{Multimodal Convolutional Neural Networks for Matching Image and Sentence}},
url = {http://ieeexplore.ieee.org/document/7410658/},
volume = {17},
year = {2015}
}
@article{Kuznetsova2014,
abstract = {We present a new tree based approach to composing expressive image descriptions that makes use of naturally occuring web images with captions. We investigate two related tasks: image caption generalization and generation , where the former is an optional sub-task of the latter. The high-level idea of our approach is to harvest expressive phrases (as tree fragments) from existing image descriptions, then to compose a new description by selectively combining the extracted (and optionally pruned) tree fragments. Key algorithmic components are tree composition and compression , both integrating tree structure with sequence structure. Our proposed system attains significantly better performance than previous approaches for both image caption generalization and generation . In addition, our work is the first to show the empirical benefit of automatically generalized captions for composing natural image descriptions.},
author = {Kuznetsova, Polina and Ordonez, Vicente and Berg, Tamara L. and Choi, Yejin},
doi = {10.1162/tacl_a_00188},
file = {:home/mario/Documents/Mendeley/2014/Kuznetsova et al. - 2014 - TreeTalk Composition and Compression of Trees for Image Descriptions.pdf:pdf},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics},
month = {dec},
pages = {351--362},
title = {{TreeTalk: Composition and Compression of Trees for Image Descriptions}},
url = {https://www.mitpressjournals.org/doi/abs/10.1162/tacl{\_}a{\_}00188},
volume = {2},
year = {2014}
}
@inproceedings{Mao2015b,
abstract = {In this paper, we address the task of learning novel visual concepts, and their interactions with other concepts, from a few images with sentence descriptions. Using linguistic context and visual features, our method is able to efficiently hypothesize the semantic meaning of new words and add them to its word dictionary so that they can be used to describe images which contain these novel concepts. Our method has an image captioning module based on m-RNN with several improvements. In particular, we propose a transposed weight sharing scheme, which not only improves performance on image captioning, but also makes the model more suitable for the novel concept learning task. We propose methods to prevent overfitting the new concepts. In addition, three novel concept datasets are constructed for this new task. In the experiments, we show that our method effectively learns novel visual concepts from a few examples without disturbing the previously learned concepts. The project page is http://www.stat.ucla.edu/{\~{}}junhua.mao/projects/child{\_}learning.html},
archivePrefix = {arXiv},
arxivId = {arXiv:1504.06692v2},
author = {Mao, Junhua and Wei, Xu and Yang, Yi and Wang, Jiang and Huang, Zhiheng and Yuille, Alan L.},
booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.291},
eprint = {arXiv:1504.06692v2},
file = {:home/mario/Documents/Mendeley/2015/Mao et al. - 2015 - Learning Like a Child Fast Novel Visual Concept Learning from Sentence Descriptions of Images.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {15505499},
month = {dec},
pages = {2533--2541},
publisher = {IEEE},
title = {{Learning Like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images}},
url = {http://ieeexplore.ieee.org/document/7410648/},
year = {2015}
}
@inproceedings{Rashtchian2010,
abstract = {Crowd-sourcing approaches such as Ama-zon's Mechanical Turk (MTurk) make it possible to annotate or collect large amounts of linguistic data at a relatively low cost and high speed. However, MTurk offers only limited control over who is allowed to particpate in a particular task. This is particularly problematic for tasks requiring free-form text entry. Unlike multiple-choice tasks there is no correct answer, and therefore control items for which the correct answer is known cannot be used. Furthermore, MTurk has no effective built-in mechanism to guarantee workers are proficient English writers. We describe our experience in creating corpora of images annotated with multiple one-sentence descriptions on MTurk and explore the effectiveness of different quality control strategies for collecting linguistic data using Mechanical MTurk. We find that the use of a qualification test provides the highest improvement of quality, whereas refining the annotations through follow-up tasks works rather poorly. Using our best setup, we construct two image corpora, totaling more than 40,000 descriptive captions for 9000 images.},
address = {Los Angeles, USA},
author = {Rashtchian, Cyrus and Young, Peter and Hodosh, Micah and Hockenmaier, Julia},
booktitle = {Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk,},
file = {:home/mario/Documents/Mendeley/2010/Rashtchian et al. - 2010 - Collecting Image Annotations Using Amazon's Mechanical Turk.pdf:pdf},
keywords = {Flickr8K},
mendeley-tags = {Flickr8K},
pages = {139--147},
title = {{Collecting Image Annotations Using Amazon's Mechanical Turk}},
url = {https://pdfs.semanticscholar.org/bf60/322f83714523e2d7c1d39983151fe9db7146.pdf},
year = {2010}
}
@inproceedings{Chen2015,
abstract = {In this paper we explore the bi-directional mapping be-tween images and their sentence-based descriptions. We propose learning this mapping using a recurrent neural net-work. Unlike previous approaches that map both sentences and images to a common embedding, we enable the gener-ation of novel sentences given an image. Using the same model, we can also reconstruct the visual features associ-ated with an image given its visual description. We use a novel recurrent visual memory that automatically learns to remember long-term visual concepts to aid in both sentence generation and visual feature reconstruction. We evaluate our approach on several tasks. These include sentence gen-eration, sentence retrieval and image retrieval. State-of-the-art results are shown for the task of generating novel im-age descriptions. When compared to human generated cap-tions, our automatically generated captions are preferred by humans over 19.8{\%} of the time. Results are better than or comparable to state-of-the-art results on the image and sentence retrieval tasks for methods using similar visual features.},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.5654v1},
author = {Chen, Xinlei and Zitnick, C Lawrence},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2015.7298856},
eprint = {arXiv:1411.5654v1},
file = {:home/mario/Documents/Mendeley/2015/Chen, Zitnick - 2015 - Mind's eye A recurrent visual representation for image caption generation.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {10636919},
month = {jun},
pages = {2422--2431},
publisher = {IEEE},
title = {{Mind's eye: A recurrent visual representation for image caption generation}},
url = {http://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2015/html/Chen{\_}Minds{\_}Eye{\_}A{\_}2015{\_}CVPR{\_}paper.html http://ieeexplore.ieee.org/document/7298856/},
year = {2015}
}
@article{Socher2014,
abstract = {Previous work on Recursive Neural Networks (RNNs) shows that these models can produce compositional feature vectors for accurately representing and classifying sentences or images. However, the sentence vectors of previous models cannot accurately represent visually grounded meaning. We introduce the DT-RNN model which uses dependency trees to embed sentences into a vector space in order to retrieve images that are described by those sentences. Unlike previous RNN-based models which use constituency trees, DT-RNNs naturally focus on the action and agents in a sentence. They are better able to abstract from the details of word order and syntactic expression. DT-RNNs outperform other recursive and recurrent neural networks, kernelized CCA and a bag-of-words baseline on the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image.},
author = {Socher, Richard and Karpathy, Andrej and Le, Quoc V. and Manning, Christopher D. and Ng, Andrew Y.},
doi = {10.1162/tacl_a_00177},
file = {:home/mario/Documents/Mendeley/2014/Socher et al. - 2014 - Grounded Compositional Semantics for Finding and Describing Images with Sentences.pdf:pdf},
isbn = {2307-387X},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics},
month = {dec},
pages = {207--218},
title = {{Grounded Compositional Semantics for Finding and Describing Images with Sentences}},
url = {https://www.mitpressjournals.org/doi/abs/10.1162/tacl{\_}a{\_}00177},
volume = {2},
year = {2014}
}
@inproceedings{Tran2016,
abstract = {We present an image caption system that addresses new challenges of automatically describing images in the wild. The challenges include high quality caption quality with respect to human judgments, out-of-domain data handling, and low latency required in many applications. Built on top of a state-of-the-art framework, we developed a deep vision model that detects a broad range of visual concepts, an entity recognition model that identifies celebrities and landmarks, and a confidence model for the caption output. Experimental results show that our caption engine outperforms previous state-of-the-art systems significantly on both in-domain dataset (i.e. MS COCO) and out of-domain datasets.},
author = {Tran, Kenneth and He, Xiaodong and Zhang, Lei and Sun, Jian},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
doi = {10.1109/CVPRW.2016.61},
file = {:home/mario/Documents/Mendeley/2016/Tran et al. - 2016 - Rich Image Captioning in the Wild.pdf:pdf},
isbn = {978-1-5090-1437-8},
month = {jun},
pages = {434--441},
publisher = {IEEE},
title = {{Rich Image Captioning in the Wild}},
url = {http://ieeexplore.ieee.org/document/7789551/},
year = {2016}
}
@inproceedings{Ushiku2015,
author = {Ushiku, Yoshitaka and Yamaguchi, Masataka and Mukuta, Yusuke and Harada, Tatsuya},
booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.306},
file = {:home/mario/Documents/Mendeley/2015/Ushiku et al. - 2015 - Common Subspace for Model and Similarity Phrase Learning for Caption Generation from Images.pdf:pdf},
isbn = {978-1-4673-8391-2},
month = {dec},
pages = {2668--2676},
publisher = {IEEE},
title = {{Common Subspace for Model and Similarity: Phrase Learning for Caption Generation from Images}},
url = {http://ieeexplore.ieee.org/document/7410663/},
year = {2015}
}
@inproceedings{Elliott2013,
abstract = {Describing the main event of an image involves identifying the objects depicted and predicting the relationships between them. Previous approaches have represented images as unstructured bags of regions, which makes it difficult to accurately predict meaningful relationships between regions. In this paper , we introduce visual dependency representations to capture the relationships between the objects in an image, and hypothesize that this representation can improve image description. We test this hypothesis using a new data set of region-annotated images, associated with visual dependency representations and gold-standard descriptions. We describe two template-based description generation models that operate over visual dependency representations. In an image description task, we find that these models outper-form approaches that rely on object proximity or corpus information to generate descriptions on both automatic measures and on human judgements.},
address = {Seatle, USA},
author = {Elliott, Desmond and Keller, Frank},
booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013)},
file = {:home/mario/Documents/Mendeley/2013/Elliott, Keller - 2013 - Image Description using Visual Dependency Representations.pdf:pdf},
keywords = {VLT2K},
mendeley-tags = {VLT2K},
pages = {1292--1302},
publisher = {Association for Computational Linguistics},
title = {{Image Description using Visual Dependency Representations}},
url = {https://pdfs.semanticscholar.org/88e2/0b2a0d079002ae8d1baed6f1208b93dbfd73.pdf},
year = {2013}
}
@article{Tan2019,
author = {Tan, Ying Hua and Chan, Chee Seng},
doi = {10.1016/j.neucom.2018.12.026},
file = {:home/mario/Documents/Mendeley/2019/Tan, Chan - 2019 - Phrase-based image caption generator with hierarchical LSTM network.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
month = {mar},
pages = {86--100},
title = {{Phrase-based image caption generator with hierarchical LSTM network}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231218314802},
volume = {333},
year = {2019}
}
@article{He2019,
abstract = {Recently, attribute has demonstrated its effectiveness in guiding image captioning system. However, most attributes based image captioning methods treat the attributes prediction task as a separate task and rely on a standalone stage to obtain the attributes for the given image, e.g., a pre-trained network like Fully Convolutional Neural Network (FCN) is usually adopted. Inherently, they ignore the correlation between the attribute prediction task and image representation extraction task, and at the same time increases the complexity of the image captioning system. In this paper, we aim to couple the attributes prediction stage and image representation extraction stage tightly and propose a novel and efficient image captioning framework called Visual-Densely Semantic Attention Network(VD-SAN). In particular, the whole captioning system consists of shared convolutional layers from Dense Convolutional Network (DenseNet), which are further split into a semantic attributes prediction branch and an image feature extraction branch, two semantic attention models, and a long short-term memory networks (LSTM) for caption generation. To evaluate the proposed architecture, we construct Flickr30K-ATT and MS-COCO-ATT datasets based on the original popular image caption datasets Flickr30K and MS COCO respectively, and each image from Flickr30K-ATT or MS-COCO-ATT is annotated with an attribute list in addition to the corresponding caption. Empirical results demonstrate that our captioning system can achieve significant improvements over state-of-the-art approaches.},
author = {He, Xinwei and Yang, Yang and Shi, Baoguang and Bai, Xiang},
doi = {10.1016/j.neucom.2018.02.106},
file = {:home/mario/Documents/Mendeley/2019/He et al. - 2019 - VD-SAN Visual-Densely Semantic Attention Network for Image Caption Generation.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Convolutional neural network,Image caption,Long short-term memory networks,Semantic attributes},
pages = {48--55},
publisher = {Elsevier B.V.},
title = {{VD-SAN: Visual-Densely Semantic Attention Network for Image Caption Generation}},
url = {https://doi.org/10.1016/j.neucom.2018.02.106},
volume = {328},
year = {2019}
}
@inproceedings{GilbertoMateosOrtiz2015,
abstract = {Given a (static) scene, a human can effort- lessly describe what is going on (who is do- ing what to whom, how, and why). The pro- cess requires knowledge about the world, how it is perceived, and described. In this paper we study the problem of interpreting and verbal- izing visual information using abstract scenes created from collections of clip art images. We propose a model inspired by machine trans- lation operating over a large parallel corpus of visual relations and linguistic descriptions. We demonstrate that this approach produces human-like scene descriptions which are both fluent and relevant, outperforming a num- ber of competitive alternatives based on tem- plates, sentence-based retrieval, and a multi- modal neural language model. 1},
address = {Stroudsburg, PA, USA},
author = {Mateos-Ortiz, Luis Gilberto and Wolff, Clemens and Lapata, Mirella},
booktitle = {Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies},
doi = {10.3115/v1/N15-1174},
file = {:home/mario/Documents/Mendeley/2015/Mateos-Ortiz, Wolff, Lapata - 2015 - Learning to Interpret and Describe Abstract Scenes.pdf:pdf},
pages = {1505--1515},
publisher = {Association for Computational Linguistics},
title = {{Learning to Interpret and Describe Abstract Scenes}},
url = {http://aclweb.org/anthology/N15-1174},
year = {2015}
}
