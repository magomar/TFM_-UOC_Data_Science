Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@techreport{Lin2014,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
archivePrefix = {arXiv},
arxivId = {1405.0312},
author = {Lin, Tsung Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C. Lawrence},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-10602-1_48},
eprint = {1405.0312},
file = {:home/mario/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin et al. - Unknown - Microsoft COCO Common Objects in Context.pdf:pdf},
isbn = {978-3-319-10601-4},
issn = {16113349},
number = {PART 5},
pages = {740--755},
pmid = {16190471},
title = {{Microsoft COCO: Common objects in context}},
url = {https://arxiv.org/pdf/1405.0312.pdf},
volume = {8693 LNCS},
year = {2014}
}
@article{Sharma2018,
abstract = {We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and filtering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image cap-tioning models and show that a model architecture based on Inception-ResNet-v2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.},
author = {Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
file = {:home/mario/magomar@gmail.com/UOC/TFM/Bibliograf{\'{i}}a/P18-1238.pdf:pdf},
journal = {Annual Meeting of the Association for Computational Linguistics},
pages = {2556--2565},
title = {{Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning}},
url = {https://en.wikipedia.org/wiki/Alt},
year = {2018}
}
@article{Karpathy2017,
abstract = {We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations.},
archivePrefix = {arXiv},
arxivId = {1503.08909v2},
author = {Karpathy, Andrej and Fei-Fei, Li},
doi = {10.1109/TPAMI.2016.2598339},
eprint = {1503.08909v2},
file = {:home/mario/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Karpathy, Fei-Fei - 2017 - Deep Visual-Semantic Alignments for Generating Image Descriptions.pdf:pdf},
isbn = {9781467369640},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Image captioning,deep neural networks,language model,recurrent neural network,visual-semantic embeddings},
pmid = {16873662},
title = {{Deep Visual-Semantic Alignments for Generating Image Descriptions}},
year = {2017}
}
@inproceedings{Srivastava2018,
author = {Srivastava, Gargi and Srivastava, Rajeev},
booktitle = {Communications in Computer and Information Science},
doi = {10.1007/978-981-13-0023-3_8},
isbn = {9789811300226},
issn = {18650929},
keywords = {Computer vision,Image captioning,Scene analysis},
pages = {74--83},
title = {{A survey on automatic image captioning}},
volume = {834},
year = {2018}
}