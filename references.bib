Automatically generated by Mendeley Desktop 1.19.5
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@book{Zhang2019d2l,
author = {Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
title = {{Dive into Deep Learning (D2L Book)}},
url = {http://www.d2l.ai},
year = {2019}
}
@inproceedings{Frome2013,
abstract = {Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources-such as text data-both to train visual models and to constrain their predictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recognition challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18{\%} across thousands of novel labels never seen by the visual model.},
author = {Frome, Andrea and Corrado, Greg S and Shlens, Jonathon and {Bengio Jeffrey Dean}, Samy and Ranzato, Aurelio and Mikolov, Tomas},
booktitle = {Advances in Neural Information Processing Systems 26 (NIPS 2013)},
file = {:home/mario/Documents/Mendeley/2013/Frome et al. - 2013 - DeViSE A Deep Visual-Semantic Embedding Model.pdf:pdf},
title = {{DeViSE: A Deep Visual-Semantic Embedding Model}},
url = {https://pdfs.semanticscholar.org/85ac/48c3cc267671fd948ec1ae4c55ccbdd17ed6.pdf},
year = {2013}
}
@article{Young2014,
abstract = {We propose to use the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show to be at least as beneficial as distributional similarities for two tasks that require semantic inference. To compute these denotational similarities, we construct a denotation graph, i.e. a subsump- tion hierarchy over constituents and their de- notations, based on a large corpus of 30K im- ages and 150K descriptive captions.},
author = {Young, M. H. Peter and Lai, Alice and Hockenmaier, J.},
file = {:home/mario/Documents/Mendeley/2014/Young, Lai, Hockenmaier - 2014 - From image descriptions to visual denotations New similarity metrics for semantic inference over event.pdf:pdf},
isbn = {1-58113-905-5},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics},
pages = {67--78},
title = {{From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions}},
volume = {2},
year = {2014}
}
@inproceedings{Anderson2018_BUTD,
abstract = {Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge. determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge.},
address = {Salt Lake City, USA},
author = {Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00636},
file = {:home/mario/Documents/Mendeley/2018/Anderson et al. - 2018 - Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering.pdf:pdf},
isbn = {978-1-5386-6420-9},
month = {jun},
pages = {6077--6086},
publisher = {IEEE},
title = {{Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering}},
url = {https://ieeexplore.ieee.org/document/8578734/ http://openaccess.thecvf.com/content{\_}cvpr{\_}2018/html/Anderson{\_}Bottom-Up{\_}and{\_}Top-Down{\_}CVPR{\_}2018{\_}paper.html},
year = {2018}
}
@article{Scholl2001,
abstract = {What are the units of attention? In addition to standard models holding that attention can select spatial regions and visual features, recent work suggests that in some cases attention can directly select discrete objects. This paper reviews the state of the art with regard to such ‘object-based' attention, and explores how objects of attention relate to locations, reference frames, perceptual groups, surfaces, parts, and features. Also discussed are the dynamic aspects of objecthood, including the question of how attended objects are individuated in time, and the possibility of attending to simple dynamic motions and events. The final sections of this review generalize these issues beyond vision science, to other modalities and fields such as auditory objects of attention and the infant's ‘object concept'.},
author = {Scholl, Brian J},
doi = {10.1016/S0010-0277(00)00152-9},
issn = {0010-0277},
journal = {Cognition},
month = {jun},
number = {1-2},
pages = {1--46},
publisher = {Elsevier},
title = {{Objects and attention: the state of the art}},
url = {https://www.sciencedirect.com/science/article/pii/S0010027700001529},
volume = {80},
year = {2001}
}
@inproceedings{MateosOrtiz2015,
abstract = {Given a (static) scene, a human can effort- lessly describe what is going on (who is do- ing what to whom, how, and why). The pro- cess requires knowledge about the world, how it is perceived, and described. In this paper we study the problem of interpreting and verbal- izing visual information using abstract scenes created from collections of clip art images. We propose a model inspired by machine trans- lation operating over a large parallel corpus of visual relations and linguistic descriptions. We demonstrate that this approach produces human-like scene descriptions which are both fluent and relevant, outperforming a num- ber of competitive alternatives based on tem- plates, sentence-based retrieval, and a multi- modal neural language model. 1},
address = {Stroudsburg, PA, USA},
author = {Mateos-Ortiz, Luis Gilberto and Wolff, Clemens and Lapata, Mirella},
booktitle = {Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies},
doi = {10.3115/v1/N15-1174},
file = {:home/mario/Documents/Mendeley/2015/Mateos-Ortiz, Wolff, Lapata - 2015 - Learning to Interpret and Describe Abstract Scenes.pdf:pdf},
pages = {1505--1515},
publisher = {Association for Computational Linguistics},
title = {{Learning to Interpret and Describe Abstract Scenes}},
url = {http://aclweb.org/anthology/N15-1174},
year = {2015}
}
@inproceedings{Neishi2017,
abstract = {In this paper, we describe the team UT-IIS's system and results for the WAT 2017 translation tasks. We further investigated several tricks including a novel technique for initializing embedding layers using only the parallel corpus, which increased the BLEU score by 1.28, found a practical large batch size of 256, and gained insights regarding hyperparameter settings. Ultimately , our system obtained a better result than the state-of-the-art system of WAT 2016. Our code is available on https: //github.com/nem6ishi/wat17.},
address = {Taipei, Taiwan},
author = {Neishi, Masato and Sakuma, Jin and Tohda, Satoshi and Ishiwatari, Shonosuke and Yoshinaga, Naoki and Toyoda, Masashi},
booktitle = {Proceedings of the 4th Workshop on Asian Translation,},
file = {:home/mario/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Neishi et al. - 2017 - A Bag of Useful Tricks for Practical Neural Machine Translation Embedding Layer Initialization and Large Batch Si.pdf:pdf},
keywords = {Deep Learning,NLP,Translation},
mendeley-tags = {Deep Learning,NLP,Translation},
pages = {99--109},
publisher = {AFNLP},
title = {{A Bag of Useful Tricks for Practical Neural Machine Translation: Embedding Layer Initialization and Large Batch Size}},
url = {https://google.github.io/seq2seq/},
year = {2017}
}
@inproceedings{Hendricks2016,
abstract = {While recent deep neural network models have achieved promising results on the image captioning task, they rely largely on the availability of corpora with paired im-age and sentence captions to describe objects in context. In this work, we propose the Deep Compositional Cap-tioner (DCC) to address the task of generating descriptions of novel objects which are not present in paired image-sentence datasets. Our method achieves this by leveraging large object recognition datasets and external text corpora and by transferring knowledge between semantically sim-ilar concepts. Current deep caption models can only de-scribe objects contained in paired image-sentence corpora, despite the fact that they are pre-trained with large object recognition datasets, namely ImageNet. In contrast, our model can compose sentences that describe novel objects and their interactions with other objects. We demonstrate our model's ability to describe novel concepts by empir-ically evaluating its performance on MSCOCO and show qualitative results on ImageNet images of objects for which no paired image-sentence data exist. Further, we extend our approach to generate descriptions of objects in video clips. Our results show that DCC has distinct advantages over existing image and video captioning approaches for generating descriptions of new objects in context.},
author = {Hendricks, Lisa Anne and Venugopalan, Subhashini and Rohrbach, Marcus and Mooney, Raymond and Saenko, Kate and Darrell, Trevor},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.8},
file = {:home/mario/Documents/Mendeley/2016/Hendricks et al. - 2016 - Deep Compositional Captioning Describing Novel Object Categories without Paired Training Data.pdf:pdf},
isbn = {978-1-4673-8851-1},
month = {jun},
pages = {1--10},
publisher = {IEEE},
title = {{Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data}},
url = {http://ieeexplore.ieee.org/document/7780377/},
year = {2016}
}
@article{Cao2019,
author = {Cao, Pengfei and Yang, Zhongyi and Sun, Liang and Liang, Yanchun and Yang, Mary Qu and Guan, Renchu},
doi = {10.1007/s11063-018-09973-5},
file = {:home/mario/Documents/Mendeley/2019/Cao et al. - 2019 - Image Captioning with Bidirectional Semantic Attention-Based Guiding of Long Short-Term Memory.pdf:pdf},
isbn = {1106301809973},
issn = {1573773X},
journal = {Neural Processing Letters},
keywords = {Bidirectional guiding LSTM,Convolution neural network,Image captioning,Semantic attention mechanism},
publisher = {Springer US},
title = {{Image Captioning with Bidirectional Semantic Attention-Based Guiding of Long Short-Term Memory}},
url = {https://doi.org/10.1007/s11063-018-09973-5},
year = {2019}
}
@inproceedings{Wu2016,
abstract = {Much of the recent progress in Vision-to-Language (V2L) problems has been achieved through a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). This approach does not explicitly represent high-level semantic concepts, but rather seeks to progress directly from image features to text. We propose here a method of incorporating high-level concepts into the very successful CNN-RNN approach, and show that it achieves a significant improvement on the state-of-the-art performance in both image captioning and visual question answering. We also show that the same mechanism can be used to introduce external semantic information and that doing so further improves performance. In doing so we provide an analysis of the value of high level semantic information in V2L problems.},
archivePrefix = {arXiv},
arxivId = {1506.01144},
author = {Wu, Qi and Shen, Chunhua and Liu, Lingqiao and Dick, Anthony and van den Hengel, Anton},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.29},
eprint = {1506.01144},
file = {:home/mario/Documents/Mendeley/2016/Wu et al. - 2016 - What Value Do Explicit High Level Concepts Have in Vision to Language Problems.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10636919},
month = {jun},
pages = {203--212},
publisher = {IEEE},
title = {{What Value Do Explicit High Level Concepts Have in Vision to Language Problems?}},
url = {http://arxiv.org/abs/1506.01144 http://ieeexplore.ieee.org/document/7780398/},
year = {2016}
}
@article{Mao2015_mRNN,
abstract = {In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions. It directly models the probability distribution of generating a word given previous words and an image. Image captions are gen- erated according to this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K, Flickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In addition, we apply the m-RNN model to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which di- rectly optimize the ranking objective function for retrieval. The},
archivePrefix = {arXiv},
arxivId = {https://arxiv.org/pdf/1410.1090},
author = {Mao, Junhua and Yuille, Alan},
eprint = {/arxiv.org/pdf/1410.1090},
file = {:home/mario/Documents/Mendeley/2015/Mao, Yuille - 2015 - Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN).pdf:pdf},
journal = {2015 International Conference on Learning Representations (ICLR 2015)},
number = {2014},
pages = {1--17},
primaryClass = {https:},
title = {{Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)}},
volume = {1090},
year = {2015}
}
@inproceedings{Kalchbrenner2013,
abstract = {We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolu-tional Sentence Model. Through various experiments , we show first that our models obtain a perplexity with respect to gold translations that is {\textgreater} 43{\%} lower than that of state-of-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.},
address = {Seattle, USA},
author = {Kalchbrenner, Nal and Blunsom, Phil},
booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
file = {:home/mario/Documents/Mendeley/2013/Kalchbrenner, Blunsom - 2013 - Recurrent Continuous Translation Models.pdf:pdf},
pages = {1700--1709},
publisher = {Association for Computational Linguistics},
title = {{Recurrent Continuous Translation Models}},
url = {https://www.aclweb.org/anthology/D13-1176},
year = {2013}
}
@inproceedings{Karpathy2015,
abstract = {We present a model that generates natural language de- scriptions ofimages and their regions. Our approach lever- ages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between lan- guage and visual data. Our alignment model is based on a novel combination ofConvolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architec- ture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state ofthe art results in re- trieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions sig- nificantly outperform retrieval baselines on both full images and on a new dataset ofregion-level annotations.},
address = {Boston, USA},
author = {Karpathy, Andrej and Fei-Fei, Li},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2015.7298932},
file = {:home/mario/Documents/Mendeley/2015/Karpathy, Fei-Fei - 2015 - Deep visual-semantic alignments for generating image descriptions.pdf:pdf},
isbn = {978-1-4673-6964-0},
keywords = {Image captioning,deep neural networks,language model,recurrent neural network,visual-semantic embeddings},
month = {jun},
pages = {3128--3137},
publisher = {IEEE},
title = {{Deep visual-semantic alignments for generating image descriptions}},
url = {http://ieeexplore.ieee.org/document/7298932/},
volume = {39},
year = {2015}
}
@inproceedings{Vaswani2017,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
address = {Long Beach, USA},
author = {Vaswani, Ashish and Brain, Google and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
booktitle = {31st Conference on Neural Information Processing Systems (NIPS 2017)},
file = {:home/mario/Documents/Mendeley/2017/Vaswani et al. - 2017 - Attention Is All You Need.pdf:pdf},
title = {{Attention Is All You Need}},
url = {https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf},
year = {2017}
}
@inproceedings{Tran2016,
abstract = {We present an image caption system that addresses new challenges of automatically describing images in the wild. The challenges include high quality caption quality with respect to human judgments, out-of-domain data handling, and low latency required in many applications. Built on top of a state-of-the-art framework, we developed a deep vision model that detects a broad range of visual concepts, an entity recognition model that identifies celebrities and landmarks, and a confidence model for the caption output. Experimental results show that our caption engine outperforms previous state-of-the-art systems significantly on both in-domain dataset (i.e. MS COCO) and out of-domain datasets.},
author = {Tran, Kenneth and He, Xiaodong and Zhang, Lei and Sun, Jian},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
doi = {10.1109/CVPRW.2016.61},
file = {:home/mario/Documents/Mendeley/2016/Tran et al. - 2016 - Rich Image Captioning in the Wild.pdf:pdf},
isbn = {978-1-5090-1437-8},
month = {jun},
pages = {434--441},
publisher = {IEEE},
title = {{Rich Image Captioning in the Wild}},
url = {http://ieeexplore.ieee.org/document/7789551/},
year = {2016}
}
@inproceedings{Jang2017,
abstract = {Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax esti-mator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.},
address = {Toulon, France},
archivePrefix = {arXiv},
arxivId = {1611.01144v5},
author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
booktitle = {5th International Conference on Learning Representations},
eprint = {1611.01144v5},
file = {:home/mario/Documents/Mendeley/2017/Jang, Gu, Poole - 2017 - Categorical reparameterization with gumbel-softmax.pdf:pdf},
title = {{Categorical reparameterization with gumbel-softmax}},
url = {https://arxiv.org/pdf/1611.01144.pdf},
year = {2017}
}
@inproceedings{Kim2018,
abstract = {Image captioning has evolved with the progress of deep neu-ral networks. However, generating qualitatively detailed and distinctive captions is still an open issue. In previous works, a caption involving semantic description can be generated by applying additional information into the RNNs. In this approach, we propose a distinctive-attribute extraction (DaE) method that extracts attributes which explicitly encourage RNNs to generate an accurate caption. We evaluate the proposed method with a challenge data and verify that this method improves the performance, describing images in more detail. The method can be plugged into various models to improve their performance.},
address = {Munich, Germany},
author = {Kim, Boeun and Lee, Young Han and Jung, Hyedong and Cho, Choongsang},
booktitle = {15th European Conference on Computer Vision (ECCV 2018)},
file = {:home/mario/Documents/Mendeley/2018/Kim et al. - 2018 - Distinctive-attribute Extraction for Image Captioning.pdf:pdf},
title = {{Distinctive-attribute Extraction for Image Captioning}},
url = {http://openaccess.thecvf.com/content{\_}ECCVW{\_}2018/papers/11132/Kim{\_}Distinctive-attribute{\_}Extraction{\_}for{\_}Image{\_}Captioning{\_}ECCVW{\_}2018{\_}paper.pdf},
year = {2018}
}
@inproceedings{Bach2003,
address = {Hong Kong, China},
author = {Bach, F.R. and Jordan, M.I.},
booktitle = {2003 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP'03)},
doi = {10.1109/ICASSP.2003.1202783},
file = {:home/mario/Documents/Mendeley/2003/Bach, Jordan - 2003 - Kernel independent component analysis.pdf:pdf},
isbn = {0-7803-7663-3},
month = {jun},
pages = {IV--876--9},
publisher = {IEEE},
title = {{Kernel independent component analysis}},
url = {http://ieeexplore.ieee.org/document/1202783/},
volume = {4},
year = {2003}
}
@inproceedings{He2017,
author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2017.322},
file = {:home/mario/Documents/Mendeley/2017/He et al. - 2017 - Mask R-CNN.pdf:pdf},
isbn = {978-1-5386-1032-9},
month = {oct},
pages = {2980--2988},
publisher = {IEEE},
title = {{Mask R-CNN}},
url = {http://ieeexplore.ieee.org/document/8237584/},
year = {2017}
}
@inproceedings{Chen2015,
abstract = {In this paper we explore the bi-directional mapping be-tween images and their sentence-based descriptions. We propose learning this mapping using a recurrent neural net-work. Unlike previous approaches that map both sentences and images to a common embedding, we enable the gener-ation of novel sentences given an image. Using the same model, we can also reconstruct the visual features associ-ated with an image given its visual description. We use a novel recurrent visual memory that automatically learns to remember long-term visual concepts to aid in both sentence generation and visual feature reconstruction. We evaluate our approach on several tasks. These include sentence gen-eration, sentence retrieval and image retrieval. State-of-the-art results are shown for the task of generating novel im-age descriptions. When compared to human generated cap-tions, our automatically generated captions are preferred by humans over 19.8{\%} of the time. Results are better than or comparable to state-of-the-art results on the image and sentence retrieval tasks for methods using similar visual features.},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.5654v1},
author = {Chen, Xinlei and Zitnick, C Lawrence},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2015.7298856},
eprint = {arXiv:1411.5654v1},
file = {:home/mario/Documents/Mendeley/2015/Chen, Zitnick - 2015 - Mind's eye A recurrent visual representation for image caption generation.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {10636919},
month = {jun},
pages = {2422--2431},
publisher = {IEEE},
title = {{Mind's eye: A recurrent visual representation for image caption generation}},
url = {http://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2015/html/Chen{\_}Minds{\_}Eye{\_}A{\_}2015{\_}CVPR{\_}paper.html http://ieeexplore.ieee.org/document/7298856/},
year = {2015}
}
@inproceedings{Ling2015,
address = {Stroudsburg, PA, USA},
author = {Ling, Wang and Dyer, Chris and Black, Alan W and Trancoso, Isabel and Fermandez, Ramon and Amir, Silvio and Marujo, Luis and Luis, Tiago},
booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
doi = {10.18653/v1/D15-1176},
pages = {1520--1530},
publisher = {Association for Computational Linguistics},
title = {{Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation}},
url = {http://aclweb.org/anthology/D15-1176},
year = {2015}
}
@inproceedings{Mnih2014,
abstract = {Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies. We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.},
author = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and Kavukcuoglu, Koray and Deepmind, Google},
booktitle = {Advances in Neural Information Processing Systems 27 (NIPS 2014)},
file = {:home/mario/Documents/Mendeley/2014/Mnih et al. - 2014 - Recurrent Models of Visual Attention.pdf:pdf},
title = {{Recurrent Models of Visual Attention}},
url = {http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf},
year = {2014}
}
@article{Bengio2003,
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
author = {Bengio, Yoshua and Ducharme, R{\'{e}}jean and Vincent, Pascal and Jauvin, Christian and Ca, Jauvinc@iro Umontreal and Kandola, Jaz and Hofmann, Thomas and Poggio, Tomaso and Shawe-Taylor, John},
file = {:home/mario/Documents/Mendeley/2003/Bengio et al. - 2003 - A Neural Probabilistic Language Model.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {Statistical language modeling,artificial neural networks,curse of dimensionality,distributed representation},
pages = {1137--1155},
title = {{A Neural Probabilistic Language Model}},
url = {http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf},
volume = {3},
year = {2003}
}
@inproceedings{Chen2018,
abstract = {Visual attention has shown usefulness in image captioning, with the goal of enabling a caption model to selectively focus on regions of interest. Existing models typically rely on top-down language information and learn attention implicitly by optimizing the captioning objectives. While somewhat effective, the learned top-down attention can fail to focus on correct regions of interest without direct supervision of attention. Inspired by the human visual system which is driven by not only the task-specific top-down signals but also the visual stimuli, we in this work propose to use both types of attention for image captioning. In particular, we highlight the complementary nature of the two types of attention and develop a model (Boosted Attention) to integrate them for image captioning. We validate the proposed approach with state-of-the-art performance across various evaluation metrics.},
address = {Munich, Germany},
author = {Chen, Shi and Zhao, Qi},
booktitle = {2018 European Conference on Copmputer Vision (ECCV)},
doi = {10.1007/978-3-030-01252-6_5},
file = {:home/mario/Documents/Mendeley/2018/Chen, Zhao - 2018 - Boosted Attention Leveraging Human Attention for Image Captioning.pdf:pdf},
isbn = {9783030012519},
issn = {16113349},
keywords = {Human attention,Image captioning,Visual attention},
pages = {72--88},
publisher = {Elsevier},
title = {{Boosted Attention: Leveraging Human Attention for Image Captioning}},
volume = {11215 LNCS},
year = {2018}
}
@inproceedings{Yan2015,
abstract = {Image-caption matching을 deep canonical correlation analysis(DCCA)에 기반하여 제안. DCCA를 통해 고차원 이미지와 텍스트 표현이 가능하며, non-trivial complexity 문제와 overfitting issue를 large dataset을 이용하여 해결함},
author = {Yan, Fei and Mikolajczyk, Krystian},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2015.7298966},
file = {:home/mario/Documents/Mendeley/2015/Yan, Mikolajczyk - 2015 - Deep correlation for matching images and text.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {10636919},
month = {jun},
pages = {3441--3450},
publisher = {IEEE},
title = {{Deep correlation for matching images and text}},
url = {http://ieeexplore.ieee.org/document/7298966/},
volume = {07-12-June},
year = {2015}
}
@article{Patterson2014,
abstract = {In this paper we present the first large-scale scene attribute database. First, we perform crowdsourced human studies to find a taxonomy of 102 discriminative attributes. We discover attributes related to materials, surface properties, lighting, affordances, and spatial layout. Next, we build the 'SUN attribute database' on top of the diverse SUN categorical database. We use crowdsourcing to annotate attributes for 14,340 images from 707 scene categories. We perform numerous experiments to study the interplay between scene attributes and scene categories. We train and evaluate attribute classifiers and then study the feasibility of attributes as an intermediate scene representation for scene classification, zero shot learning, automatic image captioning, semantic image search, and parsing natural images. We show that when used as features for these tasks, low dimensional scene attributes can compete with or improve on the state of the art performance. The experiments suggest that scene attributes are an effective low-dimensional feature for capturing high-level context and semantics in scenes. [ABSTRACT FROM AUTHOR]},
author = {Patterson, Genevieve and Xu, Chen and Su, Hang and Hays, James},
doi = {10.1007/s11263-013-0695-z},
file = {:home/mario/Documents/Mendeley/2014/Patterson et al. - 2014 - The SUN Attribute Database Beyond Categories for Deeper Scene Understanding.pdf:pdf},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
keywords = {Attributes,Crowdsourcing,Image captioning,Scene parsing,Scene understanding},
month = {may},
number = {1-2},
pages = {59--81},
title = {{The SUN Attribute Database: Beyond Categories for Deeper Scene Understanding}},
url = {http://link.springer.com/10.1007/s11263-013-0695-z},
volume = {108},
year = {2014}
}
@inproceedings{Mao2014,
abstract = {In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel sentence descriptions to explain the content of images. It directly models the probability distribution of generating a word given previous words and the image. Image descriptions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on three benchmark datasets (IAPR TC-12, Flickr 8K, and Flickr 30K). Our model outperforms the state-of-the-art generative method. In addition, the m-RNN model can be applied to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval.},
archivePrefix = {arXiv},
arxivId = {1410.1090},
author = {Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Yuille, Alan L.},
booktitle = {NIPS 2014 Deep Learning Workshop},
eprint = {1410.1090},
file = {:home/mario/Documents/Mendeley/2014/Mao et al. - 2014 - Explain Images with Multimodal Recurrent Neural Networks.pdf:pdf},
title = {{Explain Images with Multimodal Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1410.1090},
year = {2014}
}
@misc{Mahmood2018,
abstract = {Humans make accurate decisions by interpreting complex data from multiple sources. Medical diagnostics, in particular, often hinge on human interpretation of multi-modal information. In order for artificial intelligence to make progress in automated, objective, and accurate diagnosis and prognosis, methods to fuse information from multiple medical imaging modalities are required. However, combining information from multiple data sources has several challenges, as current deep learning architectures lack the ability to extract useful representations from multimodal information, and often simple concatenation is used to fuse such information. In this work, we propose Multimodal DenseNet, a novel architecture for fusing multimodal data. Instead of focusing on concatenation or early and late fusion, our proposed architectures fuses information over several layers and gives the model flexibility in how it combines information from multiple sources. We apply this architecture to the challenge of polyp characterization and landmark identification in endoscopy. Features from white light images are fused with features from narrow band imaging or depth maps. This study demonstrates that Multimodal DenseNet outperforms monomodal classification as well as other multimodal fusion techniques by a significant margin on two different datasets.},
archivePrefix = {arXiv},
arxivId = {1811.07407},
author = {Mahmood, Faisal and Yang, Ziyun and Ashley, Thomas and Durr, Nicholas J.},
eprint = {1811.07407},
file = {:home/mario/Documents/Mendeley/2018/Mahmood et al. - 2018 - Multimodal Densenet.pdf:pdf},
title = {{Multimodal Densenet}},
url = {http://arxiv.org/abs/1811.07407},
year = {2018}
}
@inproceedings{Lin1998,
abstract = {Similarity is an important and widely used concept. Previous definitions of similarity are tied to a particular application or a form of knowledge representation. We present an information-theoretic definition of similarity that is applicable as long as there is a probabilistic model. We demonstrate how our definition can be used to measure the similarity in a number of different domains.},
address = {Madison, USA},
author = {Lin, Dekang},
booktitle = {Proceedings of the Fifteenth International Conference on Machine Learning (ICML 1998)},
file = {:home/mario/Documents/Mendeley/1998/Lin - 1998 - An Information-Theoretic Definition of Similarity.pdf:pdf},
isbn = {1-55860-556-8},
pages = {296--304},
title = {{An Information-Theoretic Definition of Similarity}},
year = {1998}
}
@inproceedings{Mao2015_Child,
abstract = {In this paper, we address the task of learning novel visual concepts, and their interactions with other concepts, from a few images with sentence descriptions. Using linguistic context and visual features, our method is able to efficiently hypothesize the semantic meaning of new words and add them to its word dictionary so that they can be used to describe images which contain these novel concepts. Our method has an image captioning module based on m-RNN with several improvements. In particular, we propose a transposed weight sharing scheme, which not only improves performance on image captioning, but also makes the model more suitable for the novel concept learning task. We propose methods to prevent overfitting the new concepts. In addition, three novel concept datasets are constructed for this new task. In the experiments, we show that our method effectively learns novel visual concepts from a few examples without disturbing the previously learned concepts. The project page is http://www.stat.ucla.edu/{\~{}}junhua.mao/projects/child{\_}learning.html},
archivePrefix = {arXiv},
arxivId = {arXiv:1504.06692v2},
author = {Mao, Junhua and Wei, Xu and Yang, Yi and Wang, Jiang and Huang, Zhiheng and Yuille, Alan L.},
booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.291},
eprint = {arXiv:1504.06692v2},
file = {:home/mario/Documents/Mendeley/2015/Mao et al. - 2015 - Learning Like a Child Fast Novel Visual Concept Learning from Sentence Descriptions of Images.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {15505499},
month = {dec},
pages = {2533--2541},
publisher = {IEEE},
title = {{Learning Like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images}},
url = {http://ieeexplore.ieee.org/document/7410648/},
year = {2015}
}
@inproceedings{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-ferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make train-ing faster, we used non-saturating neurons and a very efficient GPU implemen-tation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called " dropout " that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
address = {Lake Tahoe, USA},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
booktitle = {Proceedings of the 25 th International Conference on Neural Information Processing Systems},
doi = {10.1145/3065386},
file = {:home/mario/Documents/Mendeley/2012/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet classification with deep convolutional neural networks.pdf:pdf},
issn = {00010782},
month = {may},
pages = {1097--1105},
title = {{ImageNet classification with deep convolutional neural networks}},
url = {http://ascelibrary.org/doi/10.1061/{\%}28ASCE{\%}29GT.1943-5606.0001284 http://dl.acm.org/citation.cfm?doid=3098997.3065386},
year = {2012}
}
@inproceedings{Zhou2017,
abstract = {Attention mechanisms have attracted considerable interest in image captioning due to its powerful performance. However, existing methods use only visual content as attention and whether textual context can improve attention in image captioning remains unsolved. To explore this problem, we propose a novel attention mechanism, called $\backslash$textit{\{}text-conditional attention{\}}, which allows the caption generator to focus on certain image features given previously generated text. To obtain text-related image features for our attention model, we adopt the guiding Long Short-Term Memory (gLSTM) captioning architecture with CNN fine-tuning. Our proposed method allows joint learning of the image embedding, text embedding, text-conditional attention and language model with one network architecture in an end-to-end manner. We perform extensive experiments on the MS-COCO dataset. The experimental results show that our method outperforms state-of-the-art captioning methods on various quantitative metrics as well as in human evaluation, which supports the use of our text-conditional attention in image captioning.},
address = {New York, USA},
archivePrefix = {arXiv},
arxivId = {1606.04621},
author = {Zhou, Luowei and Xu, Chenliang and Koch, Parker and Corso, Jason J.},
booktitle = {Thematic Workshops of ACM Multimedia 2017},
doi = {10.1145/3126686.3126717},
eprint = {1606.04621},
file = {:home/mario/Documents/Mendeley/2017/Zhou et al. - 2017 - Watch What You Just Said Image Captioning with Text-Conditional Attention.pdf:pdf},
isbn = {9781450354165},
keywords = {-  Computing methodologies  -{\textgreater}  Natural language g,Computer vision representations,Neural networks,image captioning,lstm,multi-modal embedding,neural},
month = {jun},
pages = {305--313},
publisher = {ACM Press},
title = {{Watch What You Just Said: Image Captioning with Text-Conditional Attention}},
url = {http://arxiv.org/abs/1606.04621 http://dl.acm.org/citation.cfm?doid=3126686.3126717},
year = {2017}
}
@inproceedings{Gu2017,
abstract = {Language models based on recurrent neural networks have dominated recent image caption generation tasks. In this paper, we introduce a language CNN model which is suitable for statistical language modeling tasks and shows competitive performance in image captioning. In contrast to previous models which predict next word based on one previous word and hidden state, our language CNN is fed with all the previous words and can model the long-range dependencies in history words, which are critical for image captioning. The effectiveness of our approach is validated on two datasets: Flickr30K and MS COCO. Our extensive experimental results show that our method outperforms the vanilla recurrent neural network based language models and is competitive with the state-of-the-art methods.},
address = {Venice, Italy},
author = {Gu, Jiuxiang and Wang, Gang and Cai, Jianfei and Chen, Tsuhan},
booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2017.138},
file = {:home/mario/Documents/Mendeley/2017/Gu et al. - 2017 - An Empirical Study of Language CNN for Image Captioning.pdf:pdf},
isbn = {978-1-5386-1032-9},
month = {oct},
pages = {1231--1240},
publisher = {IEEE},
title = {{An Empirical Study of Language CNN for Image Captioning}},
url = {http://ieeexplore.ieee.org/document/8237400/},
year = {2017}
}
@inproceedings{Rashtchian2010,
abstract = {Crowd-sourcing approaches such as Ama-zon's Mechanical Turk (MTurk) make it possible to annotate or collect large amounts of linguistic data at a relatively low cost and high speed. However, MTurk offers only limited control over who is allowed to particpate in a particular task. This is particularly problematic for tasks requiring free-form text entry. Unlike multiple-choice tasks there is no correct answer, and therefore control items for which the correct answer is known cannot be used. Furthermore, MTurk has no effective built-in mechanism to guarantee workers are proficient English writers. We describe our experience in creating corpora of images annotated with multiple one-sentence descriptions on MTurk and explore the effectiveness of different quality control strategies for collecting linguistic data using Mechanical MTurk. We find that the use of a qualification test provides the highest improvement of quality, whereas refining the annotations through follow-up tasks works rather poorly. Using our best setup, we construct two image corpora, totaling more than 40,000 descriptive captions for 9000 images.},
address = {Los Angeles, USA},
author = {Rashtchian, Cyrus and Young, Peter and Hodosh, Micah and Hockenmaier, Julia},
booktitle = {Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk,},
file = {:home/mario/Documents/Mendeley/2010/Rashtchian et al. - 2010 - Collecting Image Annotations Using Amazon's Mechanical Turk.pdf:pdf},
keywords = {Flickr8K},
mendeley-tags = {Flickr8K},
pages = {139--147},
title = {{Collecting Image Annotations Using Amazon's Mechanical Turk}},
url = {https://pdfs.semanticscholar.org/bf60/322f83714523e2d7c1d39983151fe9db7146.pdf},
year = {2010}
}
@inproceedings{Schuster2015,
abstract = {Semantically complex queries which include attributes of objects and relations between objects still pose a major challenge to image retrieval systems. Recent work in computer vision has shown that a graph-based semantic representation called a scene graph is an effective representation for very detailed image descriptions and for complex queries for retrieval. In this paper, we show that scene graphs can be effectively created automatically from a natural language scene description. We present a rule-based and a classifier-based scene graph parser whose output can be used for image retrieval. We show that including relations and attributes in the query graph outperforms a model that only considers objects and that using the output of our parsers is almost as effective as using human-constructed scene graphs (Re-call@10 of 27.1{\%} vs. 33.4{\%}). Additionally , we demonstrate the general usefulness of parsing to scene graphs by showing that the output can also be used to generate 3D scenes.},
address = {Lisbon, Portugal},
author = {Schuster, Sebastian and Krishna, Ranjay and Chang, Angel and Fei-Fei, Li and Manning, Christopher D},
booktitle = {Proceedings of the Fourth Workshop on Vision and Language},
doi = {10.18653/v1/W15-2812},
file = {:home/mario/Documents/Mendeley/2015/Schuster et al. - 2015 - Generating Semantically Precise Scene Graphs from Textual Descriptions for Improved Image Retrieval.pdf:pdf},
pages = {70--80},
publisher = {Association for Computational Linguistics},
title = {{Generating Semantically Precise Scene Graphs from Textual Descriptions for Improved Image Retrieval}},
url = {http://nlp.stanford.edu/software/scenegraph- http://aclweb.org/anthology/W15-2812},
year = {2015}
}
@inproceedings{Zhang2017,
abstract = {Generating natural language descriptions of images is an important capability for a robot or other visual-intelligence driven AI agent that may need to communicate with human users about what it is seeing. Such image captioning methods are typically trained by maximising the likelihood of ground-truth annotated caption given the image. While simple and easy to implement, this approach does not directly maximise the language quality metrics we care about such as CIDEr. In this paper we investigate training image captioning methods based on actor-critic reinforcement learning in order to directly optimise non-differentiable quality metrics of interest. By formulating a per-token advantage and value computation strategy in this novel reinforcement learning based captioning model, we show that it is possible to achieve the state of the art performance on the widely used MSCOCO benchmark.},
address = {Long Beach, USA},
archivePrefix = {arXiv},
arxivId = {1706.09601v2},
author = {Zhang, Li and Sung, Flood and Liu, Feng and Xiang, Tao and Gong, Shaogang and Yang, Yongxin and Hospedales, Timothy M},
booktitle = {31st Conference on Neural Information Processing Systems (NIPS 2017)},
eprint = {1706.09601v2},
file = {:home/mario/Documents/Mendeley/2017/Zhang et al. - 2017 - Actor-Critic Sequence Training for Image Captioning.pdf:pdf},
title = {{Actor-Critic Sequence Training for Image Captioning}},
url = {https://arxiv.org/pdf/1706.09601.pdf},
year = {2017}
}
@inproceedings{Zoph2018,
abstract = {Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the "NASNet search space") which enables transferability. In our experiments, we search for the best convolutional layer (or "cell") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named "NASNet architecture". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4{\%} error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7{\%} top-1 and 96.2{\%} top-5 on ImageNet. Our model is 1.2{\%} better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28{\%} in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74{\%} top-1 accuracy, which is 3.1{\%} better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0{\%} achieving 43.1{\%} mAP on the COCO dataset.},
address = {Salt Lake City, USA},
archivePrefix = {arXiv},
arxivId = {1707.07012},
author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00907},
eprint = {1707.07012},
file = {:home/mario/Documents/Mendeley/2018/Zoph et al. - 2018 - Learning Transferable Architectures for Scalable Image Recognition.pdf:pdf},
isbn = {978-1-5386-6420-9},
month = {jun},
pages = {8697--8710},
publisher = {IEEE},
title = {{Learning Transferable Architectures for Scalable Image Recognition}},
url = {http://arxiv.org/abs/1707.07012 https://ieeexplore.ieee.org/document/8579005/},
year = {2018}
}
@inproceedings{Ren2015,
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
booktitle = {Advances in Neural Information Processing Systems 28 (NIPS 2015)},
file = {:home/mario/Documents/Mendeley/2015/Ren et al. - 2015 - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf:pdf},
pages = {91--99},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
url = {http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks},
year = {2015}
}
@inproceedings{Zitnick2013,
author = {Zitnick, C. Lawrence and Parikh, Devi and Vanderwende, Lucy},
booktitle = {2013 IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2013.211},
isbn = {978-1-4799-2840-8},
month = {dec},
pages = {1681--1688},
publisher = {IEEE},
title = {{Learning the Visual Interpretation of Sentences}},
url = {http://ieeexplore.ieee.org/document/6751319/},
year = {2013}
}
@article{Collobert2011,
abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.},
author = {Collobert, Ronan and Weston, Jason and Bottou, L{\'{e}}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel P.},
journal = {Journal of Machine Learning Research},
pages = {2493--2537},
title = {{Natural Language Processing (almost) from Scratch}},
url = {https://www.semanticscholar.org/paper/Natural-Language-Processing-(almost)-from-Scratch-Collobert-Weston/2538e3eb24d26f31482c479d95d2e26c0e79b990},
volume = {12},
year = {2011}
}
@inproceedings{Ma2015,
abstract = {In this paper, we propose multimodal convolutional neu-ral networks (m-CNNs) for matching image and sentence. Our m-CNN provides an end-to-end framework with convo-lutional architectures to exploit image representation, word composition, and the matching relations between the two modalities. More specifically, it consists of one image CNN encoding the image content, and one matching CNN learn-ing the joint representation of image and sentence. The matching CNN composes words to different semantic frag-ments and learns the inter-modal relations between image and the composed fragments at different levels, thus fully exploit the matching relations between image and sentence. Experimental results on benchmark databases of bidirec-tional image and sentence retrieval demonstrate that the proposed m-CNNs can effectively capture the information necessary for image and sentence matching. Specifically, our proposed m-CNNs for bidirectional image and sentence retrieval on Flickr8K and Flickr30K databases significantly outperform the state-of-the-art approaches.},
author = {Ma, Lin and Lu, Zhengdong and Shang, Lifeng and Li, Hang},
booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.301},
file = {:home/mario/Documents/Mendeley/2015/Ma et al. - 2015 - Multimodal Convolutional Neural Networks for Matching Image and Sentence.pdf:pdf},
isbn = {978-1-4673-8391-2},
keywords = {Administrative health data,Lung cancer,Palliative care,Survival time},
month = {dec},
pages = {2623--2631},
publisher = {IEEE},
title = {{Multimodal Convolutional Neural Networks for Matching Image and Sentence}},
url = {http://ieeexplore.ieee.org/document/7410658/},
volume = {17},
year = {2015}
}
@inproceedings{Banerjee2005,
abstract = {We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machine-produced translation and human-produced reference translations. Unigrams can be matched based on their surface forms, stemmed forms, and meanings; furthermore , METEOR can be easily extended to include more advanced matching strategies. Once all generalized unigram matches between the two strings have been found, METEOR computes a score for this matching using a combination of unigram-precision, unigram-recall, and a measure of fragmentation that is designed to directly capture how well-ordered the matched words in the machine translation are in relation to the reference. We evaluate METEOR by measuring the correlation between the metric scores and human judgments of translation quality. We compute the Pearson R correlation value between its scores and human quality assessments of the LDC TIDES 2003 Arabic-to-English and Chinese-to-English datasets. We perform segment-by-segment correlation, and show that METEOR gets an R correlation value of 0.347 on the Arabic data and 0.331 on the Chinese data. This is shown to be an improvement on using simply unigram-precision, unigram-recall and their harmonic F1 combination. We also perform experiments to show the relative contributions of the various mapping modules.},
address = {Ann Arbor, USA},
author = {Banerjee, Satanjeev and Lavie, Alon},
booktitle = {Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization},
file = {:home/mario/Documents/Mendeley/2005/Banerjee, Lavie - 2005 - METEOR An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.pdf:pdf},
month = {dec},
pages = {65--72},
publisher = {Association for Computational Linguistics},
title = {{METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments}},
url = {https://pdfs.semanticscholar.org/7533/d30329cfdbf04ee8ee82bfef792d08015ee5.pdf},
year = {2005}
}
@inproceedings{Antol2015,
author = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C. Lawrence and Parikh, Devi},
booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.279},
file = {:home/mario/Documents/Mendeley/2015/Antol et al. - 2015 - VQA Visual Question Answering.pdf:pdf},
isbn = {978-1-4673-8391-2},
month = {dec},
pages = {2425--2433},
publisher = {IEEE},
title = {{VQA: Visual Question Answering}},
url = {http://ieeexplore.ieee.org/document/7410636/},
year = {2015}
}
@inproceedings{Jiang2018,
abstract = {Recently, much advance has been made in image captioning, and an encoder-decoder framework has achieved outstanding performance for this task. In this paper, we propose an extension of the encoder-decoder framework by adding a component called guiding network. The guiding network models the attribute properties of input images, and its output is leveraged to compose the input of the decoder at each time step. The guiding network can be plugged into the current encoder-decoder framework and trained in an end-to-end manner. Hence, the guiding vector can be adaptively learned according to the signal from the decoder, making itself to embed information from both image and language. Additionally, discriminative supervision can be employed to further improve the quality of guidance. The advantages of our proposed approach are verified by experiments carried out on the MS COCO dataset.},
address = {New Orleans, USA},
author = {Jiang, Wenhao and Ma, Lin and Chen, Xinpeng and Zhang, Hanwang and Liu, Wei},
booktitle = {The 32nd AAAI Conference on Artificial Intelligence (AAAI-18)},
file = {:home/mario/Documents/Mendeley/2018/Jiang et al. - 2018 - Learning to Guide Decoding for Image Captioning.pdf:pdf},
title = {{Learning to Guide Decoding for Image Captioning}},
url = {https://www.semanticscholar.org/paper/Learning-to-Guide-Decoding-for-Image-Captioning-Jiang-Ma/885d589101ab3c09bda20ee9578f2c6d2f6cddfa},
year = {2018}
}
@article{Zhao2019,
abstract = {Deep convolution neural networks connected with the recurrent neural networks are potent models that have achieved excellent performance on image caption task. Although many methods based on the neural network can generate fluent and complete sentences, the image feature vectors extracted by the convolution neural network can only retain a few significant features of the original image, which will lose a lot of useful image information. Moreover, RNNs have a gradient vanishing problem with the growth of RNNs time step, and the generation of sentences will lack the guidance of previous information. In this paper, we introduce a multimodal fusion method for generating descriptions to explain the content of images. Our model consists of four sub-networks: a convolutional neural network for image feature extraction, a ATTssd model for image attributes extraction, a language CNN model CNNm for sentence modeling and a recurrent network (e.g., GRU, LSTM, etc.) for word prediction. Compared with existing methods which predict next word based on one previous word and hidden state, our model uses image attributes information to enhance the image representation and handles all the previous words to modeling the long-term dependencies of history words. The methods are evaluated on the Flickr8k, Flickr30k and MSCOCO datasets. We prove that our model combined with ATTssd and CNNm can significantly enhance the performance, and achieve the competitive results.},
author = {Zhao, Dexin and Chang, Zhi and Guo, Shutao},
doi = {10.1016/j.neucom.2018.11.004},
file = {:home/mario/Documents/Mendeley/2019/Zhao, Chang, Guo - 2019 - A multimodal fusion approach for image captioning.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Deep neural network,Image caption,Multimodal fusion},
pages = {476--485},
publisher = {Elsevier B.V.},
title = {{A multimodal fusion approach for image captioning}},
url = {https://doi.org/10.1016/j.neucom.2018.11.004},
volume = {329},
year = {2019}
}
@article{Ordonez2011,
abstract = {We develop and demonstrate automatic image description methods using a large captioned photo collection. One contribution is our technique for the automatic collection of this new dataset -- performing a huge number of Flickr queries and then filtering the noisy results down to 1 million images with associated visually relevant captions. Such a collection allows us to approach the extremely challenging problem of description generation using relatively simple non-parametric methods and produces surprisingly effective results. We also develop methods incorporating many state of the art, but fairly noisy, estimates of image content to produce even more pleasing results. Finally we introduce a new objective performance measure for image captioning.},
author = {Ordonez, V and Kulkarni, G and Berg, Tl},
file = {:home/mario/Documents/Mendeley/2011/Ordonez, Kulkarni, Berg - 2011 - Im2text Describing Images Using 1 Million Captioned Photographs.pdf:pdf},
isbn = {9781618395993},
journal = {Advances in Neural Information Processing Systems (NIPS)},
pages = {1143--1151},
title = {{Im2text: Describing Images Using 1 Million Captioned Photographs}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2011{\_}0671.pdf},
year = {2011}
}
@inproceedings{Vinyals2015,
abstract = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.},
address = {Boston, USA},
archivePrefix = {arXiv},
arxivId = {1411.4555},
author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2015.7298935},
eprint = {1411.4555},
file = {:home/mario/Documents/Mendeley/2015/Vinyals et al. - 2015 - Show and tell A neural image caption generator.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {10636919},
month = {jun},
pages = {3156--3164},
pmid = {903},
publisher = {IEEE},
title = {{Show and tell: A neural image caption generator}},
url = {http://ieeexplore.ieee.org/document/7298935/},
volume = {07-12-June},
year = {2015}
}
@inproceedings{Li2011,
abstract = {Studying natural language, and especially how people describe the world around them can help us better understand the visual world. In turn, it can also help us in the quest to generate natural language that describes this world in a human manner. We present a simple yet effec- tive approach to automatically compose im- age descriptions given computer vision based inputs and using web-scale n-grams. Unlike most previous work that summarizes or re- trieves pre-existing text relevant to an image, our method composes sentences entirely from scratch. Experimental results indicate that it is viable to generate simple textual descriptions that are pertinent to the specific content of an image, while permitting creativity in the de- scription – making for more human-like anno- tations than previous approaches.},
address = {Portland, USA},
author = {Li, Siming and Kulkarni, Girish and Berg, TL and Berg, AC and Choi, Yejin},
booktitle = {Proceedings of the 15th Conference on Computational Natural Language Learning (CoNLL 2011)},
file = {:home/mario/Documents/Mendeley/2011/Li et al. - 2011 - Composing simple image descriptions using web-scale n-grams.pdf:pdf},
isbn = {978-1-932432-92-3},
pages = {220--228},
publisher = {Association for Computational Linguistics},
title = {{Composing simple image descriptions using web-scale n-grams}},
url = {http://dl.acm.org/citation.cfm?id=2018962},
year = {2011}
}
@inproceedings{Shetty2017,
abstract = {While strong progress has been made in image caption-ing recently, machine and human captions are still quite distinct. This is primarily due to the deficiencies in the generated word distribution, vocabulary size, and strong bias in the generators towards frequent captions. Furthermore, humans rightfully so-generate multiple, diverse captions, due to the inherent ambiguity in the captioning task which is not explicitly considered in today's systems. To address these challenges, we change the training objective of the caption generator from reproducing ground-truth captions to generating a set of captions that is indistinguishable from human written captions. Instead of handcrafting such a learning target, we employ adversar-ial training in combination with an approximate Gumbel sampler to implicitly match the generated distribution to the human one. While our method achieves comparable performance to the state-of-the-art in terms of the correctness of the captions, we generate a set of diverse captions that are significantly less biased and better match the global uni-, bi-and tri-gram distributions of the human captions.},
address = {Venice, Italy},
author = {Shetty, Rakshith and Rohrbach, Marcus and Hendricks, Lisa Anne and Fritz, Mario and Schiele, Bernt},
booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2017.445},
file = {:home/mario/Documents/Mendeley/2017/Shetty et al. - 2017 - Speaking the Same Language Matching Machine to Human Captions by Adversarial Training.pdf:pdf},
isbn = {978-1-5386-1032-9},
month = {oct},
pages = {4155--4164},
publisher = {IEEE},
title = {{Speaking the Same Language: Matching Machine to Human Captions by Adversarial Training}},
url = {https://goo.gl/3yRVnq http://ieeexplore.ieee.org/document/8237707/},
year = {2017}
}
@article{Kuznetsova2012,
abstract = {We present a holistic data-driven approach to image description generation, exploit- ing the vast amount of (noisy) parallel im- age data and associated natural language descriptions available on the web. More specifically, given a query image, we re- trieve existing human-composed phrases used to describe visually similar images, then selectively combine those phrases to generate a novel description for the query image. We cast the generation pro- cess as constraint optimization problems, collectively incorporating multiple inter- connected aspects of language composition for content planning, surface realization and discourse structure. Evaluation by hu- man annotators indicates that our final system generates more semantically cor- rect and linguistically appealing descrip- tions than two nontrivial baselines.},
author = {Kuznetsova, Polina and Ordonez, Vicente and Berg, Ac},
file = {:home/mario/Documents/Mendeley/2012/Kuznetsova, Ordonez, Berg - 2012 - Collective generation of natural image descriptions.pdf:pdf},
isbn = {9781937284244},
journal = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics},
number = {July},
pages = {359--368},
title = {{Collective generation of natural image descriptions}},
url = {http://dl.acm.org/citation.cfm?id=2390575},
volume = {1},
year = {2012}
}
@inproceedings{Zoph2018_RESNet,
author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00907},
file = {:home/mario/Documents/Mendeley/2018/Zoph et al. - 2018 - Learning Transferable Architectures for Scalable Image Recognition.pdf:pdf},
isbn = {978-1-5386-6420-9},
month = {jun},
pages = {8697--8710},
publisher = {IEEE},
title = {{Learning Transferable Architectures for Scalable Image Recognition}},
url = {https://ieeexplore.ieee.org/document/8579005/},
year = {2018}
}
@article{Lecun1998,
author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
doi = {10.1109/5.726791},
file = {:home/mario/Documents/Mendeley/1998/Lecun et al. - 1998 - Gradient-based learning applied to document recognition.pdf:pdf},
issn = {00189219},
journal = {Proceedings of the IEEE},
number = {11},
pages = {2278--2324},
title = {{Gradient-based learning applied to document recognition}},
url = {http://ieeexplore.ieee.org/document/726791/{\#}full-text-section http://ieeexplore.ieee.org/document/726791/},
volume = {86},
year = {1998}
}
@article{Yuan2019,
abstract = {It is a big challenge of computer vision to make machine automatically describe the content of an image with a natural language sentence. Previous works have made great progress on this task, but they only use the global or local image feature, which may lose some important subtle or global information of an image. In this paper, we propose a model with 3-gated model which fuses the global and local image features together for the task of image caption generation. The model mainly has three gated structures. (1) Gate for the global image feature, which can adaptively decide when and how much the global image feature should be imported into the sentence generator. (2) The gated recurrent neural network (RNN) is used as the sentence generator. (3) The gated feedback method for stacking RNN is employed to increase the capability of nonlinearity fitting. More specially, the global and local image features are combined together in this paper, which makes full use of the image information. The global image feature is controlled by the first gate and the local image feature is selected by the attention mechanism. With the latter two gates, the relationship between image and text can be well explored, which improves the performance of the language part as well as the multi-modal embedding part. Experimental results show that our proposed method outperforms the state-of-the-art for image caption generation.},
author = {Yuan, Aihong and Li, Xuelong and Lu, Xiaoqiang},
doi = {10.1016/j.neucom.2018.10.059},
file = {:home/mario/Documents/Mendeley/2019/Yuan, Li, Lu - 2019 - 3G structure for image caption generation.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Convolutional neural network,Deep learning,Image caption generation,Multi-modal learning,Recurrent neural network},
pages = {17--28},
publisher = {Elsevier B.V.},
title = {{3G structure for image caption generation}},
url = {https://doi.org/10.1016/j.neucom.2018.10.059},
volume = {330},
year = {2019}
}
@inproceedings{Oruganti2016,
author = {Oruganti, Ram Manohar and Sah, Shagan and Pillai, Suhas and Ptucha, Raymond},
booktitle = {2016 IEEE International Conference on Image Processing (ICIP)},
doi = {10.1109/ICIP.2016.7533033},
file = {:home/mario/Documents/Mendeley/2016/Oruganti et al. - 2016 - Image description through fusion based recurrent multi-modal learning.pdf:pdf},
isbn = {978-1-4673-9961-6},
month = {sep},
pages = {3613--3617},
publisher = {IEEE},
title = {{Image description through fusion based recurrent multi-modal learning}},
url = {http://ieeexplore.ieee.org/document/7533033/},
year = {2016}
}
@article{Mnih2015,
abstract = {The theory of reinforcement learning provides a normative account 1 , deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems 4,5 , the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopa-minergic neurons and temporal difference reinforcement learning algorithms 3. While reinforcement learning agents have achieved some successes in a variety of domains 6-8 , their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9-11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games 12. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks. We set out to create a single algorithm that would be able to develop a wide range of competencies on a varied range of challenging tasks-a central goal of general artificial intelligence 13 that has eluded previous efforts 8,14,15. To achieve this, we developed a novel agent, a deep Q-network (DQN), which is able to combine reinforcement learning with a class of artificial neural network 16 known as deep neural networks. Notably, recent advances in deep neural networks 9-11 , in which several layers of nodes are used to build up progressively more abstract representations of the data, have made it possible for artificial neural networks to learn concepts such as object categories directly from raw sensory data. We use one particularly successful architecture, the deep convolutional network 17 , which uses hierarchical layers of tiled convolutional filters to mimic the effects of receptive fields-inspired by Hubel and Wiesel's seminal work on feedforward processing in early visual cortex 18-thereby exploiting the local spatial correlations present in images, and building in robustness to natural transformations such as changes of viewpoint or scale. We consider tasks in which the agent interacts with an environment through a sequence of observations, actions and rewards. The goal of the agent is to select actions in a fashion that maximizes cumulative future reward. More formally, we use a deep convolutional neural network to approximate the optimal action-value function Q {\~{A}} s,a ð Þ{\~{}} max p r t zcr tz1 zc 2 r tz2 z. .. js t {\~{}}s, a t {\~{}}a, p {\^{A}} {\~{A}} , which is the maximum sum of rewards r t discounted by c at each time-step t, achievable by a behaviour policy p 5 P(ajs), after making an observation (s) and taking an action (a) (see Methods) 19. Reinforcement learning is known to be unstable or even to diverge when a nonlinear function approximator such as a neural network is used to represent the action-value (also known as Q) function 20. This instability has several causes: the correlations present in the sequence of observations, the fact that small updates to Q may significantly change the policy and therefore change the data distribution, and the correlations between the action-values (Q) and the target values rzc max a 0 Q s 0 , a 0 ð Þ. We address these instabilities with a novel variant of Q-learning, which uses two key ideas. First, we used a biologically inspired mechanism termed experience replay 21-23 that randomizes over the data, thereby removing correlations in the observation sequence and smoothing over changes in the data distribution (see below for details). Second, we used an iterative update that adjusts the action-values (Q) towards target values that are only periodically updated, thereby reducing correlations with the target. While other stable methods exist for training neural networks in the reinforcement learning setting, such as neural fitted Q-iteration 24 , these methods involve the repeated training of networks de novo on hundreds of iterations. Consequently, these methods, unlike our algorithm, are too inefficient to be used successfully with large neural networks. We parameterize an approximate value function Q(s,a;h i) using the deep convolutional neural network shown in Fig. 1, in which h i are the parameters (that is, weights) of the Q-network at iteration i. To perform experience replay we store the agent's experiences e t 5 (s t ,a t ,r t ,s t 1 1) at each time-step t in a data set D t 5 {\{}e 1 ,{\ldots},e t {\}}. During learning, we apply Q-learning updates, on samples (or minibatches) of experience (s,a,r,s9) , U(D), drawn uniformly at random from the pool of stored samples. The Q-learning update at iteration i uses the following loss function:},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
file = {:home/mario/Documents/Mendeley/2015/Mnih et al. - 2015 - Human-level control through deep reinforcement learning.pdf:pdf},
journal = {Nature},
pages = {529--541},
title = {{Human-level control through deep reinforcement learning}},
url = {https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf},
volume = {518},
year = {2015}
}
@article{Lowe2004,
author = {Lowe, David G.},
doi = {10.1023/B:VISI.0000029664.99615.94},
file = {:home/mario/Documents/Mendeley/2004/Lowe - 2004 - Distinctive Image Features from Scale-Invariant Keypoints.pdf:pdf},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
month = {nov},
number = {2},
pages = {91--110},
title = {{Distinctive Image Features from Scale-Invariant Keypoints}},
url = {http://link.springer.com/10.1023/B:VISI.0000029664.99615.94},
volume = {60},
year = {2004}
}
@inproceedings{Hodosh2013b,
abstract = {Associating photographs with complete sentences that describe what is depicted in them is a challenging problem. This paper examines how an approach that is inspired by image tagging techniques which can scale to very large data sets performs on this much harder task, and examines some of the linguistic difficulties that this bag-of-words model faces.},
author = {Hodosh, Micah and Hockenmaier, Julia},
booktitle = {2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2013.51},
file = {:home/mario/Documents/Mendeley/2013/Hodosh, Hockenmaier - 2013 - Sentence-Based Image Description with Scalable, Explicit Models.pdf:pdf},
isbn = {978-0-7695-4990-3},
month = {jun},
pages = {294--300},
publisher = {IEEE},
title = {{Sentence-Based Image Description with Scalable, Explicit Models}},
url = {http://openaccess.thecvf.com/content{\_}cvpr{\_}workshops{\_}2013/W06/papers/Hodosh{\_}Sentence-Based{\_}Image{\_}Description{\_}2013{\_}CVPR{\_}paper.pdf http://ieeexplore.ieee.org/document/6595890/},
year = {2013}
}
@inproceedings{Chatfield2014,
author = {Chatfield, Ken and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
booktitle = {Proceedings of the British Machine Vision Conference 2014},
doi = {10.5244/C.28.6},
file = {:home/mario/Documents/Mendeley/2014/Chatfield et al. - 2014 - Return of the Devil in the Details Delving Deep into Convolutional Nets.pdf:pdf},
isbn = {1-901725-52-9},
pages = {6.1--6.12},
publisher = {British Machine Vision Association},
title = {{Return of the Devil in the Details: Delving Deep into Convolutional Nets}},
url = {http://www.bmva.org/bmvc/2014/papers/paper054/index.html},
year = {2014}
}
@inproceedings{Liu2017_SAM,
abstract = {Attention mechanisms have recently been introduced in deep learning for various tasks in natural language processing and computer vision. But despite their popularity, the "correct-ness" of the implicitly-learned attention maps has only been assessed qualitatively by visualization of several examples. In this paper we focus on evaluating and improving the correct-ness of attention in neural image captioning models. Specifically , we propose a quantitative evaluation metric for the consistency between the generated attention maps and human annotations, using recently released datasets with alignment between regions in images and entities in captions. We then propose novel models with different levels of explicit supervision for learning attention maps during training. The supervision can be strong when alignment between regions and caption entities are available, or weak when only object segments and categories are provided. We show on the popular Flickr30k and COCO datasets that introducing supervision of attention maps during training solidly improves both attention correctness and caption quality, showing the promise of making machine perception more human-like.},
author = {Liu, Chenxi and Mao, Junhua and Sha, Fei and Yuille, Alan},
booktitle = {Proceedings of the 31st AAAI Conference on Artificial Intelligence (AAAI-17)},
file = {:home/mario/Documents/Mendeley/2017/Liu et al. - 2017 - Attention Correctness in Neural Image Captioning.pdf:pdf},
title = {{Attention Correctness in Neural Image Captioning}},
url = {www.aaai.org},
year = {2017}
}
@inproceedings{Wang2016_Parallel,
author = {Wang, Minsi and Song, Li and Yang, Xiaokang and Luo, Chuanfei},
booktitle = {2016 IEEE International Conference on Image Processing (ICIP)},
doi = {10.1109/ICIP.2016.7533201},
file = {:home/mario/Documents/Mendeley/2016/Wang et al. - 2016 - A parallel-fusion RNN-LSTM architecture for image caption generation.pdf:pdf},
isbn = {978-1-4673-9961-6},
month = {sep},
pages = {4448--4452},
publisher = {IEEE},
title = {{A parallel-fusion RNN-LSTM architecture for image caption generation}},
url = {http://ieeexplore.ieee.org/document/7533201/},
year = {2016}
}
@inproceedings{Ushiku2015,
author = {Ushiku, Yoshitaka and Yamaguchi, Masataka and Mukuta, Yusuke and Harada, Tatsuya},
booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.306},
file = {:home/mario/Documents/Mendeley/2015/Ushiku et al. - 2015 - Common Subspace for Model and Similarity Phrase Learning for Caption Generation from Images.pdf:pdf},
isbn = {978-1-4673-8391-2},
month = {dec},
pages = {2668--2676},
publisher = {IEEE},
title = {{Common Subspace for Model and Similarity: Phrase Learning for Caption Generation from Images}},
url = {http://ieeexplore.ieee.org/document/7410663/},
year = {2015}
}
@inproceedings{Lebret2015b,
abstract = {Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutional Neural Network) and phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on caption syntax statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results on the recently release Microsoft COCO dataset.},
archivePrefix = {arXiv},
arxivId = {1412.8419},
author = {Lebret, Remi and Pinheiro, Pedro O. and Collobert, Ronan},
booktitle = {Proceedings of the 2015 International Conference on Learning Representations},
eprint = {1412.8419},
file = {:home/mario/Documents/Mendeley/2015/Lebret, Pinheiro, Collobert - 2015 - Simple Image Description Generator via a Linear Phrase-Based Approach.pdf:pdf},
title = {{Simple Image Description Generator via a Linear Phrase-Based Approach}},
url = {http://arxiv.org/abs/1412.8419},
year = {2015}
}
@inproceedings{Mitchell2012,
abstract = {This paper introduces a novel generation system that composes humanlike descriptions of images from computer vision detections. By leveraging syntactically informed word co-occurrence statistics, the generator ﬁlters and constrains the noisy detections output from a vision system to generate syntactic trees that detail what the computer vision system sees. Results show that the generation system outperforms state-of-the-art systems, automatically generating some of the most natural image descriptions to date.},
author = {Mitchell, Margaret and Dodge, Jesse and Goyal, Amit and Yamaguchi, Kota and Stratos, Karl and Han, Xufeng and Mensch, Alyssa and Berg, Alexander C. and Berg, Tamara L. and {Daume III}, Hal},
booktitle = {Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics},
file = {:home/mario/Documents/Mendeley/2012/Mitchell et al. - 2012 - Midge Generating Image Descriptions From Computer Vision Detections.pdf:pdf},
isbn = {978-1-937284-19-0},
pages = {747--756},
title = {{Midge: Generating Image Descriptions From Computer Vision Detections}},
year = {2012}
}
@inproceedings{Hoyer2008,
author = {Hoyer, Patrik O. and Janzing, Dominik and Mooij, Joris M. and Peters, Jonas and Sch{\"{o}}lkopf, Bernhard},
booktitle = {Twenty-Second Annual Conference on Neural Information Processing Systems (NIPS 2008)},
file = {:home/mario/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoyer et al. - 2009 - Nonlinear causal discovery with additive noise models.pdf:pdf},
pages = {689--696},
title = {{Nonlinear causal discovery with additive noise models}},
url = {https://papers.nips.cc/paper/3548-nonlinear-causal-discovery-with-additive-noise-models},
year = {2008}
}
@article{Barto1983,
abstract = {It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem. The task is to balance a pole that is hinged to a movable cart by applying forces to the cart's base. It is argued that the learning problems faced by adaptive elements that are components of adaptive networks are at least as difficult as this version of the pole-balancing problem. The learning system consists of a single associative search element (ASE) and a single adaptive critic element (ACE). In the course of learning to balance the pole, the ASE constructs associations between input and output by searching under the influence of reinforcement feedback, and the ACE constructs a more informative evaluation function than reinforcement feedback alone can provide. The differences between this approach and other attempts to solve problems using neurolike elements are discussed, as is the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences.},
author = {Barto, Andrew G. and Sutton, Richard S. and Anderson, Charles W.},
doi = {10.1109/TSMC.1983.6313077},
issn = {0018-9472},
journal = {IEEE Transactions on Systems, Man, and Cybernetics},
month = {sep},
number = {5},
pages = {834--846},
title = {{Neuronlike adaptive elements that can solve difficult learning control problems}},
url = {http://ieeexplore.ieee.org/document/6313077/},
volume = {SMC-13},
year = {1983}
}
@inproceedings{LeiBa2015,
abstract = {We present an attention-based model for recognizing multiple objects in images. The proposed model is a deep recurrent neural network trained with reinforcement learning to attend to the most relevant regions of the input image. We show that the model learns to both localize and recognize multiple objects despite being given only class labels during training. We evaluate the model on the challenging task of transcribing house number sequences from Google Street View images and show that it is both more accurate than the state-of-the-art convolutional networks and uses fewer parameters and less computation.},
address = {San Diego, USA},
archivePrefix = {arXiv},
arxivId = {1412.7755v2},
author = {{Lei Ba}, Jimmy and Mnih, Volodymyr and Deepmind, Google and Kavukcuoglu, Koray},
booktitle = {Proceedings of the 3rd International Conference on Learning Representations},
eprint = {1412.7755v2},
file = {:home/mario/Documents/Mendeley/2015/Lei Ba et al. - 2015 - Multiple Object Recognition with Visual Attention.pdf:pdf},
title = {{Multiple Object Recognition with Visual Attention}},
url = {https://arxiv.org/pdf/1412.7755.pdf},
year = {2015}
}
@inproceedings{Kulkarni2011,
abstract = {We posit that visually descriptive language offers com- puter vision researchers both information about the world, and information about how people describe the world. The potential benefit from this source is made more significant due to the enormous amount of language data easily avail- able today. We present a system to automatically gener- ate natural language descriptions from images that exploits both statistics gleaned from parsing large quantities of text data and recognition algorithms from computer vision. The system is very effective at producing relevant sentences for images. It also generates descriptions that are notably more true to the specific image content than previous work.},
address = {Colorado Springs, USA},
author = {Kulkarni, Girish and Premraj, Visruth and Dhar, Sagnik and Li, Siming and Choi, Yejin and Berg, Alexander C and Berg, Tamara L},
booktitle = {24th IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2011)},
doi = {10.1109/CVPR.2011.5995466},
file = {:home/mario/Documents/Mendeley/2011/Kulkarni et al. - 2011 - Baby talk Understanding and generating simple image descriptions.pdf:pdf},
isbn = {978-1-4577-0394-2},
issn = {0018-019X},
month = {jun},
pages = {1601--1608},
pmid = {22848128},
publisher = {IEEE},
title = {{Baby talk: Understanding and generating simple image descriptions}},
url = {http://doi.wiley.com/10.1002/hlca.19350180143 http://ieeexplore.ieee.org/document/5995466/},
volume = {18},
year = {2011}
}
@inproceedings{Girshick2014,
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.81},
file = {:home/mario/Documents/Mendeley/2014/Girshick et al. - 2014 - Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.pdf:pdf},
isbn = {978-1-4799-5118-5},
month = {jun},
pages = {580--587},
publisher = {IEEE},
title = {{Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation}},
url = {http://ieeexplore.ieee.org/document/6909475/},
year = {2014}
}
@inproceedings{Farhadi2010,
abstract = {Humans can prepare concise descriptions of pictures, focus- ing on what they find important.We demonstrate that automatic meth- ods can do so too.We describe a system that can compute a score linking an image to a sentence. This score can be used to attach a descriptive sentence to a given image, or to obtain images that illustrate a given sentence. The score is obtained by comparing an estimate of meaning ob- tained from the image to one obtained from the sentence. Each estimate of meaning comes from a discriminative procedure that is learned us- ing data. We evaluate on a novel dataset consisting of human-annotated images. While our underlying estimate of meaning is impoverished, it is sufficient to produce very good quantitative results, evaluated with a novel score that can account for synecdoche.},
address = {Heraklion, Greece},
author = {Farhadi, Ali and Hejrati, Mohsen and Sadeghi, Mohammad Amin and Young, Peter and Rashtchian, Cyrus and Hockenmaier, Julia and Forsyth, David},
booktitle = {Proceedings of the 11th European Conference on Computer Vision (ECCV)},
doi = {10.1007/978-3-642-15561-1_2},
file = {:home/mario/Documents/Mendeley/2010/Farhadi et al. - 2010 - Every Picture Tells a Story Generating Sentences from Images.pdf:pdf},
isbn = {364215560X},
issn = {03029743},
pages = {15--29},
publisher = {Elsevier},
title = {{Every Picture Tells a Story: Generating Sentences from Images}},
url = {http://link.springer.com/10.1007/978-3-642-15561-1{\_}2},
volume = {6314 LNCS},
year = {2010}
}
@article{Tanti2018,
abstract = {When a recurrent neural network language model is used for caption generation, the image information can be fed to the neural network either by directly incorporating it in the RNN -- conditioning the language model by `injecting' image features -- or in a layer following the RNN -- conditioning the language model by `merging' image features. While both options are attested in the literature, there is as yet no systematic comparison between the two. In this paper we empirically show that it is not especially detrimental to performance whether one architecture is used or another. The merge architecture does have practical advantages, as conditioning by merging allows the RNN's hidden state vector to shrink in size by up to four times. Our results suggest that the visual and linguistic modalities for caption generation need not be jointly encoded by the RNN as that yields large, memory-intensive models with few tangible advantages in performance; rather, the multimodal integration should be delayed to a subsequent stage.},
archivePrefix = {arXiv},
arxivId = {1703.09137},
author = {Tanti, Marc and Gatt, Albert and Camilleri, Kenneth P.},
doi = {10.1017/S1351324918000098},
eprint = {1703.09137},
file = {:home/mario/Documents/Mendeley/2017/Tanti, Gatt, Camilleri - 2017 - Where to put the Image in an Image Caption Generator.pdf:pdf},
issn = {1351-3249},
journal = {Natural Language Engineering},
month = {mar},
number = {03},
pages = {467--489},
title = {{Where to put the Image in an Image Caption Generator}},
url = {http://arxiv.org/abs/1703.09137 http://dx.doi.org/10.1017/S1351324918000098 https://www.cambridge.org/core/product/identifier/S1351324918000098/type/journal{\_}article},
volume = {24},
year = {2017}
}
@misc{Feng2018,
abstract = {Deep neural networks have achieved great successes on the image captioning task. However, most of the existing models depend heavily on paired image-sentence datasets, which are very expensive to acquire. In this paper, we make the first attempt to train an image captioning model in an unsupervised manner. Instead of relying on manually labeled image-sentence pairs, our proposed model merely requires an image set, a sentence corpus, and an existing visual concept detector. The sentence corpus is used to teach the captioning model how to generate plausible sentences. Meanwhile, the knowledge in the visual concept detector is distilled into the captioning model to guide the model to recognize the visual concepts in an image. In order to further encourage the generated captions to be semantically consistent with the image, the image and caption are projected into a common latent space so that they can be used to reconstruct each other. Given that the existing sentence corpora are mainly designed for linguistic research and thus with little reference to image contents, we crawl a large-scale image description corpus of 2 million natural sentences to facilitate the unsupervised image captioning scenario. Experimental results show that our proposed model is able to produce quite promising results without using any labeled training pairs.},
archivePrefix = {arXiv},
arxivId = {1811.10787},
author = {Feng, Yang and Ma, Lin and Liu, Wei and Luo, Jiebo},
eprint = {1811.10787},
file = {:home/mario/Documents/Mendeley/2018/Feng et al. - 2018 - Unsupervised Image Captioning.pdf:pdf},
title = {{Unsupervised Image Captioning}},
url = {http://arxiv.org/abs/1811.10787},
volume = {1},
year = {2018}
}
@inproceedings{Jaderberg2015,
author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and koray Kavukcuoglu},
booktitle = {Advances in Neural Information Processing Systems 28 (NIPS 2015)},
file = {:home/mario/Documents/Mendeley/2015/Jaderberg et al. - 2015 - Spatial Transformer Networks.pdf:pdf},
pages = {2017--2025},
title = {{Spatial Transformer Networks}},
url = {https://papers.nips.cc/paper/5854-spatial-transformer-networks},
year = {2015}
}
@article{Vinyals2017,
author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
doi = {10.1109/TPAMI.2016.2587640},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = {apr},
number = {4},
pages = {652--663},
title = {{Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge}},
url = {http://ieeexplore.ieee.org/document/7505636/},
volume = {39},
year = {2017}
}
@inproceedings{Le2013,
abstract = {We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting with these learned features, we trained our network to obtain 15.8{\%} accuracy in recognizing 20,000 object categories from ImageNet, a leap of 70{\%} relative improvement over the previous state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1112.6209},
author = {Le, Quoc V. and Ranzato, Marc'Aurelio and Monga, Rajat and Devin, Matthieu and Chen, Kai and Corrado, Greg S. and Dean, Jeff and Ng, Andrew Y.},
booktitle = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
doi = {10.1109/ICASSP.2013.6639343},
eprint = {1112.6209},
file = {:home/mario/Documents/Mendeley/2011/Le et al. - 2011 - Building high-level features using large scale unsupervised learning.pdf:pdf},
isbn = {978-1-4799-0356-6},
month = {may},
pages = {8595--8598},
publisher = {IEEE},
title = {{Building high-level features using large scale unsupervised learning}},
url = {http://arxiv.org/abs/1112.6209 http://ieeexplore.ieee.org/document/6639343/},
year = {2011}
}
@article{Buschman2007,
abstract = {Attention can be focused volitionally by "top-down" signals derived from task demands and automatically by "bottom-up" signals from salient stimuli. The frontal and parietal cortices are involved, but their neural activity has not been directly compared. Therefore, we recorded from them simultaneously in monkeys. Prefrontal neurons reflected the target location first during top-down attention, whereas parietal neurons signaled it earlier during bottom-up attention. Synchrony between frontal and parietal areas was stronger in lower frequencies during top-down attention and in higher frequencies during bottom-up attention. This result indicates that top-down and bottom-up signals arise from the frontal and sensory cortex, respectively, and different modes of attention may emphasize synchrony at different frequencies.},
author = {Buschman, T. J. and Miller, E. K.},
doi = {10.1126/science.1138071},
issn = {0036-8075},
journal = {Science},
month = {mar},
number = {5820},
pages = {1860--1862},
pmid = {17395832},
title = {{Top-Down Versus Bottom-Up Control of Attention in the Prefrontal and Posterior Parietal Cortices}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17395832 http://www.sciencemag.org/cgi/doi/10.1126/science.1138071},
volume = {315},
year = {2007}
}
@book{Peters2015,
abstract = {"The mathematization of causality is a relatively recent development, and has become increasingly important in data science and machine learning. This book offers a self-contained and concise introduction to causal models and how to learn them from data"--Back of book.},
author = {Peters, Jonas and Janzing, Dominik and Sch{\"{o}}lkopf, Bernhard},
isbn = {9780262037310},
pages = {265},
publisher = {MIT Press},
title = {{Elements of causal inference : foundations and learning algorithms}},
url = {https://mitpress.mit.edu/books/elements-causal-inference},
year = {2015}
}
@article{Kuznetsova2014,
abstract = {We present a new tree based approach to composing expressive image descriptions that makes use of naturally occuring web images with captions. We investigate two related tasks: image caption generalization and generation , where the former is an optional sub-task of the latter. The high-level idea of our approach is to harvest expressive phrases (as tree fragments) from existing image descriptions, then to compose a new description by selectively combining the extracted (and optionally pruned) tree fragments. Key algorithmic components are tree composition and compression , both integrating tree structure with sequence structure. Our proposed system attains significantly better performance than previous approaches for both image caption generalization and generation . In addition, our work is the first to show the empirical benefit of automatically generalized captions for composing natural image descriptions.},
author = {Kuznetsova, Polina and Ordonez, Vicente and Berg, Tamara L. and Choi, Yejin},
doi = {10.1162/tacl_a_00188},
file = {:home/mario/Documents/Mendeley/2014/Kuznetsova et al. - 2014 - TreeTalk Composition and Compression of Trees for Image Descriptions.pdf:pdf},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics},
month = {dec},
pages = {351--362},
title = {{TreeTalk: Composition and Compression of Trees for Image Descriptions}},
url = {https://www.mitpressjournals.org/doi/abs/10.1162/tacl{\_}a{\_}00188},
volume = {2},
year = {2014}
}
@inproceedings{Chen2017_StructCap,
address = {New York, New York, USA},
author = {Chen, Fuhai and Ji, Rongrong and Su, Jinsong and Wu, Yongjian and Wu, Yunsheng},
booktitle = {Proceedings of the 2017 ACM on Multimedia Conference - MM '17},
doi = {10.1145/3123266.3123275},
isbn = {9781450349062},
keywords = {deep learning,image captioning,structured learning,visual relation},
pages = {46--54},
publisher = {ACM Press},
title = {{StructCap: Structured Semantic Embedding for Image Captioning}},
url = {http://dl.acm.org/citation.cfm?doid=3123266.3123275},
year = {2017}
}
@inproceedings{Liu2017_PG,
author = {Liu, Siqi and Zhu, Zhenhai and Ye, Ning and Guadarrama, Sergio and Murphy, Kevin},
booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2017.100},
file = {:home/mario/Documents/Mendeley/2017/Liu et al. - 2017 - Improved Image Captioning via Policy Gradient optimization of SPIDEr.pdf:pdf},
isbn = {978-1-5386-1032-9},
pages = {873--881},
publisher = {IEEE},
title = {{Improved Image Captioning via Policy Gradient optimization of SPIDEr}},
url = {http://ieeexplore.ieee.org/document/8237362/},
year = {2017}
}
@article{Wood2011,
author = {Wood, Frank and Gasthaus, Jan and Archambeau, C{\'{e}}dric and James, Lancelot and Teh, Yee Whye},
doi = {10.1145/1897816.1897842},
file = {:home/mario/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wood et al. - 2011 - The sequence memoizer.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
month = {feb},
number = {2},
pages = {91},
publisher = {ACM},
title = {{The sequence memoizer}},
url = {http://portal.acm.org/citation.cfm?doid=1897816.1897842},
volume = {54},
year = {2011}
}
@article{Reiter2009,
abstract = {There is growing interest in using automatically computed corpus-based evaluation metrics to evaluate Natural Language Generation (NLG) systems, because these are often considerably cheaper than the human-based evaluations which have traditionally been used in NLG. We review previous work on NLG evaluation and on validation of automatic metrics in NLP, and then present the results of two studies of how well some metrics which are popular in other areas of NLP (notably BLEU and ROUGE) correlate with human judgments in the domain of computer-generated weather forecasts. Our results suggest that, at least in this domain, metrics may provide a useful measure of language quality, although the evidence for this is not as strong as we would ideally like to see; however, they do not provide a useful measure of content quality. We also discuss a number of caveats which must be kept in mind when interpreting this and other validation studies.},
author = {Reiter, Ehud and Belz, Anja},
doi = {10.1162/coli.2009.35.4.35405},
file = {:home/mario/Documents/Mendeley/2009/Reiter, Belz - 2009 - An Investigation into the Validity of Some Metrics for Automatically Evaluating Natural Language Generation System.pdf:pdf},
issn = {0891-2017},
journal = {Computational Linguistics},
month = {dec},
number = {4},
pages = {529--558},
publisher = {MIT Press 238 Main St., Suite 500, Cambridge, MA 02142-1046USA journals-info@mit.edu},
title = {{An Investigation into the Validity of Some Metrics for Automatically Evaluating Natural Language Generation Systems}},
url = {http://www.mitpressjournals.org/doi/10.1162/coli.2009.35.4.35405},
volume = {35},
year = {2009}
}
@inproceedings{Gan2017_SCN,
author = {Gan, Zhe and Gan, Chuang and He, Xiaodong and Pu, Yunchen and Tran, Kenneth and Gao, Jianfeng and Carin, Lawrence and Deng, Li},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.127},
file = {:home/mario/Documents/Mendeley/2017/Gan et al. - 2017 - Semantic Compositional Networks for Visual Captioning.pdf:pdf},
isbn = {978-1-5386-0457-1},
month = {jul},
pages = {1141--1150},
publisher = {IEEE},
title = {{Semantic Compositional Networks for Visual Captioning}},
url = {http://ieeexplore.ieee.org/document/8099610/},
year = {2017}
}
@inproceedings{He2016a,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.90},
eprint = {1512.03385},
file = {:home/mario/Documents/Mendeley/2016/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:pdf},
isbn = {978-1-4673-8851-1},
month = {jun},
pages = {770--778},
publisher = {IEEE},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://ieeexplore.ieee.org/document/7780459/ http://image-net.org/challenges/LSVRC/2015/ http://arxiv.org/abs/1512.03385},
year = {2016}
}
@inproceedings{Chen2017_RLSTM,
abstract = {Image captioning is an important problem in artificial intelligence, related to both computer vision and natural language processing. There are two main problems in existing methods: in the training phase, it is difficult to find which parts of the captions are more essential to the image; in the caption generation phase, the objects or the scenes are sometimes misrecognized. In this paper , we consider the training images as the references and propose a Reference based Long Short Term Memory (R-LSTM) model, aiming to solve these two problems in one goal. When training the model, we assign different weights to different words, which enables the network to better learn the key information of the captions. When generating a caption, the consensus score is utilized to exploit the reference information of neighbor images, which might fix the misrecognition and make the descriptions more natural-sounding. The proposed R-LSTM model outperforms the state-of-the-art approaches on the benchmark dataset MS COCO and obtains top 2 position on 11 of the 14 metrics on the online test server.},
address = {San Francisco, USA},
author = {Chen, Minghai and Ding, Guiguang and Zhao, Sicheng and Chen, Hui and Han, Jungong and Liu, Qiang},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)},
file = {:home/mario/Documents/Mendeley/2017/Chen et al. - 2017 - Reference Based LSTM for Image Captioning.pdf:pdf},
publisher = {AAAI Press},
title = {{Reference Based LSTM for Image Captioning}},
url = {www.aaai.org},
year = {2017}
}
@inproceedings{Plummer2015,
abstract = {The Flickr30k dataset has become a standard benchmark for sentence-based image description. This paper presents Flickr30k Entities, which augments the 158k captions from Flickr30k with 244k coreference chains linking mentions of the same entities in images, as well as 276k manually annotated bounding boxes corresponding to each entity. Such annotation is essential for continued progress in automatic image description and grounded language understanding. We present experiments demonstrating the usefulness of our annotations for text-to-image reference resolution, or the task of localizing textual entity mentions in an image, and for bidirectional image-sentence retrieval. These experiments confirm that we can further improve the accuracy of state-of-the-art retrieval methods by training with explicit region-to-phrase correspondence, but at the same time, they show that accurately inferring this correspondence given an image and a caption remains really challenging.},
author = {Plummer, Bryan A. and Wang, Liwei and Cervantes, Chris M. and Caicedo, Juan C. and Hockenmaier, Julia and Lazebnik, Svetlana},
booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.303},
file = {:home/mario/Documents/Mendeley/2015/Plummer et al. - 2015 - Flickr30k Entities Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models.pdf:pdf},
isbn = {978-1-4673-8391-2},
month = {dec},
pages = {2641--2649},
publisher = {IEEE},
title = {{Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models}},
url = {http://ieeexplore.ieee.org/document/7410660/},
year = {2015}
}
@inproceedings{Zagoruyko2016,
author = {Zagoruyko, Sergey and Komodakis, Nikos},
booktitle = {Procedings of the British Machine Vision Conference 2016},
doi = {10.5244/C.30.87},
file = {:home/mario/Documents/Mendeley/2016/Zagoruyko, Komodakis - 2016 - Wide Residual Networks.pdf:pdf},
isbn = {1-901725-59-6},
pages = {87.1--87.12},
publisher = {British Machine Vision Association},
title = {{Wide Residual Networks}},
url = {http://www.bmva.org/bmvc/2016/papers/paper087/index.html},
year = {2016}
}
@article{Torralba2008,
author = {Torralba, A. and Fergus, R. and Freeman, W.T.},
doi = {10.1109/TPAMI.2008.128},
file = {:home/mario/Documents/Mendeley/2008/Torralba, Fergus, Freeman - 2008 - 80 Million Tiny Images A Large Data Set for Nonparametric Object and Scene Recognition.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = {nov},
number = {11},
pages = {1958--1970},
title = {{80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition}},
url = {http://ieeexplore.ieee.org/document/4531741/},
volume = {30},
year = {2008}
}
@inproceedings{Gupta2012,
abstract = {In this paper, we address the problem of automatically generating human-like descriptions for unseen images, given a collection of images and their corresponding human-generated descriptions. Previous attempts for this task mostly rely on visual clues and corpus statistics, but do not take much advantage of the semantic information inherent in the available image descriptions. Here, we present a generic method which benefits from all these three sources (i.e. visual clues, corpus statistics and available descriptions) simultaneously, and is capable of constructing novel descriptions. Our approach works on syntactically and linguistically motivated phrases extracted from the human descriptions. Experimental evaluations demonstrate that our formulation mostly generates lucid and semantically correct descriptions, and significantly outperforms the previous methods on automatic evaluation metrics. One of the significant advantages of our approach is that we can generate multiple interesting descriptions for an image. Unlike any previous work, we also test the applicability of our method on a large dataset containing complex images with rich descriptions.},
address = {Toronto, Ontario, Canada},
author = {Gupta, Ankush and Verma, Yashaswi and Jawahar, C.V. V.},
booktitle = {Proceedings of the Twenty-Sixth (AAAI) Conference on Artificial Intelligence},
file = {:home/mario/Documents/Mendeley/2012/Gupta, Verma, Jawahar - 2012 - Choosing linguistics over vision to describe images.pdf:pdf},
isbn = {9781577355687},
keywords = {Knowledge-Based Information Systems (Main Track)},
pages = {606--612},
publisher = {AAAI Press},
title = {{Choosing linguistics over vision to describe images}},
url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/viewFile/5039/5245},
year = {2012}
}
@inproceedings{Johnson2016,
abstract = {We introduce the dense captioning task, which requires a computer vision system to both localize and describe salient regions in images in natural language. The dense captioning task generalizes object detection when the descriptions consist of a single word, and Image Captioning when one predicted region covers the full image. To address the localization and description task jointly we propose a Fully Convolutional Localization Network (FCLN) architecture that processes an image with a single, efficient forward pass, requires no external regions proposals, and can be trained end-to-end with a single round of optimization. The architecture is composed of a Convolutional Network, a novel dense localization layer, and Recurrent Neural Network language model that generates the label sequences. We evaluate our network on the Visual Genome dataset, which comprises 94,000 images and 4,100,000 region-grounded captions. We observe both speed and accuracy improvements over baselines based on current state of the art approaches in both generation and retrieval settings.},
address = {Las Vegas, NV, USA},
author = {Johnson, Justin and Karpathy, Andrej and Fei-Fei, Li},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.494},
file = {:home/mario/Documents/Mendeley/2016/Johnson, Karpathy, Fei-Fei - 2016 - DenseCap Fully Convolutional Localization Networks for Dense Captioning.pdf:pdf},
isbn = {978-1-4673-8851-1},
month = {jun},
pages = {4565--4574},
publisher = {IEEE},
title = {{DenseCap: Fully Convolutional Localization Networks for Dense Captioning}},
url = {http://ieeexplore.ieee.org/document/7780863/},
year = {2016}
}
@misc{Chen2017_SemiSup,
abstract = {State-of-the-art approaches for image captioning require supervised training data consisting of captions with paired image data. These methods are typically unable to use un-supervised data such as textual data with no corresponding images, which is a much more abundant commodity. We here propose a novel way of using such textual data by artificially generating missing visual information. We evaluate this learning approach on a newly designed model that detects visual concepts present in an image and feed them to a reviewer-decoder architecture with an attention mechanism. Unlike previous approaches that encode visual concepts using word embeddings, we instead suggest using regional image features which capture more intrinsic information. The main benefit of this architecture is that it synthesizes meaningful thought vectors that capture salient image properties and then applies a soft attentive decoder to decode the thought vectors and generate image captions. We evaluate our model on both Microsoft COCO and Flickr30K datasets and demonstrate that this model combined with our semi-supervised learning method can largely improve performance and help the model to generate more accurate and diverse captions.},
archivePrefix = {arXiv},
arxivId = {1611.05321v3},
author = {Chen, Wenhu and Aachen, Rwth and Lucchi, Aurelien and Z{\"{u}}rich, Eth and Hofmann, Thomas},
eprint = {1611.05321v3},
file = {:home/mario/Documents/Mendeley/2017/Chen et al. - 2017 - A Semi-supervised Framework for Image Captioning.pdf:pdf},
title = {{A Semi-supervised Framework for Image Captioning}},
url = {https://github.com/wenhuchen/},
year = {2017}
}
@inproceedings{Fang2015,
abstract = {This paper presents a novel approach for automatically generating image descriptions: visual detectors, language models, and multimodal similarity models learnt directly from a dataset of image captions. We use multiple instance learning to train visual detectors for words that commonly occur in captions, including many different parts of speech such as nouns, verbs, and adjectives. The word detector outputs serve as conditional inputs to a maximum-entropy language model. The language model learns from a set of over 400,000 image descriptions to capture the statistics of word usage. We capture global semantics by re-ranking caption candidates using sentence-level features and a deep multimodal similarity model. Our system is state-of-the-art on the official Microsoft COCO benchmark, producing a BLEU-4 score of 29.1{\%}. When human judges compare the system captions to ones written by other people on our held-out test set, the system captions have equal or better quality 34{\%} of the time.},
archivePrefix = {arXiv},
arxivId = {1411.4952},
author = {Fang, Hao and Gupta, Saurabh and Iandola, Forrest and Srivastava, Rupesh K. and Deng, Li and Dollar, Piotr and Gao, Jianfeng and He, Xiaodong and Mitchell, Margaret and Platt, John C and Zitnick, C Lawrence and Zweig, Geoffrey},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2015.7298754},
eprint = {1411.4952},
file = {:home/mario/Documents/Mendeley/2015/Fang et al. - 2015 - From captions to visual concepts and back.pdf:pdf},
isbn = {978-1-4673-6964-0},
month = {jun},
pages = {1473--1482},
publisher = {IEEE},
title = {{From captions to visual concepts and back}},
url = {http://arxiv.org/abs/1411.4952 http://ieeexplore.ieee.org/document/7298754/},
year = {2015}
}
@incollection{Lindh2018,
address = {Rhodes, Greece},
author = {Lindh, Annika and Ross, Robert J and Mahalunkar, Abhijit and Salton, Giancarlo and Kelleher, John D.},
booktitle = {27th International Conference on Artificial Neural Networks (ICANN 2018)},
doi = {10.1007/978-3-030-01418-6_18},
file = {:home/mario/Documents/Mendeley/2018/Lindh et al. - 2018 - Generating Diverse and Meaningful Captions.pdf:pdf},
pages = {176--187},
publisher = {Springer},
title = {{Generating Diverse and Meaningful Captions}},
url = {https://doi.org/10.1007/978-3-030-01418-6{\_}18 http://link.springer.com/10.1007/978-3-030-01418-6{\_}18},
year = {2018}
}
@article{Escalante2010,
abstract = {In this paper, we describe an image collection created for the CLEF cross-language image retrieval track (ImageCLEF). This image retrieval benchmark (referred to as the IAPR TC-12 Benchmark) has developed from an initiative started by the Technical Committee 12 (TC-12) of the International Association of Pattern Recognition (IAPR). The collection consists of 20,000 images from a private photographic image collection. The construction and composition of the IAPR TC-12 Benchmark is described, including its associated text captions which are expressed in multiple languages, making the collection well-suited for evaluating the effectiveness of both text-based and visual retrieval methods. We also discuss the current and expected uses of the collection, including its use to benchmark and compare different image retrieval systems in ImageCLEF 2006.},
author = {Escalante, Hugo Jair and Hern{\'{a}}ndez, Carlos A. and Gonzalez, Jesus A. and L{\'{o}}pez-L{\'{o}}pez, A. and Montes, Manuel and Morales, Eduardo F. and {Enrique Sucar}, L. and Villase{\~{n}}or, Luis and Grubinger, Michael},
doi = {10.1016/j.cviu.2009.03.008},
file = {:home/mario/Documents/Mendeley/2010/Escalante et al. - 2010 - The segmented and annotated IAPR TC-12 benchmark.pdf:pdf},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {IAPTR-TC12},
mendeley-tags = {IAPTR-TC12},
month = {apr},
number = {4},
pages = {419--428},
publisher = {Elsevier},
title = {{The segmented and annotated IAPR TC-12 benchmark}},
url = {https://www.sciencedirect.com/science/article/pii/S1077314209000575},
volume = {114},
year = {2010}
}
@article{Plummer2017,
abstract = {The Flickr30k dataset has become a standard benchmark for sentence-based image description. This paper presents Flickr30k Entities, which augments the 158k captions from Flickr30k with 244k coreference chains, linking mentions of the same entities across different captions for the same image, and associating them with 276k manually annotated bounding boxes. Such annotations are essential for continued progress in automatic image description and grounded language understanding. They enable us to define a new benchmark for localization of textual entity mentions in an image. We present a strong baseline for this task that combines an image-text embedding, detectors for common objects, a color classifier, and a bias towards selecting larger objects. While our baseline rivals in accuracy more complex state-of-the-art models, we show that its gains cannot be easily parlayed into improvements on such tasks as image-sentence retrieval, thus underlining the limitations of current methods and the need for further research.},
archivePrefix = {arXiv},
arxivId = {1505.04870},
author = {Plummer, Bryan A. and Wang, Liwei and Cervantes, Chris M. and Caicedo, Juan C. and Hockenmaier, Julia and Lazebnik, Svetlana},
doi = {10.1007/s11263-016-0965-7},
eprint = {1505.04870},
file = {:home/mario/Documents/Mendeley/2017/Plummer et al. - 2017 - Flickr30k Entities Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models.pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Computer vision,Crowdsourcing,Datasets,Language,Region phrase correspondence},
number = {1},
pages = {74--93},
title = {{Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models}},
volume = {123},
year = {2017}
}
@inproceedings{You2016,
abstract = {Automatically generating a natural language description of an image has attracted interests recently both because of its importance in practical applications and because it connects two major artificial intelligence fields: computer vision and natural language processing. Existing approaches are either top-down, which start from a gist of an image and convert it into words, or bottom-up, which come up with words describing various aspects of an image and then combine them. In this paper, we propose a new algorithm that combines both approaches through a model of semantic attention. Our algorithm learns to selectively attend to semantic concept proposals and fuse them into hidden states and outputs of recurrent neural networks. The selection and fusion form a feedback connecting the top-down and bottom-up computation. We evaluate our algorithm on two public benchmarks: Microsoft COCO and Flickr30K. Experimental results show that our algorithm significantly outperforms the state-of-the-art approaches consistently across different evaluation metrics.},
author = {You, Quanzeng and Jin, Hailin and Wang, Zhaowen and Fang, Chen and Luo, Jiebo},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/mario/Documents/Mendeley/2016/You et al. - 2016 - Image Captioning with Semantic Attention.pdf:pdf},
isbn = {978-1-4673-8851-1},
month = {jun},
pages = {4651--4659},
publisher = {IEEE},
title = {{Image Captioning with Semantic Attention}},
url = {http://ieeexplore.ieee.org/document/7780872/},
year = {2016}
}
@inproceedings{Huang2017,
abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convo-lutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections-one between each layer and its subsequent layer-our network has L(L+1) 2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and models are available at https://github.com/liuzhuang13/ DenseNet.},
archivePrefix = {arXiv},
arxivId = {1608.06993v3},
author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.243},
eprint = {1608.06993v3},
file = {:home/mario/Documents/Mendeley/2017/Huang et al. - 2017 - Densely Connected Convolutional Networks.pdf:pdf},
isbn = {978-1-5386-0457-1},
month = {jul},
pages = {2261--2269},
publisher = {IEEE},
title = {{Densely Connected Convolutional Networks}},
url = {https://github.com/liuzhuang13/ http://ieeexplore.ieee.org/document/8099726/},
year = {2017}
}
@inproceedings{Jegou2017,
abstract = {State-of-the-art approaches for semantic image segmen-tation are built on Convolutional Neural Networks (CNNs). The typical segmentation architecture is composed of (a) a downsampling path responsible for extracting coarse semantic features, followed by (b) an upsampling path trained to recover the input image resolution at the output of the model and, optionally, (c) a post-processing module (e.g. Conditional Random Fields) to refine the model predictions. Recently, a new CNN architecture, Densely Connected Convolutional Networks (DenseNets), has shown excellent results on image classification tasks. The idea of DenseNets is based on the observation that if each layer is directly connected to every other layer in a feed-forward fashion then the network will be more accurate and easier to train. In this paper, we extend DenseNets to deal with the problem of semantic segmentation. We achieve state-of-the-art results on urban scene benchmark datasets such as CamVid and Gatech 1 , without any further post-processing module nor pretraining. Moreover, due to smart construction of the model, our approach has much less parameters than currently published best entries for these datasets.},
archivePrefix = {arXiv},
arxivId = {1611.09326v2},
author = {Jegou, Simon and Drozdzal, Michal and Vazquez, David and Romero, Adriana and Bengio, Yoshua},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
doi = {10.1109/CVPRW.2017.156},
eprint = {1611.09326v2},
file = {:home/mario/Documents/Mendeley/2017/Jegou et al. - 2017 - The One Hundred Layers Tiramisu Fully Convolutional DenseNets for Semantic Segmentation.pdf:pdf},
isbn = {978-1-5386-0733-6},
keywords = {DenseNet,Semantic Segmentation},
mendeley-tags = {DenseNet,Semantic Segmentation},
month = {jul},
pages = {1175--1183},
publisher = {IEEE},
title = {{The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation}},
url = {https://arxiv.org/pdf/1611.09326v2.pdf http://ieeexplore.ieee.org/document/8014890/},
year = {2017}
}
@inproceedings{Mun2017,
abstract = {Visual attention plays an important role to understand images and demonstrates its effectiveness in generating natural language descriptions of images. On the other hand, recent studies show that language associated with an image can steer visual attention in the scene during our cognitive process. Inspired by this, we introduce a text-guided attention model for image captioning, which learns to drive visual attention using associated captions. For this model, we propose an exemplar-based learning approach that retrieves from training data associated captions with each image, and use them to learn attention on visual features. Our attention model enables to describe a detailed state of scenes by distinguishing small or confusable objects effectively. We validate our model on MS-COCO Captioning benchmark and achieve the state-of-the-art performance in standard metrics.},
address = {San Francisco, USA},
archivePrefix = {arXiv},
arxivId = {1612.03557},
author = {Mun, Jonghwan and Cho, Minsu and Han, Bohyung},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)},
eprint = {1612.03557},
file = {:home/mario/Documents/Mendeley/2017/Mun, Cho, Han - 2017 - Text-guided Attention Model for Image Captioning.pdf:pdf},
keywords = {Vision},
pages = {4233--4239},
title = {{Text-guided Attention Model for Image Captioning}},
url = {http://arxiv.org/abs/1612.03557},
year = {2017}
}
@article{Everingham2010,
author = {Everingham, Mark and {Van Gool}, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
doi = {10.1007/s11263-009-0275-4},
file = {:home/mario/Documents/Mendeley/2010/Everingham et al. - 2010 - The Pascal Visual Object Classes (VOC) Challenge.pdf:pdf},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
month = {jun},
number = {2},
pages = {303--338},
publisher = {Springer US},
title = {{The Pascal Visual Object Classes (VOC) Challenge}},
url = {http://link.springer.com/10.1007/s11263-009-0275-4},
volume = {88},
year = {2010}
}
@inproceedings{Jing2018,
address = {Melbourne, Australia},
author = {Jing, Baoyu and Xie, Pengtao and Xing, Eric P},
booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
doi = {10.18653/v1/P18-1240},
file = {:home/mario/Documents/Mendeley/2018/Jing, Xie, Xing - 2018 - On the Automatic Generation of Medical Imaging Reports.pdf:pdf},
pages = {2577--2586},
title = {{On the Automatic Generation of Medical Imaging Reports}},
year = {2018}
}
@inproceedings{Papineni2002,
address = {Philadelphia, USA},
author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
booktitle = {Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics},
doi = {10.3115/1073083.1073135},
file = {:home/mario/Documents/Mendeley/2002/Papineni et al. - 2002 - BLEU a Method for Automatic Evaluation of Machine Translation.pdf:pdf},
pages = {311},
publisher = {Association for Computational Linguistics},
title = {{BLEU: a Method for Automatic Evaluation of Machine Translation}},
url = {http://portal.acm.org/citation.cfm?doid=1073083.1073135},
year = {2002}
}
@article{Socher2014,
abstract = {Previous work on Recursive Neural Networks (RNNs) shows that these models can produce compositional feature vectors for accurately representing and classifying sentences or images. However, the sentence vectors of previous models cannot accurately represent visually grounded meaning. We introduce the DT-RNN model which uses dependency trees to embed sentences into a vector space in order to retrieve images that are described by those sentences. Unlike previous RNN-based models which use constituency trees, DT-RNNs naturally focus on the action and agents in a sentence. They are better able to abstract from the details of word order and syntactic expression. DT-RNNs outperform other recursive and recurrent neural networks, kernelized CCA and a bag-of-words baseline on the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image.},
author = {Socher, Richard and Karpathy, Andrej and Le, Quoc V. and Manning, Christopher D. and Ng, Andrew Y.},
doi = {10.1162/tacl_a_00177},
file = {:home/mario/Documents/Mendeley/2014/Socher et al. - 2014 - Grounded Compositional Semantics for Finding and Describing Images with Sentences.pdf:pdf},
isbn = {2307-387X},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics},
month = {dec},
pages = {207--218},
title = {{Grounded Compositional Semantics for Finding and Describing Images with Sentences}},
url = {https://www.mitpressjournals.org/doi/abs/10.1162/tacl{\_}a{\_}00177},
volume = {2},
year = {2014}
}
@article{Hossain2019,
abstract = {Generating a description of an image is called image captioning. Image captioning requires to recognize the important objects, their attributes and their relationships in an image. It also needs to generate syntactically and semantically correct sentences. Deep learning-based techniques are capable of handling the complexities and challenges of image captioning. In this survey paper, we aim to present a comprehensive review of existing deep learning-based image captioning techniques. We discuss the foundation of the techniques to analyze their performances, strengths and limitations. We also discuss the datasets and the evaluation metrics popularly used in deep learning based automatic image captioning.},
archivePrefix = {arXiv},
arxivId = {1810.04020v2},
author = {Hossain, MD. Zakir and Sohel, Ferdous and Shiratuddin, Mohd Fairuz and Laga, Hamid and Hossain, Zakir and Sohel, Ferdous and Shiratuddin, Mohd Fairuz and Laga, Hamid},
doi = {10.1145/3295748},
eprint = {1810.04020v2},
file = {:home/mario/Documents/Mendeley/2019/Hossain et al. - 2019 - A Comprehensive Survey of Deep Learning for Image Captioning.pdf:pdf},
issn = {03600300},
journal = {ACM Computing Surveys},
keywords = {Additional Key Words and Phrases: Image Captioning,CCS Concepts: • Computing methodologies → Machine,CNN,Computer Vision,Deep Learning,LSTM,Natural Language Processing,Neural networks,Reinforcement learning,Supervised learning,Unsupervised learning},
month = {feb},
number = {6},
pages = {1--36},
title = {{A Comprehensive Survey of Deep Learning for Image Captioning}},
url = {http://dl.acm.org/citation.cfm?doid=3303862.3295748},
volume = {51},
year = {2019}
}
@inproceedings{Berg2010,
abstract = {It is common to use domain specific terminology-attributes-to describe the visual appearance of objects. In order to scale the use of these describable visual attributes to a large number of categories, especially those not well studied by psychologists or linguists, it will be necessary to find alternative techniques for identifying attribute vocabularies and for learning to recognize attributes without hand labeled training data. We demonstrate that it is possible to accomplish both these tasks automatically by mining text and image data sampled from the Internet. The proposed approach also characterizes attributes according to their visual representation: global or local, and type: color, texture, or shape. This work focuses on discovering attributes and their visual appearance, and is as agnostic as possible about the textual description.},
author = {Berg, Tamara L and Berg, Alexander C and Shih, Jonathan},
booktitle = {Proceedings of the 9th European Conference on Computer Vision (ECCV)},
file = {:home/mario/Documents/Mendeley/2010/Berg, Berg, Shih - 2010 - Automatic Attribute Discovery and Characterization from Noisy Web Data.pdf:pdf},
title = {{Automatic Attribute Discovery and Characterization from Noisy Web Data}},
url = {http://tamaraberg.com/papers/attributediscovery.pdf},
year = {2010}
}
@inproceedings{Elliott2013,
abstract = {Describing the main event of an image involves identifying the objects depicted and predicting the relationships between them. Previous approaches have represented images as unstructured bags of regions, which makes it difficult to accurately predict meaningful relationships between regions. In this paper , we introduce visual dependency representations to capture the relationships between the objects in an image, and hypothesize that this representation can improve image description. We test this hypothesis using a new data set of region-annotated images, associated with visual dependency representations and gold-standard descriptions. We describe two template-based description generation models that operate over visual dependency representations. In an image description task, we find that these models outper-form approaches that rely on object proximity or corpus information to generate descriptions on both automatic measures and on human judgements.},
address = {Seatle, USA},
author = {Elliott, Desmond and Keller, Frank},
booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013)},
file = {:home/mario/Documents/Mendeley/2013/Elliott, Keller - 2013 - Image Description using Visual Dependency Representations.pdf:pdf},
keywords = {VLT2K},
mendeley-tags = {VLT2K},
pages = {1292--1302},
publisher = {Association for Computational Linguistics},
title = {{Image Description using Visual Dependency Representations}},
url = {https://pdfs.semanticscholar.org/88e2/0b2a0d079002ae8d1baed6f1208b93dbfd73.pdf},
year = {2013}
}
@incollection{Srivastava2018,
author = {Srivastava, Gargi and Srivastava, Rajeev},
booktitle = {Communications in Computer and Information Science},
doi = {10.1007/978-981-13-0023-3_8},
file = {:home/mario/Documents/Mendeley/2018/Srivastava, Srivastava - 2018 - A Survey on Automatic Image Captioning.pdf:pdf},
isbn = {9789811300226},
issn = {18650929},
keywords = {Computer vision,Image captioning,Scene analysis},
pages = {74--83},
title = {{A Survey on Automatic Image Captioning}},
url = {http://link.springer.com/10.1007/978-981-13-0023-3{\_}8},
volume = {834},
year = {2018}
}
@inproceedings{Sutton1999,
abstract = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy. Large applications of reinforcement learning (RL) require the use of generalizing function approximators such neural networks, decision-trees, or instance-based methods. The dominant approach for the last decade has been the value-function approach, in which all function approximation effort goes into estimating a value function, with the action-selection policy represented implicitly as the "greedy" policy with respect to the estimated values (e.g., as the policy that selects in each state the action with highest estimated value). The value-function approach has worked well in many applications , but has several limitations. First, it is oriented toward finding deterministic policies, whereas the optimal policy is often stochastic, selecting different actions with specific probabilities (e.g., see Singh, Jaakkola, and Jordan, 1994). Second, an arbitrarily small change in the estimated value of an action can cause it to be, or not be, selected. Such discontinuous changes have been identified as a key obstacle to establishing convergence assurances for algorithms following the value-function approach (Bertsekas and Tsitsiklis, 1996). For example, Q-Iearning, Sarsa, and dynamic programming methods have all been shown unable to converge to any policy for simple MDPs and simple function approximators (Gordon, 1995, 1996; Baird, 1995; Tsit-siklis and van Roy, 1996; Bertsekas and Tsitsiklis, 1996). This can occur even if the best approximation is found at each step before changing the policy, and whether the notion of "best" is in the mean-squared-error sense or the slightly different senses of residual-gradient, temporal-difference, and dynamic-programming methods. In this paper we explore an alternative approach to function approximation in RL.},
address = {Denver, USA},
author = {Sutton, Richard S and Mcallester, David and Singh, Satinder and Mansour, Yishay},
booktitle = {Proceedings of the 12th International Conference on Neural Information Processing Systems (NIPS'99)},
pages = {1057--1063},
publisher = {MIT Press},
title = {{Policy Gradient Methods for Reinforcement Learning with Function Approximation}},
url = {https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf},
year = {1999}
}
@article{Zhang2019,
author = {Zhang, Mingxing and Yang, Yang and Zhang, Hanwang and Ji, Yanli and Shen, Heng Tao and Chua, Tat-Seng},
doi = {10.1109/TIP.2018.2855415},
file = {:home/mario/Documents/Mendeley/2019/Zhang et al. - 2019 - More is Better Precise and Detailed Image Captioning Using Online Positive Recall and Missing Concepts Mining.pdf:pdf},
issn = {1057-7149},
journal = {IEEE Transactions on Image Processing},
month = {jan},
number = {1},
pages = {32--44},
title = {{More is Better: Precise and Detailed Image Captioning Using Online Positive Recall and Missing Concepts Mining}},
url = {https://ieeexplore.ieee.org/document/8410582/},
volume = {28},
year = {2019}
}
@inproceedings{Yu2017,
abstract = {As a new way of training generative models, Generative Adversarial Nets (GAN) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real-valued data. However, it has limitations when the goal is for generating sequences of discrete tokens. A major reason lies in that the discrete outputs from the generative model make it difficult to pass the gradient update from the discriminative model to the generative model. Also, the discriminative model can only assess a complete sequence, while for a partially generated sequence, it is non-trivial to balance its current score and the future one once the entire sequence has been generated. In this paper, we propose a sequence generation framework, called SeqGAN, to solve the problems. Modeling the data generator as a stochastic policy in reinforcement learning (RL), SeqGAN bypasses the generator differentiation problem by directly performing gradient policy update. The RL reward signal comes from the GAN discriminator judged on a complete sequence, and is passed back to the intermediate state-action steps using Monte Carlo search. Extensive experiments on synthetic data and real-world tasks demonstrate significant improvements over strong baselines.},
address = {San Francisco, USA},
archivePrefix = {arXiv},
arxivId = {1609.05473},
author = {Yu, Lantao and Zhang, Weinan and Wang, Jun and Yu, Yong},
booktitle = {The 31st AAAI Conference on Artificial Intelligence (AAAI 2017)},
eprint = {1609.05473},
file = {:home/mario/Documents/Mendeley/2017/Yu et al. - 2017 - SeqGAN Sequence Generative Adversarial Nets with Policy Gradient.pdf:pdf},
month = {feb},
title = {{SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient}},
url = {http://arxiv.org/abs/1609.05473},
year = {2017}
}
@inproceedings{Dai2017_CGAN,
abstract = {Despite the substantial progress in recent years, the image captioning techniques are still far from being perfect. Sentences produced by existing methods, e.g. those based on RNNs, are often overly rigid and lacking in variability. This issue is related to a learning principle widely used in practice, that is, to maximize the likelihood of training samples. This principle encourages high resemblance to the "ground-truth" captions, while suppressing other reasonable descriptions. Conventional evaluation metrics, e.g. BLEU and METEOR, also favor such restrictive methods. In this paper, we explore an alternative approach, with the aim to improve the naturalness and diversity-two essential properties of human expression. Specifically, we propose a new framework based on Conditional Generative Adversarial Networks (CGAN), which jointly learns a generator to produce descriptions conditioned on images and an evaluator to assess how well a description fits the visual content. It is noteworthy that training a sequence generator is nontrivial. We overcome the difficulty by Policy Gradient, a strategy stemming from Reinforcement Learning, which allows the generator to receive early feedback along the way. We tested our method on two large datasets, where it performed competitively against real people in our user study and outperformed other methods on various tasks.},
address = {Venice, Italy},
author = {Dai, Bo and Fidler, Sanja and Urtasun, Raquel and Lin, Dahua},
booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2017.323},
file = {:home/mario/Documents/Mendeley/2017/Dai et al. - 2017 - Towards Diverse and Natural Image Descriptions via a Conditional GAN.pdf:pdf},
isbn = {978-1-5386-1032-9},
month = {oct},
pages = {2989--2998},
publisher = {IEEE},
title = {{Towards Diverse and Natural Image Descriptions via a Conditional GAN}},
url = {https://www.cs.toronto.edu/{~}urtasun/publications/dai{\_}etal{\_}iccv17.pdf http://ieeexplore.ieee.org/document/8237585/},
year = {2017}
}
@inproceedings{Khademi2018,
address = {Salt Lake City, USA},
author = {Khademi, Mahmoud and Schulte, Oliver},
booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
doi = {10.1109/CVPRW.2018.00260},
file = {:home/mario/Documents/Mendeley/2018/Khademi, Schulte - 2018 - Image Caption Generation with Hierarchical Contextual Visual Spatial Attention.pdf:pdf},
isbn = {978-1-5386-6100-0},
month = {jun},
pages = {2024--20248},
publisher = {IEEE},
title = {{Image Caption Generation with Hierarchical Contextual Visual Spatial Attention}},
url = {https://ieeexplore.ieee.org/document/8575427/},
year = {2018}
}
@inproceedings{Kalchbrenner2014,
address = {Stroudsburg, PA, USA},
author = {Kalchbrenner, Nal and Grefenstette, Edward and Blunsom, Phil},
booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics},
doi = {10.3115/v1/P14-1062},
file = {:home/mario/Documents/Mendeley/2014/Kalchbrenner, Grefenstette, Blunsom - 2014 - A Convolutional Neural Network for Modelling Sentences.pdf:pdf},
pages = {655--665},
publisher = {Association for Computational Linguistics},
title = {{A Convolutional Neural Network for Modelling Sentences}},
url = {http://aclweb.org/anthology/P14-1062},
year = {2014}
}
@inproceedings{Aneja2018,
abstract = {Image captioning is an important task, applicable to virtual assistants, editing tools, image indexing, and support of the disabled. In recent years significant progress has been made in image captioning, using Recurrent Neu-ral Networks powered by long-short-term-memory (LSTM) units. Despite mitigating the vanishing gradient problem, and despite their compelling ability to memorize dependencies , LSTM units are complex and inherently sequential across time. To address this issue, recent work has shown benefits of convolutional networks for machine translation and conditional image generation [9, 34, 35]. Inspired by their success, in this paper, we develop a convolutional image captioning technique. We demonstrate its efficacy on the challenging MSCOCO dataset and demonstrate performance on par with the LSTM baseline [16], while having a faster training time per number of parameters. We also perform a detailed analysis, providing compelling reasons in favor of convolutional language generation approaches.},
address = {Salt Lake City, USA},
author = {Aneja, Jyoti and Deshpande, Aditya and Schwing, Alexander G},
booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2018.00583},
file = {:home/mario/Documents/Mendeley/2018/Aneja, Deshpande, Schwing - 2018 - Convolutional Image Captioning.pdf:pdf},
isbn = {978-1-5386-6420-9},
month = {jun},
pages = {5561--5570},
publisher = {IEEE},
title = {{Convolutional Image Captioning}},
url = {http://openaccess.thecvf.com/content{\_}cvpr{\_}2018/papers/Aneja{\_}Convolutional{\_}Image{\_}Captioning{\_}CVPR{\_}2018{\_}paper.pdf https://ieeexplore.ieee.org/document/8578681/},
year = {2018}
}
@inproceedings{Tanti2017,
abstract = {In neural image captioning systems, a recurrent neural network (RNN) is typically viewed as the primary `generation' component. This view suggests that the image features should be `injected' into the RNN. This is in fact the dominant view in the literature. Alternatively, the RNN can instead be viewed as only encoding the previously generated words. This view suggests that the RNN should only be used to encode linguistic features and that only the final representation should be `merged' with the image features at a later stage. This paper compares these two architectures. We find that, in general, late merging outperforms injection, suggesting that RNNs are better viewed as encoders, rather than generators.},
address = {Stroudsburg, PA, USA},
archivePrefix = {arXiv},
arxivId = {1708.02043},
author = {Tanti, Marc and Gatt, Albert and Camilleri, Kenneth},
booktitle = {Proceedings of the 10th International Conference on Natural Language Generation},
doi = {10.18653/v1/W17-3506},
eprint = {1708.02043},
file = {:home/mario/Documents/Mendeley/2017/Tanti, Gatt, Camilleri - 2017 - What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator.pdf:pdf},
month = {aug},
pages = {51--60},
publisher = {Association for Computational Linguistics},
title = {{What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator?}},
url = {http://arxiv.org/abs/1708.02043 http://aclweb.org/anthology/W17-3506},
year = {2017}
}
@inproceedings{Vedantam2015,
author = {Vedantam, Ramakrishna and Zitnick, C. Lawrence and Parikh, Devi},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2015.7299087},
file = {:home/mario/Documents/Mendeley/2015/Vedantam, Zitnick, Parikh - 2015 - CIDEr Consensus-based image description evaluation.pdf:pdf},
isbn = {978-1-4673-6964-0},
month = {jun},
pages = {4566--4575},
publisher = {IEEE},
title = {{CIDEr: Consensus-based image description evaluation}},
url = {http://ieeexplore.ieee.org/document/7299087/},
year = {2015}
}
@article{Bernardi2017,
abstract = {Automatic description generation from natural images is a challenging problem that has recently received a large amount of interest from the computer vision and natural language processing communities. In this survey, we classify the existing approaches based on how they conceptualize this problem, viz., models that cast description as either generation problem or as a retrieval problem over a visual or multimodal representational space. We provide a detailed review of existing models, highlighting their advantages and disadvantages. Moreover, we give an overview of the benchmark image datasets and the evaluation measures that have been developed to assess the quality of machine-generated image descriptions. Finally we extrapolate future directions in the area of automatic image description generation.},
archivePrefix = {arXiv},
arxivId = {1601.03896},
author = {Bernardi, Raffaella and {\c{C}}akici, Ruket and Elliott, Desmond and Erdem, Aykut and Erdem, Erkut and Ikizler-Cinbis, Nazli and Keller, Frank and Muscat, Adrian and Plank, Barbara},
doi = {10.1613/jair.4900},
eprint = {1601.03896},
file = {:home/mario/Documents/Mendeley/2017/Bernardi et al. - 2017 - Automatic description generation from images A survey of models, datasets, and evaluation measures.pdf:pdf},
isbn = {9780999241103},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
pages = {4970--4974},
pmid = {11329188},
title = {{Automatic description generation from images: A survey of models, datasets, and evaluation measures}},
volume = {55},
year = {2017}
}
@inproceedings{Pu2016_VAE,
abstract = {A novel variational autoencoder is developed to model images, as well as associated labels or captions. The Deep Generative Deconvolutional Network (DGDN) is used as a decoder of the latent image features, and a deep Convolutional Neural Network (CNN) is used as an image encoder; the CNN is used to approximate a distribution for the latent DGDN features/code. The latent code is also linked to generative models for labels (Bayesian support vector machine) or captions (recurrent neural network). When predicting a label/caption for a new image at test, averaging is performed across the distribution of latent codes; this is computationally efficient as a consequence of the learned CNN-based encoder. Since the framework is capable of modeling the image in the presence/absence of associated labels/captions, a new semi-supervised setting is manifested for CNN learning with images; the framework even allows unsupervised CNN learning, based on images alone.},
address = {Barcelona, Spain},
archivePrefix = {arXiv},
arxivId = {arXiv:1609.08976v1},
author = {Pu, Yunchen and Gan, Zhe and Henao, Ricardo and Yuan, Xin and Li, Chunyuan and Stevens, Andrew and Carin, Lawrence},
booktitle = {NIPS'16 Proceedings of the 30th International Conference on Neural Information Processing Systems},
eprint = {arXiv:1609.08976v1},
file = {:home/mario/Documents/Mendeley/2016/Pu et al. - 2016 - Variational Autoencoder for Deep Learning of Images , Labels and Captions.pdf:pdf},
pages = {2360--2368},
title = {{Variational Autoencoder for Deep Learning of Images , Labels and Captions}},
year = {2016}
}
@inproceedings{Donahue2015,
abstract = {Models based on deep convolutional networks have dom- inated recent image interpretation tasks; we investigate whether models which are also recurrent, or “temporally deep”, are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image de- scription and retrieval problems, and video narration chal- lenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averag- ing for sequential processing, recurrent convolutional mod- els are “doubly deep” in that they can be compositional in spatial and temporal “layers”. Such models may have advantages when target concepts are complex and/or train- ing data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the net- work state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural lan- guage text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual rep- resentations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized. 1. Introduction Recognition and description of images and videos is a fundamental challenge of computer vision. Dramatic Visual},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.4389v3},
author = {Donahue, Jeff and Hendricks, Lisa Anne and Guadarrama, Sergio and Rohrbach, Marcus and Venugopalan, Subhashini and Darrell, Trevor and Saenko, Kate},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2015.7298878},
eprint = {arXiv:1411.4389v3},
file = {:home/mario/Documents/Mendeley/2015/Donahue et al. - 2015 - Long-term recurrent convolutional networks for visual recognition and description.pdf:pdf},
isbn = {978-1-4673-6964-0},
pages = {2625--2634},
publisher = {IEEE},
title = {{Long-term recurrent convolutional networks for visual recognition and description}},
url = {http://ieeexplore.ieee.org/document/7298878/},
year = {2015}
}
@article{Fu2017,
abstract = {Recent progress on automatic generation of image captions has shown that it is possible to describe the most salient information conveyed by images with accurate and meaningful sentences. In this paper, we propose an image caption system that exploits the parallel structures between images and sentences. In our model, the process of generating the next word, given the previously generated ones, is aligned with the visual perception experience where the attention shifting among the visual regions imposes a thread of visual ordering. This alignment characterizes the flow of "abstract meaning", encoding what is semantically shared by both the visual scene and the text description. Our system also makes another novel modeling contribution by introducing scene-specific contexts that capture higher-level semantic information encoded in an image. The contexts adapt language models for word generation to specific scene types. We benchmark our system and contrast to published results on several popular datasets. We show that using either region-based attention or scene-specific contexts improves systems without those components. Furthermore, combining these two modeling ingredients attains the state-of-the-art performance.},
archivePrefix = {arXiv},
arxivId = {1506.06272},
author = {Fu, Kun and Jin, Junqi and Cui, Runpeng and Sha, Fei and Zhang, Changshui},
doi = {10.1109/TPAMI.2016.2642953},
eprint = {1506.06272},
file = {:home/mario/Documents/Mendeley/2017/Fu et al. - 2017 - Aligning Where to See and What to Tell Image Captioning with Region-Based Attention and Scene-Specific Contexts.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = {dec},
number = {12},
pages = {2321--2334},
title = {{Aligning Where to See and What to Tell: Image Captioning with Region-Based Attention and Scene-Specific Contexts}},
url = {http://ieeexplore.ieee.org/document/7792748/ http://arxiv.org/abs/1506.06272},
volume = {39},
year = {2017}
}
@inproceedings{Rennie2017,
abstract = {Recently it has been shown that policy-gradient methods for reinforcement learning can be utilized to train deep end-to-end systems directly on non-differentiable metrics for the task at hand. In this paper we consider the problem of optimizing image captioning systems using reinforcement learning , and show that by carefully optimizing our systems using the test metrics of the MSCOCO task, significant gains in performance can be realized. Our systems are built using a new optimization approach that we call self-critical sequence training (SCST). SCST is a form of the popular REINFORCE algorithm that, rather than estimating a "base-line" to normalize the rewards and reduce variance, utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences. Using this approach, estimating the reward signal (as actor-critic methods must do) and estimating normalization (as REINFORCE algorithms typically do) is avoided, while at the same time harmonizing the model with respect to its test-time inference procedure. Empirically we find that directly optimizing the CIDEr metric with SCST and greedy decoding at test-time is highly effective. Our results on the MSCOCO evaluation sever establish a new state-of-the-art on the task, improving the best result in terms of CIDEr from 104.9 to 114.7.},
address = {Honolulu, USA},
author = {Rennie, Steven J and Marcheret, Etienne and Mroueh, Youssef and Ross, Jerret and Goel, Vaibhava},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.131},
file = {:home/mario/Documents/Mendeley/2017/Rennie et al. - 2017 - Self-Critical Sequence Training for Image Captioning.pdf:pdf},
isbn = {978-1-5386-0457-1},
month = {jul},
pages = {1179--1195},
publisher = {IEEE},
title = {{Self-Critical Sequence Training for Image Captioning}},
url = {http://openaccess.thecvf.com/content{\_}cvpr{\_}2017/papers/Rennie{\_}Self-Critical{\_}Sequence{\_}Training{\_}CVPR{\_}2017{\_}paper.pdf http://ieeexplore.ieee.org/document/8099614/},
year = {2017}
}
@inproceedings{Karpathy2014,
abstract = {We introduce a model for bidirectional retrieval of images and sentences through a deep, multi-modal embedding of visual and natural language data. Unlike pre- vious models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (ob- jects) and fragments of sentences (typed dependency tree relations) into a com- mon space. We then introduce a structured max-margin objective that allows our model to explicitly associate these fragments across modalities. Extensive exper- imental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions for the image-sentence retrieval task since the inferred inter-modal alignment of fragments is explicit.},
author = {Karpathy, Andrej and Joulin, Armand and Fei-Fei, Li},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS 2014)},
file = {:home/mario/Documents/Mendeley/2014/Karpathy, Joulin, Fei-Fei - 2014 - Deep Fragment Embeddings for Bidirectional Image Sentence Mapping.pdf:pdf},
pages = {1889--1897},
publisher = {MIT Press},
title = {{Deep Fragment Embeddings for Bidirectional Image Sentence Mapping}},
url = {https://dl.acm.org/citation.cfm?id=2969038 https://arxiv.org/abs/1406.5679},
year = {2014}
}
@inproceedings{Gong2014,
address = {Zurich, Switzerland},
author = {Gong, Yunchao and Wang, Liwei and Hodosh, Micah and Hockenmaier, Julia and Lazebnik, Svetlana},
booktitle = {Proceedings of the 13th European Conference on Computer Vision (ECCV)},
doi = {10.1007/978-3-319-10593-2_35},
file = {:home/mario/Documents/Mendeley/2014/Gong et al. - 2014 - Improving Image-Sentence Embeddings Using Large Weakly Annotated Photo Collections.pdf:pdf},
pages = {529--545},
publisher = {Springer},
title = {{Improving Image-Sentence Embeddings Using Large Weakly Annotated Photo Collections}},
url = {http://link.springer.com/10.1007/978-3-319-10593-2{\_}35},
year = {2014}
}
@inproceedings{Viola2005,
abstract = {A good image object detection algorithm is accurate, fast, and does not require exact locations of objects in a training set. We can create such an object detector by taking the architecture of the Viola-Jones detector cascade and training it with a new variant of boosting that we call MIL-Boost. MILBoost uses cost functions from the Multiple Instance Learning literature combined with the AnyBoost framework. We adapt the feature selection criterion of MILBoost to optimize the performance of the Viola-Jones cascade. Experiments show that the detection rate is up to 1.6 times better using MILBoost. This increased detection rate shows the advantage of simultaneously learning the locations and scales of the objects in the training set along with the parameters of the classifier.},
address = {Vancouver, Canada},
author = {Viola, Paul and Platt, John C and Zhang, Cha},
booktitle = {Advances in Neural Information Processing Systems 18 (NIPS 2005)},
file = {:home/mario/Documents/Mendeley/2005/Viola, Platt, Zhang - 2005 - Multiple Instance Boosting for Object Detection.pdf:pdf},
pages = {1419--1426},
publisher = {MIT Press},
title = {{Multiple Instance Boosting for Object Detection}},
url = {https://papers.nips.cc/paper/2926-multiple-instance-boosting-for-object-detection.pdf},
year = {2005}
}
@inproceedings{Li2018_CAL,
abstract = {We study how to generate captions that are not only accurate in describing an image but also diverse across different images. The problem is both fundamental and interesting, as most machine-generated captions, despite phenomenal research progresses in the past several years, are expressed in a very monotonic and featureless format. While such captions are normally accurate, they often lack important characteristics in human languages-distinctiveness for each image and diversity across different images. To address this problem, we propose a novel conditional generative adversarial network for generating diverse captions across images. Instead of estimating the quality of a caption solely on one image, the proposed comparative adversarial learning framework better assesses the quality of captions by comparing a set of captions within the image-caption joint space. By contrasting with human-written captions and image-mismatched captions, the caption generator effectively exploits the inherent characteristics of human languages, and generates more diverse captions. We show that our proposed network is capable of producing accurate and diverse captions across images.},
address = {Montreal, Canada},
author = {Li, Dianqi and Huang, Qiuyuan and He, Xiaodong and Zhang, Lei and Sun, Ming-Ting},
booktitle = {32nd Conference on Neural Information Processing Systems (NIPS 2018)},
file = {:home/mario/Documents/Mendeley/2018/Li et al. - 2018 - Generating Diverse and Accurate Visual Captions by Comparative Adversarial Learning.pdf:pdf},
title = {{Generating Diverse and Accurate Visual Captions by Comparative Adversarial Learning}},
url = {https://nips2018vigil.github.io/static/papers/accepted/27.pdf},
year = {2018}
}
@inproceedings{DeMarneffe2006,
abstract = {This paper describes a system for extracting typed dependency parses of English sentences from phrase structure parses. In order to capture inherent relations occurring in corpus texts that can be critical in real-world applications, many NP relations are included in the set of grammatical relations used. We provide a comparison of our system with Minipar and the Link parser. The typed dependency extraction facility described here is integrated in the Stanford Parser, available for download.},
author = {{De Marneffe}, Marie-Catherine and MacCartney, Bill and Manning, Christopher D},
booktitle = {Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC)},
file = {:home/mario/Documents/Mendeley/2006/De Marneffe, MacCartney, Manning - 2006 - Generating Typed Dependency Parses from Phrase Structure Parses.pdf:pdf},
title = {{Generating Typed Dependency Parses from Phrase Structure Parses}},
url = {https://nlp.stanford.edu/{~}wcmac/papers/td-lrec06.pdf https://nlp.stanford.edu/pubs/LREC06{\_}dependencies.pdf},
year = {2006}
}
@misc{Sugano2016,
abstract = {Gaze reflects how humans process visual scenes and is therefore increasingly used in computer vision systems. Previous works demonstrated the potential of gaze for object-centric tasks, such as object localization and recognition, but it remains unclear if gaze can also be beneficial for scene-centric tasks, such as image captioning. We present a new perspective on gaze-assisted image captioning by studying the interplay between human gaze and the attention mechanism of deep neural networks. Using a public large-scale gaze dataset, we first assess the relationship between state-of-the-art object and scene recognition models, bottom-up visual saliency, and human gaze. We then propose a novel split attention model for image captioning. Our model integrates human gaze information into an attention-based long short-term memory architecture, and allows the algorithm to allocate attention selectively to both fixated and non-fixated image regions. Through evaluation on the COCO/SALICON datasets we show that our method improves image captioning performance and that gaze can complement machine attention for semantic scene understanding tasks.},
archivePrefix = {arXiv},
arxivId = {1608.05203v1},
author = {Sugano, Yusuke and Bulling, Andreas},
booktitle = {Computing Research Repository (CoRR)},
eprint = {1608.05203v1},
file = {:home/mario/Documents/Mendeley/2016/Sugano, Bulling - 2016 - Seeing with Humans Gaze-Assisted Neural Image Captioning.pdf:pdf},
title = {{Seeing with Humans: Gaze-Assisted Neural Image Captioning}},
url = {https://arxiv.org/pdf/1608.05203v1.pdf},
year = {2016}
}
@inproceedings{Goodfellow2014,
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems 27 (NIPS 2014)},
file = {:home/mario/Documents/Mendeley/2014/Goodfellow et al. - 2014 - Generative Adversarial Nets.pdf:pdf},
pages = {2672--2680},
title = {{Generative Adversarial Nets}},
url = {https://papers.nips.cc/paper/5423-generative-adversarial-nets},
year = {2014}
}
@article{Spratling2004,
abstract = {Feedback connections are a prominent feature of cortical anatomy and are likely to have a significant functional role in neural information processing. We present a neural network model of cortical feedback that successfully simulates neurophysiological data associated with attention. In this domain, our model can be considered a more detailed, and biologically plausible, implementation of the biased competition model of attention. However, our model is more general as it can also explain a variety of other top-down processes in vision, such as figure/ground segmentation and contextual cueing. This model thus suggests that a common mechanism, involving cortical feedback pathways, is responsible for a range of phenomena and provides a unified account of currently disparate areas of research.},
author = {Spratling, M. W. and Johnson, M. H.},
doi = {10.1162/089892904322984526},
file = {:home/mario/Documents/Mendeley/2004/Spratling, Johnson - 2004 - A Feedback Model of Visual Attention.pdf:pdf},
issn = {0898-929X},
journal = {Journal of Cognitive Neuroscience},
month = {mar},
number = {2},
pages = {219--237},
pmid = {15068593},
title = {{A Feedback Model of Visual Attention}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15068593 http://www.mitpressjournals.org/doi/10.1162/089892904322984526},
volume = {16},
year = {2004}
}
@inproceedings{Sharma2018,
abstract = {We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and filtering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image cap-tioning models and show that a model architecture based on Inception-ResNet-v2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.},
address = {Melbourne, Australia},
author = {Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics},
file = {:home/mario/Documents/Mendeley/2018/Sharma et al. - 2018 - Conceptual Captions A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning.pdf:pdf},
pages = {2556--2565},
publisher = {Association for Computational Linguistics},
title = {{Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning}},
url = {https://en.wikipedia.org/wiki/Alt},
year = {2018}
}
@inproceedings{Gan2017_Style,
author = {Gan, Chuang and Gan, Zhe and He, Xiaodong and Gao, Jianfeng and Deng, Li},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.108},
file = {:home/mario/Documents/Mendeley/2017/Gan et al. - 2017 - StyleNet Generating Attractive Visual Captions with Styles.pdf:pdf},
isbn = {978-1-5386-0457-1},
month = {jul},
pages = {955--964},
publisher = {IEEE},
title = {{StyleNet: Generating Attractive Visual Captions with Styles}},
url = {http://ieeexplore.ieee.org/document/8099591/},
year = {2017}
}
@article{Ye2018,
author = {Ye, Senmao and Han, Junwei and Liu, Nian},
doi = {10.1109/TIP.2018.2855406},
file = {:home/mario/Documents/Mendeley/2018/Ye, Han, Liu - 2018 - Attentive Linear Transformation for Image Captioning.pdf:pdf},
issn = {1057-7149},
journal = {IEEE Transactions on Image Processing},
month = {nov},
number = {11},
pages = {5514--5524},
title = {{Attentive Linear Transformation for Image Captioning}},
url = {https://ieeexplore.ieee.org/document/8410621/},
volume = {27},
year = {2018}
}
@inproceedings{Lu2017,
author = {Lu, Jiasen and Xiong, Caiming and Parikh, Devi and Socher, Richard},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.345},
file = {:home/mario/Documents/Mendeley/2017/Lu et al. - 2017 - Knowing When to Look Adaptive Attention via a Visual Sentinel for Image Captioning.pdf:pdf},
isbn = {978-1-5386-0457-1},
month = {jul},
pages = {3242--3250},
publisher = {IEEE},
title = {{Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning}},
url = {http://ieeexplore.ieee.org/document/8099828/},
year = {2017}
}
@article{Berger1996,
author = {Berger, Adam L. and Pietra, Stephen A. Della and Pietra, Vincent J. Della},
file = {:home/mario/Documents/Mendeley/1996/Berger, Pietra, Pietra - 1996 - A Maximum Entropy Approach to Natural Language Processing.pdf:pdf},
journal = {Computational Linguistics},
number = {1},
pages = {39--71},
title = {{A Maximum Entropy Approach to Natural Language Processing}},
url = {https://aclweb.org/anthology/papers/J/J96/J96-1002/},
volume = {22},
year = {1996}
}
@inproceedings{Devlin2015,
abstract = {Two recent approaches have achieved state-of-the-art results in image captioning. The first uses a pipelined process where a set of candidate words is generated by a convolutional neural network (CNN) trained on images, and then a maximum entropy (ME) language model is used to arrange these words into a coherent sentence. The second uses the penultimate activation layer of the CNN as input to a recurrent neural network (RNN) that then generates the caption sequence. In this paper, we compare the merits of these different language modeling approaches for the first time by using the same state-of-the-art CNN as input. We examine issues in the different approaches, including linguistic irregularities, caption repetition, and data set overlap. By combining key aspects of the ME and RNN methods, we achieve a new record performance over previously published results on the benchmark COCO dataset. However, the gains we see in BLEU do not translate to human judgments.},
address = {Stroudsburg, PA, USA},
archivePrefix = {arXiv},
arxivId = {1505.01809},
author = {Devlin, Jacob and Cheng, Hao and Fang, Hao and Gupta, Saurabh and Deng, Li and He, Xiaodong and Zweig, Geoffrey and Mitchell, Margaret},
booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
doi = {10.3115/v1/P15-2017},
eprint = {1505.01809},
file = {:home/mario/Documents/Mendeley/2015/Devlin et al. - 2015 - Language Models for Image Captioning The Quirks and What Works.pdf:pdf},
pages = {100--105},
publisher = {Association for Computational Linguistics},
title = {{Language Models for Image Captioning: The Quirks and What Works}},
url = {http://arxiv.org/abs/1505.01809 http://aclweb.org/anthology/P15-2017},
year = {2015}
}
@inproceedings{Sharif2018,
address = {Melbourne, Australia},
author = {Sharif, Naeha and White, Lyndon and Bennamoun, Mohammed and Shah, Syed Afaq Ali},
booktitle = {Proceedings of ACL 2018 Student Research Workshop},
file = {:home/mario/Documents/Mendeley/2018/Sharif et al. - 2018 - Learning-based Composite Metrics for Improved Caption Evaluation.pdf:pdf},
pages = {14--20},
publisher = {Association for Computational Linguistics},
title = {{Learning-based Composite Metrics for Improved Caption Evaluation}},
url = {https://aclweb.org/anthology/papers/P/P18/P18-3003/},
year = {2018}
}
@inproceedings{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
booktitle = {Advances in Neural Information Processing Systems 27 (NIPS 2014)},
file = {:home/mario/Documents/Mendeley/2014/Sutskever, Vinyals, Le - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:pdf},
pages = {3104--3112},
title = {{Sequence to Sequence Learning with Neural Networks}},
url = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks},
year = {2014}
}
@inproceedings{Yagcioglu2015,
abstract = {In this paper, we propose a novel query expansion approach for improving transfer-based automatic image captioning. The core idea of our method is to translate the given visual query into a distributional semantics based form, which is generated by the average of the sentence vectors extracted from the captions of images visually similar to the input image. Using three image captioning benchmark datasets, we show that our approach provides more accurate results compared to the state-of-the-art data-driven methods in terms of both automatic metrics and subjective evaluation .},
address = {Beijing, China},
author = {Yagcioglu, Semih and Erdem, Erkut and Erdem, Aykut and Cakici, Ruket and {\c{C}}akici, Ruket},
booktitle = {Proceedings of the e 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing},
doi = {0.3115/v1/P15-2018},
file = {:home/mario/Documents/Mendeley/2015/Yagcioglu et al. - 2015 - A Distributed Representation Based Query Expansion Approach for Image Captioning.pdf:pdf},
pages = {106--111},
publisher = {Association for Computational Linguistics},
title = {{A Distributed Representation Based Query Expansion Approach for Image Captioning}},
url = {https://aclweb.org/anthology/P15-2018 http://aclweb.org/anthology/P15-2018},
year = {2015}
}
@inproceedings{Yao2017_Attr,
author = {Yao, Ting and Pan, Yingwei and Li, Yehao and Qiu, Zhaofan and Mei, Tao},
booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2017.524},
file = {:home/mario/Documents/Mendeley/2017/Yao et al. - 2017 - Boosting Image Captioning with Attributes.pdf:pdf},
isbn = {978-1-5386-1032-9},
month = {oct},
pages = {4904--4912},
publisher = {IEEE},
title = {{Boosting Image Captioning with Attributes}},
url = {http://ieeexplore.ieee.org/document/8237786/},
year = {2017}
}
@inproceedings{Bychkovsky2011,
author = {Bychkovsky, Vladimir and Paris, Sylvain and Chan, Eric and Durand, Fredo},
booktitle = {CVPR 2011},
doi = {10.1109/CVPR.2011.5995332},
isbn = {978-1-4577-0394-2},
month = {jun},
pages = {97--104},
publisher = {IEEE},
title = {{Learning photographic global tonal adjustment with a database of input / output image pairs}},
url = {http://ieeexplore.ieee.org/document/5995332/},
year = {2011}
}
@inproceedings{Cho2014,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
booktitle = {Proceedings of the 2014 Conference on Empirical Methods on Natural Language Processing (EMNLP)},
doi = {10.3115/v1/D14-1179},
eprint = {1406.1078},
file = {:home/mario/Documents/Mendeley/2014/Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.pdf:pdf},
month = {jun},
pages = {1724--1734},
title = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}},
url = {https://aclweb.org/anthology/D14-1179},
year = {2014}
}
@inproceedings{Yatskar2014,
abstract = {This paper studies generation of descriptive sentences from densely annotated images. Previous work studied generation from automatically detected visual information but produced a limited class of sentences, hindered by currently unreliable recognition of activities and attributes. Instead, we collect human annotations of objects, parts, attributes and activities in images. These annotations allow us to build a significantly more comprehensive model of language generation and allow us to study what visual information is required to generate human-like descriptions. Experiments demonstrate high quality output and that activity annotations and relative spatial location of objects contribute most to producing high quality sentences.},
address = {Stroudsburg, PA, USA},
author = {Yatskar, Mark and Galley, Michel and Vanderwende, Lucy and Zettlemoyer, Luke},
booktitle = {Proceedings of the 3rd Joint Conference on Lexical and Computational Semantics (SEM 2014)},
doi = {10.3115/v1/S14-1015},
file = {:home/mario/Documents/Mendeley/2014/Yatskar et al. - 2014 - See No Evil, Say No Evil Description Generation from Densely Labeled Images.pdf:pdf},
pages = {110--120},
publisher = {Association for Computational Linguistics and Dublin City University},
title = {{See No Evil, Say No Evil: Description Generation from Densely Labeled Images}},
url = {http://aclweb.org/anthology/S14-1015},
year = {2014}
}
@inproceedings{Xu2015,
abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
address = {Lille, France},
archivePrefix = {arXiv},
arxivId = {1502.03044},
author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
booktitle = {32nd International Conference on Machine Learning (ICML'15)},
doi = {10.1016/j.exger.2013.05.060},
eprint = {1502.03044},
file = {:home/mario/Documents/Mendeley/2015/Xu et al. - 2015 - Show, Attend and Tell Neural Image Caption Generation with Visual Attention.pdf:pdf},
isbn = {0531-5565},
issn = {05315565},
month = {feb},
pages = {2048--2057},
pmid = {23770107},
title = {{Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}},
url = {http://proceedings.mlr.press/v37/xuc15.pdf http://arxiv.org/abs/1502.03044 https://linkinghub.elsevier.com/retrieve/pii/S0531556513001940},
volume = {37},
year = {2015}
}
@inproceedings{Callison-Burch2006,
abstract = {We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric. We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality , and give two significant counterexamples to Bleu's correlation with human judgments of quality. This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores.},
address = {Trento, Italy},
author = {Callison-Burch, Chris and Osborne, Miles and Koehn, Philipp},
booktitle = {Proceedings of 11th Conference of the European Chapter of the Association for Computational Linguistics},
file = {:home/mario/Documents/Mendeley/2006/Callison-Burch, Osborne, Koehn - 2006 - Re-evaluating the Role of BLEU in Machine Translation Research.pdf:pdf},
title = {{Re-evaluating the Role of BLEU in Machine Translation Research}},
url = {https://pdfs.semanticscholar.org/6635/4f51f14f9935dff170907d4f3de89d4746e2.pdf},
year = {2006}
}
@incollection{Lin2014,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
archivePrefix = {arXiv},
arxivId = {1405.0312},
author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C. Lawrence},
booktitle = {Proceedings of the 13th European Conference on Computer Vision (ECCV 2014)},
doi = {10.1007/978-3-319-10602-1_48},
eprint = {1405.0312},
file = {:home/mario/Documents/Mendeley/2014/Lin et al. - 2014 - Microsoft COCO Common Objects in Context.pdf:pdf},
month = {may},
pages = {740--755},
publisher = {Springer},
title = {{Microsoft COCO: Common Objects in Context}},
url = {https://arxiv.org/pdf/1405.0312.pdf http://link.springer.com/10.1007/978-3-319-10602-1{\_}48 http://arxiv.org/abs/1405.0312},
volume = {8693 LNCS},
year = {2014}
}
@inproceedings{Lebret2015a,
abstract = {Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutional Neural Network) and phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on caption syntax statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results in two popular datasets for the task: Flickr30k and the recently proposed Microsoft COCO.},
archivePrefix = {arXiv},
arxivId = {1502.03671},
author = {Lebret, R{\'{e}}mi and Pinheiro, Pedro O. and Collobert, Ronan},
booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
eprint = {1502.03671},
file = {:home/mario/Documents/Mendeley/2015/Lebret, Pinheiro, Collobert - 2015 - Phrase-based Image Captioning.pdf:pdf},
month = {feb},
title = {{Phrase-based Image Captioning}},
url = {http://arxiv.org/abs/1502.03671},
year = {2015}
}
@inproceedings{Mao2016,
address = {Las Vegas, USA},
author = {Mao, Junhua and Huang, Jonathan and Toshev, Alexander and Camburu, Oana and Yuille, Alan and Murphy, Kevin},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.9},
file = {:home/mario/Documents/Mendeley/2016/Mao et al. - 2016 - Generation and Comprehension of Unambiguous Object Descriptions.pdf:pdf},
isbn = {978-1-4673-8851-1},
month = {jun},
pages = {11--20},
publisher = {IEEE},
title = {{Generation and Comprehension of Unambiguous Object Descriptions}},
url = {http://ieeexplore.ieee.org/document/7780378/},
year = {2016}
}
@article{Blei2003,
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
file = {:home/mario/Documents/Mendeley/2003/Blei, Ng, Jordan - 2003 - Latent Dirichlet Allocation.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {993--1022},
title = {{Latent Dirichlet Allocation}},
url = {http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf},
volume = {3},
year = {2003}
}
@inproceedings{Szegedy2015,
abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the Im-ageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular in-carnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
author = {Szegedy, Christian and {Wei Liu} and {Yangqing Jia} and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2015.7298594},
file = {:home/mario/Documents/Mendeley/2015/Szegedy et al. - 2015 - Going deeper with convolutions.pdf:pdf},
isbn = {978-1-4673-6964-0},
month = {jun},
pages = {1--9},
publisher = {IEEE},
title = {{Going deeper with convolutions}},
url = {https://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2015/papers/Szegedy{\_}Going{\_}Deeper{\_}With{\_}2015{\_}CVPR{\_}paper.pdf http://ieeexplore.ieee.org/document/7298594/},
year = {2015}
}
@misc{Karpathy2015_unreasonable,
abstract = {There's something magical about Recurrent Neural Networks (RNNs). I still remember when I trained my first recurrent network for Image Captioning. Within a few dozen minutes of training my first baby model (with rather arbitrarily-chosen hyperparameters) started to generate very nice looking descriptions of images that were on the edge of making sense. Sometimes the ratio of how simple your model is to the quality of the results you get out of it blows past your expectations, and this was one of those times. What made this result so shocking at the time was that the common wisdom was that RNNs were supposed to be difficult to train (with more experience I've in fact reached the opposite conclusion). Fast forward about a year: I'm training RNNs all the time and I've witnessed their power and robustness many times, and yet their magical outputs still find ways of amusing me. This post is about sharing some of that magic with you.},
author = {Karpathy, Andrej},
booktitle = {Web Page},
file = {:home/mario/Documents/Mendeley/2015/Karpathy - 2015 - The Unreasonable Effectiveness of Recurrent Neural Networks.pdf:pdf},
title = {{The Unreasonable Effectiveness of Recurrent Neural Networks}},
url = {http://karpathy.github.io/2015/05/21/rnn-effectiveness/},
year = {2015}
}
@article{Bai2018,
abstract = {Image captioning means automatically generating a caption for an image. As a recently emerged research area, it is attracting more and more attention. To achieve the goal of image captioning, semantic information of images needs to be captured and expressed in natural languages. Connecting both research communities of computer vision and natural language processing, image captioning is a quite challenging task. Various approaches have been proposed to solve this problem. In this paper, we present a survey on advances in image captioning research. Based on the technique adopted, we classify image captioning approaches into different categories. Representative methods in each category are summarized, and their strengths and limitations are talked about. In this paper, we first discuss methods used in early work which are mainly retrieval and template based. Then, we focus our main attention on neural network based methods, which give state of the art results. Neural network based methods are further divided into subcategories based on the specific framework they use. Each subcategory of neural network based methods are discussed in detail. After that, state of the art methods are compared on benchmark datasets. Following that, discussions on future research directions are presented.},
author = {Bai, Shuang and An, Shan},
doi = {10.1016/j.neucom.2018.05.080},
file = {:home/mario/Documents/Mendeley/2018/Bai, An - 2018 - A survey on automatic image caption generation.pdf:pdf},
isbn = {9789811300226},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Attention mechanism,Deep neural networks,Encoder–decoder framework,Image captioning,Multimodal embedding,Sentence template},
pages = {291--304},
publisher = {Elsevier B.V.},
title = {{A survey on automatic image caption generation}},
volume = {311},
year = {2018}
}
@inproceedings{Chen2017_SCA,
abstract = {Visual attention has been successfully applied in structural prediction tasks such as visual captioning and question answering. Existing visual attention models are generally spatial, i.e., the attention is modeled as spatial probabilities that re-weight the last conv-layer feature map of a CNN encoding an input image. However, we argue that such spatial attention does not necessarily conform to the attention mechanism - a dynamic feature extractor that combines contextual fixations over time, as CNN features are naturally spatial, channel-wise and multi-layer. In this paper, we introduce a novel convolutional neural network dubbed SCA-CNN that incorporates Spatial and Channel-wise Attentions in a CNN. In the task of image captioning, SCA-CNN dynamically modulates the sentence generation context in multi-layer feature maps, encoding where (i.e., attentive spatial locations at multiple layers) and what (i.e., attentive channels) the visual attention is. We evaluate the proposed SCA-CNN architecture on three benchmark image captioning datasets: Flickr8K, Flickr30K, and MSCOCO. It is consistently observed that SCA-CNN significantly outperforms state-of-the-art visual attention-based image captioning methods.},
address = {Honolulu, USA},
author = {Chen, Long and Zhang, Hanwang and Xiao, Jun and Nie, Liqiang and Shao, Jian and Liu, Wei and Chua, Tat-Seng},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.667},
file = {:home/mario/Documents/Mendeley/2017/Chen et al. - 2017 - SCA-CNN Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning.pdf:pdf},
isbn = {978-1-5386-0457-1},
month = {jul},
pages = {6298--6306},
publisher = {IEEE},
title = {{SCA-CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning}},
url = {http://ieeexplore.ieee.org/document/8100150/},
year = {2017}
}
@inproceedings{Pedersoli2017,
abstract = {We propose “Areas of Attention”, a novel attentionbased model for automatic image captioning. Our approach models the dependencies between image regions, caption words, and the state of an RNN language model, using three pairwise interactions. In contrast to previous attentionbased approaches that associate image regions only to the RNN state, our method allows a direct association between caption words and image regions. During training these associations are inferred from image-level captions, akin to weakly-supervised object detector training. These associations help to improve captioning by localizing the corresponding regions during testing. We also propose and compare different ways of generating attention areas: CNN activation grids, object proposals, and spatial transformers nets applied in a convolutional fashion. Spatial transformers give the best results. They allow for image specific attention areas, and can be trained jointly with the rest of the network. Our attention mechanism and spatial transformer attention areas together yield state-of-the-art results on the MSCOCO dataset.},
address = {Venice, Italy},
author = {Pedersoli, Marco and Lucas, Thomas and Schmid, Cordelia and Verbeek, Jakob},
booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2017.140},
file = {:home/mario/Documents/Mendeley/2017/Pedersoli et al. - 2017 - Areas of Attention for Image Captioning.pdf:pdf},
isbn = {978-1-5386-1032-9},
month = {oct},
pages = {1251--1259},
publisher = {IEEE},
title = {{Areas of Attention for Image Captioning}},
url = {http://ieeexplore.ieee.org/document/8237402/},
year = {2017}
}
@inproceedings{Masotti2017,
abstract = {Recent advancements in Deep Learning show that the combination of Convolutional Neural Networks and Recurrent Neural Networks enables the definition of very effective methods for the automatic captioning of images. Unfortunately, this straightforward result requires the existence of large-scale corpora and they are not available for many languages. This paper describes a simple methodology to automatically acquire a large-scale corpus of 600 thousand image/sentences pairs in Italian. At the best of our knowledge, this corpus has been used to train one of the first neural systems for the same language. The experimental evaluation over a subset of validated image/captions pairs suggests that results comparable with the English counterpart can be achieved.},
address = {Rome, Italy},
author = {Masotti, Caterina and Croce, Danilo and Basili, Roberto},
booktitle = {Fourth Italian Conference on Computational Linguistics (CLiC-it 2017)},
doi = {10.4000/books.aaccademia.2425},
file = {:home/mario/Documents/Mendeley/2017/Masotti, Croce, Basili - 2017 - Deep Learning for Automatic Image Captioning in poor Training Conditions.pdf:pdf},
issn = {16130073},
pages = {207--211},
publisher = {Accademia University Press},
title = {{Deep Learning for Automatic Image Captioning in poor Training Conditions}},
url = {http://books.openedition.org/aaccademia/2425},
year = {2017}
}
@inproceedings{Long2015,
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2015.7298965},
isbn = {978-1-4673-6964-0},
month = {jun},
pages = {3431--3440},
publisher = {IEEE},
title = {{Fully convolutional networks for semantic segmentation}},
url = {http://ieeexplore.ieee.org/document/7298965/},
year = {2015}
}
@inproceedings{Wang2016_Unified,
author = {Wang, Jiang and Yang, Yi and Mao, Junhua and Huang, Zhiheng and Huang, Chang and Xu, Wei},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.251},
file = {:home/mario/Documents/Mendeley/2016/Wang et al. - 2016 - CNN-RNN A Unified Framework for Multi-label Image Classification.pdf:pdf},
isbn = {978-1-4673-8851-1},
month = {jun},
pages = {2285--2294},
publisher = {IEEE},
title = {{CNN-RNN: A Unified Framework for Multi-label Image Classification}},
url = {http://ieeexplore.ieee.org/document/7780620/},
year = {2016}
}
@inproceedings{Mason2015,
abstract = {We present a nonparametric density esti-mation technique for image caption gener-ation. Data-driven matching methods have shown to be effective for a variety of com-plex problems in Computer Vision. These methods reduce an inference problem for an unknown image to finding an exist-ing labeled image which is semantically similar. However, related approaches for image caption generation (Ordonez et al., 2011; Kuznetsova et al., 2012) are ham-pered by noisy estimations of visual con-tent and poor alignment between images and human-written captions. Our work addresses this challenge by estimating a word frequency representation of the vi-sual content of a query image. This al-lows us to cast caption generation as an extractive summarization problem. Our model strongly outperforms two state-of-the-art caption extraction systems accord-ing to human judgments of caption rele-vance.},
address = {Stroudsburg, PA, USA},
author = {Mason, Rebecca and Charniak, Eugene},
booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics},
doi = {10.3115/v1/P14-2097},
file = {:home/mario/Documents/Mendeley/2014/Mason, Charniak - 2014 - Nonparametric Method for Data-driven Image Captioning.pdf:pdf},
pages = {592--598},
publisher = {Association for Computational Linguistics},
title = {{Nonparametric Method for Data-driven Image Captioning}},
url = {http://aclweb.org/anthology/P14-2097},
year = {2014}
}
@inproceedings{Kiros2014_VS,
abstract = {Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* - "blue" + "red" is near images of red cars. Sample captions generated for 800 images are made available for comparison.},
archivePrefix = {arXiv},
arxivId = {1411.2539},
author = {Kiros, Ryan and Salakhutdinov, Ruslan and Zemel, Richard S.},
booktitle = {Advances in Neural Information Processing Systems Deep Learning Workshop},
eprint = {1411.2539},
file = {:home/mario/Documents/Mendeley/2014/Kiros, Salakhutdinov, Zemel - 2014 - Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models.pdf:pdf},
title = {{Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models}},
url = {http://arxiv.org/abs/1411.2539},
year = {2014}
}
@inproceedings{Chung2014,
address = {Montreal, Canada},
author = {Chung, Junyoung and G{\"{u}}l{\c{c}}ehre, {\c{C}}aglar and Cho, Kyunghyun and Bengio, Yoshua},
booktitle = {NIPS 2014 Deep Learning and Representation Learning Workshop},
file = {:home/mario/Documents/Mendeley/2014/Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.pdf:pdf},
title = {{Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling}},
url = {https://www.semanticscholar.org/paper/Empirical-Evaluation-of-Gated-Recurrent-Neural-on-Chung-G{\"{u}}l{\c{c}}ehre/2d9e3f53fcdb548b0b3c4d4efb197f164fe0c381},
year = {2014}
}
@inproceedings{Lin2004a,
abstract = {ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluations. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale sum-marization evaluation sponsored by NIST.},
address = {Barcelona, Spain},
author = {Lin, Chin-Yew},
booktitle = {Text Summarization Branches Out. Post-Conference Workshop at the 42nd Annual Meeting of the ACL},
file = {:home/mario/Documents/Mendeley/2004/Lin - 2004 - ROUGE A Package for Automatic Evaluation of Summaries.pdf:pdf},
pages = {74--81},
publisher = {Association for Computational Linguistics},
title = {{ROUGE: A Package for Automatic Evaluation of Summaries}},
url = {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/07/was2004.pdf https://www.aclweb.org/anthology/W04-1013},
year = {2004}
}
@inproceedings{Gu2018,
abstract = {The existing image captioning approaches typically train a one-stage sentence decoder, which is difficult to generate rich fine-grained descriptions. On the other hand, multi-stage image caption model is hard to train due to the vanishing gradient problem. In this paper, we propose a coarse-to-fine multi-stage prediction framework for image captioning, composed of multiple decoders each of which operates on the output of the previous stage, producing increasingly refined image descriptions. Our proposed learning approach addresses the difficulty of vanishing gradients during training by providing a learning objective function that enforces intermediate supervisions. Particularly, we optimize our model with a reinforcement learning approach which utilizes the output of each intermediate decoder's test-time inference algorithm as well as the output of its preceding decoder to normalize the rewards, which simultaneously solves the well-known exposure bias problem and the loss-evaluation mismatch problem. We extensively evaluate the proposed approach on MSCOCO and show that our approach can achieve the state-of-the-art performance.},
address = {New Orleans, USA},
archivePrefix = {arXiv},
arxivId = {1709.03376v3},
author = {Gu, Jiuxiang and Cai, Jianfei and Wang, Gang and Chen, Tsuhan},
booktitle = {The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18) },
eprint = {1709.03376v3},
file = {:home/mario/Documents/Mendeley/2018/Gu et al. - 2018 - Stack-Captioning Coarse-to-Fine Learning for Image Captioning.pdf:pdf},
publisher = {AAAI Press},
title = {{Stack-Captioning: Coarse-to-Fine Learning for Image Captioning}},
url = {www.aaai.org},
year = {2018}
}
@incollection{Oliva2006,
author = {Oliva, Aude and Torralba, Antonio},
booktitle = {Progress of Brain Research},
doi = {10.1016/S0079-6123(06)55002-2},
file = {:home/mario/Documents/Mendeley/2006/Oliva, Torralba - 2006 - Building the gist of a scene the role of global image features in recognition.pdf:pdf},
isbn = {0079-6123},
pages = {23--36},
publisher = {Elsevier B.V.},
title = {{Building the gist of a scene: the role of global image features in recognition}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0079612306550022},
volume = {155},
year = {2006}
}
@inproceedings{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
address = {San Diego, USA},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
booktitle = {Proceedings of the 3rd International Conference on Learning Representations (ICLR)},
eprint = {1409.1556},
file = {:home/mario/Documents/Mendeley/2015/Simonyan, Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:pdf},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2015}
}
@incollection{Silberman2012,
abstract = {We present an approach to interpret the major surfaces, objects, and support relations of an indoor scene from an RGBD image. Most existing work ignores physical interactions or is applied only to tidy rooms and hallways. Our goal is to parse typical, often messy, indoor scenes into floor, walls, supporting surfaces, and object regions, and to recover support relationships. One of our main interests is to better understand how 3D cues can best inform a structured 3D interpretation. We also contribute a novel integer programming formulation to infer physical support relations. We offer a new dataset of 1449 RGBD images, capturing 464 diverse indoor scenes, with detailed annotations. Our experiments demonstrate our ability to infer support relations in complex scenes and verify that our 3D scene cues and inferred support lead to better object segmentation},
address = {Berline, Germany},
author = {Silberman, Nathan and Hoiem, Derek and Kohli, Pushmeet and Fergus, Rob},
booktitle = {Proceedings of the 12th European Conference on Computer Vision (ECCV)},
doi = {10.1007/978-3-642-33715-4_54},
pages = {746--760},
publisher = {Springer},
title = {{Indoor Segmentation and Support Inference from RGBD Images}},
url = {http://link.springer.com/10.1007/978-3-642-33715-4{\_}54},
year = {2012}
}
@book{Goodfellow2016,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
keywords = {Deep Learning,Machine Learning},
mendeley-tags = {Deep Learning,Machine Learning},
publisher = {MIT Press},
title = {{Deep Learning}},
url = {http://www.deeplearningbook.org/},
year = {2016}
}
@inproceedings{Lin2004b,
address = {Barcelona, Spain},
author = {Lin, Chin-Yew and Och, Franz Josef},
booktitle = {Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics - ACL '04},
doi = {10.3115/1218955.1219032},
file = {:home/mario/Documents/Mendeley/2004/Lin, Och - 2004 - Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics.pdf:pdf},
publisher = {Association for Computational Linguistics},
title = {{Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics}},
url = {http://portal.acm.org/citation.cfm?doid=1218955.1219032},
year = {2004}
}
@inproceedings{Ren2017,
abstract = {Image captioning is a challenging problem owing to the complexity in understanding the image content and diverse ways of describing it in natural language. Recent advances in deep neural networks have substantially improved the performance of this task. Most state-of-the-art approaches follow an encoder-decoder framework, which generates captions using a sequential recurrent prediction model. However, in this paper, we introduce a novel decision-making framework for image captioning. We utilize a "policy network" and a "value network" to collab-oratively generate captions. The policy network serves as a local guidance by providing the confidence of predicting the next word according to the current state. Additionally, the value network serves as a global and lookahead guidance by evaluating all possible extensions of the current state. In essence, it adjusts the goal of predicting the correct words towards the goal of generating captions similar to the ground truth captions. We train both networks using an actor-critic reinforcement learning model, with a novel reward defined by visual-semantic embedding. Extensive experiments and analyses on the Microsoft COCO dataset show that the proposed framework outperforms state-of-the-art approaches across different evaluation metrics.},
address = {Honolulu, USA},
author = {Ren, Zhou and Wang, Xiaoyu and Zhang, Ning and Lv, Xutao and Li, Li-Jia},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.128},
file = {:home/mario/Documents/Mendeley/2017/Ren et al. - 2017 - Deep Reinforcement Learning-Based Image Captioning with Embedding Reward.pdf:pdf},
isbn = {978-1-5386-0457-1},
month = {jul},
pages = {1151--1159},
publisher = {IEEE},
title = {{Deep Reinforcement Learning-Based Image Captioning with Embedding Reward}},
url = {http://openaccess.thecvf.com/content{\_}cvpr{\_}2017/papers/Ren{\_}Deep{\_}Reinforcement{\_}Learning-Based{\_}CVPR{\_}2017{\_}paper.pdf http://ieeexplore.ieee.org/document/8099611/},
year = {2017}
}
@article{Rensink2000,
abstract = {One of the more powerful impressions created by vision is that of a coherent, richly detailed world where everything is present simultaneously. Indeed, this impression is so compelling that we tend to ascribe these properties not only to the external world, but to our internal representations as well. But results from several recent experiments argue against this latter ascription. For example, changes in images of real-world scenes often go unnoticed when made during a saccade, flicker, blink, or movie cut. This “change blindness” provides strong evidence against the idea that our brains contain a picture-like representation of the scene that is everywhere detailed and coherent. How then do we represent a scene? It is argued here that focused attention provides spatiotemporal coherence for the stable representation of one object at a time. It is then argued that the allocation of attention can be co-ordinated to create a “virtual representation”. In such a scheme, a stable object representation is formed...},
author = {Rensink, Ronald A.},
doi = {10.1080/135062800394667},
issn = {1350-6285},
journal = {Visual Cognition},
month = {jan},
number = {1-3},
pages = {17--42},
publisher = { Taylor {\&} Francis Group },
title = {{The Dynamic Representation of Scenes}},
url = {http://www.tandfonline.com/doi/abs/10.1080/135062800394667},
volume = {7},
year = {2000}
}
@article{Tan2019,
author = {Tan, Ying Hua and Chan, Chee Seng},
doi = {10.1016/j.neucom.2018.12.026},
file = {:home/mario/Documents/Mendeley/2019/Tan, Chan - 2019 - Phrase-based image caption generator with hierarchical LSTM network.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
month = {mar},
pages = {86--100},
title = {{Phrase-based image caption generator with hierarchical LSTM network}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231218314802},
volume = {333},
year = {2019}
}
@inproceedings{Mnih2007,
address = {New York, New York, USA},
author = {Mnih, Andriy and Hinton, Geoffrey},
booktitle = {Proceedings of the 24th international conference on Machine learning - ICML '07},
doi = {10.1145/1273496.1273577},
file = {:home/mario/Documents/Mendeley/2007/Mnih, Hinton - 2007 - Three new graphical models for statistical language modelling.pdf:pdf},
isbn = {9781595937933},
pages = {641--648},
publisher = {ACM Press},
title = {{Three new graphical models for statistical language modelling}},
url = {http://portal.acm.org/citation.cfm?doid=1273496.1273577},
year = {2007}
}
@misc{Hrga,
abstract = {As a problem that resides at the intersection of Computer Vision and Natural Language Processing, image captioning has witnessed a rapid progress in a very short time, from initial template-based models to the current ones, based on deep neural networks. This paper gives an overview of current issues and recent research on image captioning, with a special emphasis on models employing deep encoder-decoder architectures. We discuss the advantages and disadvantages of different approaches, along with reviewing some of the most commonly used datasets and evaluation metrics. We point out to some open questions and conclude with directions for future research.},
author = {Hrga, Ingrid},
file = {:home/mario/Documents/Mendeley/Unknown/Hrga - Unknown - Deep Image Captioning Models, Data and Evaluation.pdf:pdf},
keywords = {attention mechanism,deep,encoder-decoder framework,image captioning,neural networks},
title = {{Deep Image Captioning: Models, Data and Evaluation}},
url = {http://www.inf.uniri.hr/files/studiji/poslijediplomski/kvalifikacijski/Hrga{\_}Ingrid{\_}Kvalifikacijski{\_}rad.pdf}
}
@article{Uijlings2013,
author = {Uijlings, J. R. R. and van de Sande, K. E. A. and Gevers, T. and Smeulders, A. W. M.},
doi = {10.1007/s11263-013-0620-5},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
number = {2},
pages = {154--171},
publisher = {Springer US},
title = {{Selective Search for Object Recognition}},
url = {http://link.springer.com/10.1007/s11263-013-0620-5},
volume = {104},
year = {2013}
}
@inproceedings{Pu2016_DGDN,
address = {Cadiz, Spain},
author = {Pu, Yunchen and Yuan, Win and Stevens, Andrew and Li, Chunyuan and Carin, Lawrence},
booktitle = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AISTATS 2016)},
file = {:home/mario/Documents/Mendeley/2016/Pu et al. - 2016 - A Deep Generative Deconvolutional Image Model.pdf:pdf},
pages = {741--750},
title = {{A Deep Generative Deconvolutional Image Model}},
url = {http://proceedings.mlr.press/v51/pu16.html},
year = {2016}
}
@inproceedings{Yao2018,
abstract = {It is always well believed that modeling relationships between objects would be helpful for representing and eventually describing an image. Nevertheless, there has not been evidence in support of the idea on image description generation. In this paper, we introduce a new design to explore the connections between objects for image captioning under the umbrella of attention-based encoder-decoder framework. Specifically, we present Graph Convolutional Networks plus Long Short-Term Memory (dubbed as GCN-LSTM) architecture that novelly integrates both semantic and spatial object relationships into image encoder. Technically , we build graphs over the detected objects in an image based on their spatial and semantic connections. The representations of each region proposed on objects are then refined by leveraging graph structure through GCN. With the learnt region-level features, our GCN-LSTM capitalizes on LSTM-based captioning framework with attention mechanism for sentence generation. Extensive experiments are conducted on COCO image captioning dataset, and superior results are reported when comparing to state-of-the-art approaches. More remarkably, GCN-LSTM increases CIDEr-D performance from 120.1{\%} to 128.7{\%} on COCO testing set.},
address = {Munich, Germany},
author = {Yao, Ting and Pan, Yingwei and Li, Yehao and Mei, Tao},
booktitle = {15th European Conference on Computer Vision},
file = {:home/mario/Documents/Mendeley/2018/Yao et al. - 2018 - Exploring Visual Relationship for Image Captioning.pdf:pdf},
publisher = {Springer},
title = {{Exploring Visual Relationship for Image Captioning}},
url = {http://openaccess.thecvf.com/content{\_}ECCV{\_}2018/papers/Ting{\_}Yao{\_}Exploring{\_}Visual{\_}Relationship{\_}ECCV{\_}2018{\_}paper.pdf},
year = {2018}
}
@inproceedings{Cui2018,
abstract = {Evaluation metrics for image captioning face two challenges. Firstly, commonly used metrics such as CIDEr, METEOR, ROUGE and BLEU often do not correlate well with human judgments. Secondly, each metric has well known blind spots to pathological caption constructions, and rule-based metrics lack provisions to repair such blind spots once identified. For example, the newly proposed SPICE correlates well with human judgments, but fails to capture the syntactic structure of a sentence. To address these two challenges, we propose a novel learning based discriminative evaluation metric that is directly trained to distinguish between human and machine-generated captions. In addition, we further propose a data augmentation scheme to explicitly incorporate pathological transformations as negative examples during training. The proposed metric is evaluated with three kinds of robustness tests and its correlation with human judgments. Extensive experiments show that the proposed data augmentation scheme not only makes our metric more robust toward several pathological transformations, but also improves its correlation with human judgments. Our metric outperforms other metrics on both caption level human correlation in Flickr 8k and system level human correlation in COCO. The proposed approach could be served as a learning based evaluation metric that is complementary to existing rule-based metrics.},
archivePrefix = {arXiv},
arxivId = {1806.06422},
author = {Cui, Yin and Yang, Guandao and Veit, Andreas and Huang, Xun and Belongie, Serge},
booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00608},
eprint = {1806.06422},
file = {:home/mario/Documents/Mendeley/2018/Cui et al. - 2018 - Learning to Evaluate Image Captioning.pdf:pdf},
isbn = {978-1-5386-6420-9},
issn = {0265-203X},
month = {jun},
pages = {5804--5812},
publisher = {IEEE},
title = {{Learning to Evaluate Image Captioning}},
url = {http://arxiv.org/abs/1806.06422 https://ieeexplore.ieee.org/document/8578706/},
year = {2018}
}
@inproceedings{Elbayad2018,
address = {Brussels, Belgium},
author = {Elbayad, Maha and Besacier, Laurent and Verbeek, Jakob},
booktitle = {Proceedings of the 22nd Conference on Computational Natural Language Learning},
file = {:home/mario/Documents/Mendeley/2018/Elbayad, Besacier, Verbeek - 2018 - Pervasive Attention 2D Convolutional Neural Networks for Sequence-to-Sequence Prediction.pdf:pdf},
pages = {97--107},
publisher = {Association for Computational Linguistics},
title = {{Pervasive Attention: 2D Convolutional Neural Networks for Sequence-to-Sequence Prediction}},
url = {https://aclweb.org/anthology/papers/K/K18/K18-1010/},
year = {2018}
}
@inproceedings{Li2018_VS-LSTM,
abstract = {In this paper, a novel image captioning approach is proposed to describe the content of images. Inspired by the visual processing of our cognitive system , we propose a visual-semantic LSTM model to locate the attention objects with their low-level features in the visual cell, and then successively extract high-level semantic features in the semantic cell. In addition, a state perturbation term is introduced to the word sampling strategy in the REINFORCE based method to explore proper vocabularies in the training process. Experimental results on MS COCO and Flickr30K validate the effectiveness of our approach when compared to the state-of-the-art methods.},
address = {Stockholm, Sweden},
author = {Li, Nannan and Chen, Zhenzhong},
booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18)},
file = {:home/mario/Documents/Mendeley/2018/Li, Chen - 2018 - Image Captioning with Visual-Semantic LSTM.pdf:pdf},
title = {{Image Captioning with Visual-Semantic LSTM}},
url = {https://www.ijcai.org/proceedings/2018/0110.pdf},
year = {2018}
}
@article{Badrinarayanan2017,
author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
doi = {10.1109/TPAMI.2016.2644615},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = {dec},
number = {12},
pages = {2481--2495},
title = {{SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation}},
url = {https://ieeexplore.ieee.org/document/7803544/},
volume = {39},
year = {2017}
}
@inproceedings{Yao2017_NOB,
author = {Yao, Ting and Pan, Yingwei and Li, Yehao and Mei, Tao},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.559},
file = {:home/mario/Documents/Mendeley/2017/Yao et al. - 2017 - Incorporating Copying Mechanism in Image Captioning for Learning Novel Objects.pdf:pdf},
isbn = {978-1-5386-0457-1},
month = {jul},
pages = {5263--5271},
publisher = {IEEE},
title = {{Incorporating Copying Mechanism in Image Captioning for Learning Novel Objects}},
url = {http://ieeexplore.ieee.org/document/8100042/},
year = {2017}
}
@inproceedings{Hu2014,
abstract = {Semantic matching is of central importance to many natural language tasks [2, 28]. A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them. As a step toward this goal, we propose convolutional neural network models for matching two sentences, by adapting the convolutional strategy in vision and speech. The proposed models not only nicely represent the hierarchical structures of sentences with their layer-by-layer composition and pooling, but also capture the rich matching patterns at different levels. Our models are rather generic, requiring no prior knowledge on language, and can hence be applied to matching tasks of different nature and in different languages. The empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models.},
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {1503.03244v1},
author = {Hu, Baotian and Lu, Zhengdong and Li, Hang and Chen, Qingcai},
booktitle = {Advances in Neural Information Processing Systems 27 (NIPS)},
eprint = {1503.03244v1},
file = {:home/mario/Documents/Mendeley/2014/Hu et al. - 2014 - Convolutional Neural Network Architectures for Matching Natural Language Sentences.pdf:pdf},
title = {{Convolutional Neural Network Architectures for Matching Natural Language Sentences}},
url = {http://www.noahlab.com.hk/technology/Learning2Match.html},
year = {2014}
}
@inproceedings{Johnson2015,
author = {Johnson, Justin and Krishna, Ranjay and Stark, Michael and Li, Li-Jia and Shamma, David A. and Bernstein, Michael S. and Fei-Fei, Li},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2015.7298990},
file = {:home/mario/Documents/Mendeley/2015/Johnson et al. - 2015 - Image retrieval using scene graphs.pdf:pdf},
isbn = {978-1-4673-6964-0},
pages = {3668--3678},
publisher = {IEEE},
title = {{Image retrieval using scene graphs}},
url = {http://ieeexplore.ieee.org/document/7298990/},
year = {2015}
}
@inproceedings{Collobert2008,
abstract = {We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art performance.},
address = {Helsinki, Finland},
author = {Collobert, Ronan and Weston, Jason},
booktitle = {25th International Conference on Machine Learning (ICML'08)},
file = {:home/mario/Documents/Mendeley/2008/Collobert, Weston - 2008 - A Unified Architecture for Natural Language Processing Deep Neural Networks with Multitask Learning.pdf:pdf},
title = {{A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning}},
url = {http://wordnet.princeton.edu},
year = {2008}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM l...},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
file = {:home/mario/Documents/Mendeley/1997/Hochreiter, Schmidhuber - 1997 - Long Short-Term Memory.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
month = {nov},
number = {8},
pages = {1735--1780},
publisher = {MIT Press 238 Main St., Suite 500, Cambridge, MA 02142-1046 USA journals-info@mit.edu},
title = {{Long Short-Term Memory}},
url = {http://www.mitpressjournals.org/doi/10.1162/neco.1997.9.8.1735},
volume = {9},
year = {1997}
}
@inproceedings{Tavakoliy2017,
abstract = {To bridge the gap between humans and machines in image understanding and describing, we need further insight into how people describe a perceived scene. In this paper, we study the agreement between bottom-up saliency-based visual attention and object referrals in scene description constructs. We investigate the properties of human-written descriptions and machine-generated ones. We then propose a saliency-boosted image captioning model in order to investigate benefits from low-level cues in language models. We learn that (1) humans mention more salient objects earlier than less salient ones in their descriptions, (2) the better a captioning model performs, the better attention agreement it has with human descriptions, (3) the proposed saliency-boosted model, compared to its baseline form, does not improve significantly on the MS COCO database, indicating explicit bottom-up boosting does not help when the task is well learnt and tuned on a data, (4) a better generalization is, however, observed for the saliency-boosted model on unseen data.},
author = {Tavakoliy, Hamed R. and Shetty, Rakshith and Borji, Ali and Laaksonen, Jorma},
booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2017.272},
file = {:home/mario/Documents/Mendeley/2017/Tavakoliy et al. - 2017 - Paying Attention to Descriptions Generated by Image Captioning Models.pdf:pdf},
isbn = {978-1-5386-1032-9},
month = {oct},
pages = {2506--2515},
publisher = {IEEE},
title = {{Paying Attention to Descriptions Generated by Image Captioning Models}},
url = {https://code.google.com/archive/p/word2vec/ http://ieeexplore.ieee.org/document/8237534/},
year = {2017}
}
@inproceedings{Hao2018,
abstract = {We introduce a novel approach that is used to convert images into the corresponding language descriptions. This method follows the most popular encoder-decoder architecture. The encoder uses the recently proposed densely convolutional neural network (DenseNet) to extract the feature maps. Meanwhile, the decoder uses the long short time memory (LSTM) to parse the feature maps to descriptions. We predict the next word of descriptions by taking the effective combination of feature maps with word embedding of current input word by “visual attention switch”. Finally, we compare the performance of the proposed model with other baseline models and achieve good results},
author = {Hao, Yanlong and Xie, Jiyang and Lin, Zhiqing},
booktitle = {2018 International Conference on Network Infrastructure and Digital Content (IC-NIDC)},
doi = {10.1109/ICNIDC.2018.8525732},
isbn = {978-1-5386-6066-9},
month = {aug},
pages = {334--338},
publisher = {IEEE},
title = {{Image Caption via Visual Attention Switch on DenseNet}},
url = {https://ieeexplore.ieee.org/document/8525732/},
year = {2018}
}
@inproceedings{Zeiler2014,
author = {Zeiler, Matthew D. and Fergus, Rob},
booktitle = {Proceedings of the 2014 European Conference on Computer Vision (ECCV)},
doi = {10.1007/978-3-319-10590-1_53},
file = {:home/mario/Documents/Mendeley/2014/Zeiler, Fergus - 2014 - Visualizing and Understanding Convolutional Networks.pdf:pdf},
pages = {818--833},
publisher = {Springer, Cham},
title = {{Visualizing and Understanding Convolutional Networks}},
url = {http://link.springer.com/10.1007/978-3-319-10590-1{\_}53},
year = {2014}
}
@inproceedings{Park2017,
abstract = {We address personalization issues of image captioning, which have not been discussed yet in previous research. For a query image, we aim to generate a descriptive sentence, accounting for prior knowledge such as the user's active vo- cabularies in previous documents. As applications of per- sonalized image captioning, we tackle two post automation tasks: hashtag prediction and post generation, on our newly collected Instagram dataset, consisting of1.1M posts from 6.3K users. We propose a novel captioning model named Context Sequence Memory Network (CSMN). Its unique up- dates over previous memory network models include (i) ex- ploiting memory as a repository for multiple types ofcontext information, (ii) appending previously generated words into memory to capture long-term information without suffer- ing from the vanishing gradient problem, and (iii) adopting CNN memory structure to jointly represent nearby ordered memory slots for better context understanding. With quan- titative evaluation and user studies via Amazon Mechanical Turk, we show the effectiveness ofthe three novel features of CSMN and its performance enhancement for personalized image captioning over state-of-the-art captioning models. 1.},
address = {Honolulu, USA},
author = {Park, Cesc Chunseong and Kim, Byeongchang and Kim, Gunhee},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.681},
file = {:home/mario/Documents/Mendeley/2017/Park, Kim, Kim - 2017 - Attend to You Personalized Image Captioning with Context Sequence Memory Networks.pdf:pdf},
isbn = {978-1-5386-0457-1},
month = {jul},
pages = {6432--6440},
publisher = {IEEE},
title = {{Attend to You: Personalized Image Captioning with Context Sequence Memory Networks}},
url = {http://ieeexplore.ieee.org/document/8100164/},
year = {2017}
}
@inproceedings{Lin2015,
abstract = {This paper proposes a novel framework for generating lingual descriptions of indoor scenes. Whereas substantial efforts have been made to tackle this problem, previous approaches focusing primarily on generating a single sentence for each image, which is not sufficient for describing complex scenes. We attempt to go beyond this, by generating coherent descriptions with multiple sentences. Our approach is distinguished from conventional ones in several aspects: (1) a 3D visual parsing system that jointly infers objects, attributes, and relations; (2) a generative grammar learned automatically from training text; and (3) a text generation algorithm that takes into account the coherence among sentences. Experiments on the augmented NYU-v2 dataset show that our framework can generate natural descriptions with substantially higher ROGUE scores compared to those produced by the baseline.},
archivePrefix = {arXiv},
arxivId = {1503.00064},
author = {Lin, Dahua and Fidler, Sanja and Kong, Chen and Urtasun, Raquel},
booktitle = {Procedings of the British Machine Vision Conference 2015},
doi = {10.5244/C.29.93},
eprint = {1503.00064},
file = {:home/mario/Documents/Mendeley/2015/Lin et al. - 2015 - Generating Multi-sentence Natural Language Descriptions of Indoor Scenes.pdf:pdf},
isbn = {1-901725-53-7},
publisher = {British Machine Vision Association},
title = {{Generating Multi-sentence Natural Language Descriptions of Indoor Scenes}},
url = {http://arxiv.org/abs/1503.00064 http://www.bmva.org/bmvc/2015/papers/paper093/index.html},
year = {2015}
}
@inproceedings{Yang2016_RevNet,
abstract = {We propose a novel extension of the encoder-decoder framework, called a review network. The review network is generic and can enhance any existing encoder- decoder model: in this paper, we consider RNN decoders with both CNN and RNN encoders. The review network performs a number of review steps with attention mechanism on the encoder hidden states, and outputs a thought vector after each review step; the thought vectors are used as the input of the attention mechanism in the decoder. We show that conventional encoder-decoders are a special case of our framework. Empirically, we show that our framework improves over state-of- the-art encoder-decoder systems on the tasks of image captioning and source code captioning.},
address = {Barcelona, Spain},
archivePrefix = {arXiv},
arxivId = {1605.07912},
author = {Yang, Zhilin and Yuan, Ye and Wu, Yuexin and Salakhutdinov, Ruslan and Cohen, William W.},
booktitle = {30th Annual Conference on Neural Information Processing Systems (NIPS 2016)},
eprint = {1605.07912},
file = {:home/mario/Documents/Mendeley/2016/Yang et al. - 2016 - Review Networks for Caption Generation(2).pdf:pdf},
issn = {0034-6748},
month = {dec},
pages = {2016--2023},
publisher = {Curran Associates, Inc},
title = {{Review Networks for Caption Generation}},
url = {http://arxiv.org/abs/1605.07912},
year = {2016}
}
@inproceedings{Wang2017,
author = {Wang, Yufei and Lin, Zhe and Shen, Xiaohui and Cohen, Scott and Cottrell, Garrison W.},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.780},
file = {:home/mario/Documents/Mendeley/2017/Wang et al. - 2017 - Skeleton Key Image Captioning by Skeleton-Attribute Decomposition.pdf:pdf},
isbn = {978-1-5386-0457-1},
month = {jul},
pages = {7378--7387},
publisher = {IEEE},
title = {{Skeleton Key: Image Captioning by Skeleton-Attribute Decomposition}},
url = {http://ieeexplore.ieee.org/document/8100263/},
year = {2017}
}
@inproceedings{Ushiku2012,
address = {New York, New York, USA},
author = {Ushiku, Yoshitaka and Harada, Tatsuya and Kuniyoshi, Yasuo},
booktitle = {Proceedings of the 20th ACM international conference on Multimedia - MM '12},
doi = {10.1145/2393347.2393424},
file = {:home/mario/Documents/Mendeley/2012/Ushiku, Harada, Kuniyoshi - 2012 - Efficient image annotation for automatic sentence generation.pdf:pdf},
isbn = {9781450310895},
pages = {549},
publisher = {ACM Press},
title = {{Efficient image annotation for automatic sentence generation}},
url = {http://dl.acm.org/citation.cfm?doid=2393347.2393424},
year = {2012}
}
@inproceedings{Kim2014,
address = {Stroudsburg, PA, USA},
author = {Kim, Yoon},
booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
doi = {10.3115/v1/D14-1181},
file = {:home/mario/Documents/Mendeley/2014/Kim - 2014 - Convolutional Neural Networks for Sentence Classification.pdf:pdf},
pages = {1746--1751},
publisher = {Association for Computational Linguistics},
title = {{Convolutional Neural Networks for Sentence Classification}},
url = {http://aclweb.org/anthology/D14-1181},
year = {2014}
}
@article{Young2018,
author = {Young, Tom and Hazarika, Devamanyu and Poria, Soujanya and Cambria, Erik},
doi = {10.1109/MCI.2018.2840738},
file = {:home/mario/Documents/Mendeley/2018/Young et al. - 2018 - Recent Trends in Deep Learning Based Natural Language Processing Review Article.pdf:pdf},
issn = {1556-603X},
journal = {IEEE Computational Intelligence Magazine},
month = {aug},
number = {3},
pages = {55--75},
title = {{Recent Trends in Deep Learning Based Natural Language Processing [Review Article]}},
url = {https://ieeexplore.ieee.org/document/8416973/},
volume = {13},
year = {2018}
}
@inproceedings{Kiros2014_LBL,
abstract = {We introduce two multimodal neural language models: models of natural language that can be conditioned on other modalities. An image-text multimodal neural language model can be used to retrieve images given complex sentence queries, retrieve phrase descriptions given image queries, as well as generate text conditioned on images. We show that in the case of image-text modelling we can jointly learn word representa-tions and image features by training our models together with a convolutional network. Unlike many of the existing methods, our approach can generate sentence descriptions for images with-out the use of templates, structured prediction, and/or syntactic trees. While we focus on image-text modelling, our algorithms can be easily ap-plied to other modalities such as audio.},
address = {Beijing, China},
author = {Kiros, Ryan and Zemel, R and Salakhutdinov, Ruslan},
booktitle = {Proceedings of the 31st International Conference on Machine Learning (ICML 2014)},
file = {:home/mario/Documents/Mendeley/2014/Kiros, Zemel, Salakhutdinov - 2014 - Multimodal Neural Language Models.pdf:pdf},
isbn = {9781634393973},
keywords = {Image Tag Inference},
pages = {595--603},
publisher = {Proceedings of Machine Learning Research},
title = {{Multimodal Neural Language Models}},
url = {http://proceedings.mlr.press/v32/kiros14.pdf},
volume = {32},
year = {2014}
}
@inproceedings{Elliott2015,
abstract = {The Visual Dependency Representation (VDR) is an explicit model of the spa-tial relationships between objects in an im-age. In this paper we present an approach to training a VDR Parsing Model without the extensive human supervision used in previous work. Our approach is to find the objects mentioned in a given descrip-tion using a state-of-the-art object detec-tor, and to use successful detections to pro-duce training data. The description of an unseen image is produced by first predict-ing its VDR over automatically detected objects, and then generating the text with a template-based generation model using the predicted VDR. The performance of our approach is comparable to a state-of-the-art multimodal deep neural network in images depicting actions.},
address = {Stroudsburg, PA, USA},
author = {Elliott, Desmond and de Vries, Arjen},
booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
doi = {10.3115/v1/P15-1005},
file = {:home/mario/Documents/Mendeley/2015/Elliott, de Vries - 2015 - Describing Images using Inferred Visual Dependency Representations.pdf:pdf},
pages = {42--52},
publisher = {Association for Computational Linguistics},
title = {{Describing Images using Inferred Visual Dependency Representations}},
url = {http://aclweb.org/anthology/P15-1005},
year = {2015}
}
@incollection{Anderson2016,
abstract = {There is considerable interest in the task of automatically generating image captions. However, evaluation is challenging. Existing automatic evaluation metrics are primarily sensitive to n-gram overlap, which is neither necessary nor sufficient for the task of simulating human judgment. We hypothesize that semantic propositional content is an important component of human caption evaluation, and propose a new automated caption evaluation metric defined over scene graphs coined SPICE. Extensive evaluations across a range of models and datasets indicate that SPICE captures human judgments over model-generated captions better than other automatic metrics (e.g., system-level correlation of 0.88 with human judgments on the MS COCO dataset, versus 0.43 for CIDEr and 0.53 for METEOR). Furthermore, SPICE can answer questions such as which caption-generator best understands colors? and can caption-generators count?},
author = {Anderson, Peter and Fernando, Basura and Johnson, Mark and Gould, Stephen},
booktitle = {4th European Conference on Computer Vision (ECCV'16)},
doi = {10.1007/978-3-319-46454-1_24},
file = {:home/mario/Documents/Mendeley/2016/Anderson et al. - 2016 - SPICE Semantic Propositional Image Caption Evaluation.pdf:pdf},
pages = {382--398},
publisher = {Springer},
title = {{SPICE: Semantic Propositional Image Caption Evaluation}},
url = {http://link.springer.com/10.1007/978-3-319-46454-1{\_}24},
year = {2016}
}
@inproceedings{Venugopalan2017,
author = {Venugopalan, Subhashini and Hendricks, Lisa Anne and Rohrbach, Marcus and Mooney, Raymond and Darrell, Trevor and Saenko, Kate},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.130},
file = {:home/mario/Documents/Mendeley/2017/Venugopalan et al. - 2017 - Captioning Images with Diverse Objects.pdf:pdf},
isbn = {978-1-5386-0457-1},
month = {jul},
pages = {1170--1178},
publisher = {IEEE},
title = {{Captioning Images with Diverse Objects}},
url = {http://ieeexplore.ieee.org/document/8099613/},
year = {2017}
}
@inproceedings{Mikolov2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
address = {Lake Tahoe, USA},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
booktitle = {Advances in Neural Information Processing Systems 26 (NIPS 2013)},
file = {:home/mario/Documents/Mendeley/2013/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
url = {https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf},
year = {2013}
}
@inproceedings{Ma2016,
abstract = {Generating semantic description draws increasing attention recently. Describing objects with adaptive adjunct words make the sentence more informative. In this paper, we focus on the generation of descriptions for images according to the structural words we have generated such as a tetrad of {\textless};object, attribute, activity, scene{\textgreater}. We propose to use deep machine translation method to generate semantically meaningful descriptions. In particular, the description is composed of objects with appropriate adjunct words, corresponding activities and scene. We propose to use a multi-task method to generate structural words. Taking these words sequence as source language, we train a LSTM encoder-decoder machine translation model to output the target language. Experiments on the benchmark datasets demonstrate our method has better performance than state-of-the-art methods of image caption in terms of language generation metrics.},
author = {Ma, Shubo and Han, Yahong},
booktitle = {2016 IEEE International Conference on Multimedia and Expo (ICME)},
doi = {10.1109/ICME.2016.7552883},
isbn = {978-1-4673-7258-9},
month = {jul},
pages = {1--6},
publisher = {IEEE},
title = {{Describing images by feeding LSTM with structural words}},
url = {http://ieeexplore.ieee.org/document/7552883/},
year = {2016}
}
@article{Olah2016,
author = {Olah, Chris and Carter, Shan},
doi = {10.23915/distill.00001},
file = {:home/mario/Documents/Mendeley/2016/Olah, Carter - 2016 - Attention and Augmented Recurrent Neural Networks.pdf:pdf},
journal = {Distill},
month = {sep},
number = {9},
title = {{Attention and Augmented Recurrent Neural Networks}},
url = {http://distill.pub/2016/augmented-rnns},
volume = {1},
year = {2016}
}
@inproceedings{Jia2015,
abstract = {In this work we focus on the problem of image caption generation. We propose an extension ofthe long short term memory (LSTM) model, which we coin gLSTM for short. In particular, we add semantic information extracted from the image as extra input to each unit of the LSTM block, with the aim ofguiding the model towards solutions that are more tightly coupled to the image content. Additionally, we explore different length normalization strategies for beam search to avoid bias towards short sentences. On various benchmark datasets such as Flickr8K, Flickr30K and MS COCO, we obtain results that are on par with or better than the current state-of-the-art.},
address = {Santiago, Chile},
author = {Jia, Xu and Gavves, Efstratios and Fernando, Basura and Tuytelaars, Tinne},
booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.277},
file = {:home/mario/Documents/Mendeley/2015/Jia et al. - 2015 - Guiding the Long-Short Term Memory Model for Image Caption Generation.pdf:pdf},
isbn = {978-1-4673-8391-2},
month = {dec},
pages = {2407--2415},
publisher = {IEEE},
title = {{Guiding the Long-Short Term Memory Model for Image Caption Generation}},
url = {http://ieeexplore.ieee.org/document/7410634/},
year = {2015}
}
@article{Wei2016,
abstract = {Convolutional Neural Network (CNN) has demonstrated promising performance in single-label image classification tasks. However, how CNN best copes with multi-label images still remains an open problem, mainly due to the complex underlying object layouts and insufficient multi-label training images. In this work, we propose a flexible deep CNN infrastructure, called Hypotheses-CNN-Pooling (HCP), where an arbitrary number of object segment hypotheses are taken as the inputs, then a shared CNN is connected with each hypothesis, and finally the CNN output results from different hypotheses are aggregated with max pooling to produce the ultimate multi-label predictions. Some unique characteristics of this flexible deep CNN infrastructure include: 1) no ground truth bounding box information is required for training; 2) the whole HCP infrastructure is robust to possibly noisy and/or redundant hypotheses; 3) no explicit hypothesis label is required; 4) the shared CNN may be well pre-trained with a large-scale single-label image dataset, e.g. ImageNet; and 5) it may naturally output multi-label prediction results. Experimental results on Pascal VOC2007 and VOC2012 multi-label image datasets well demonstrate the superiority of the proposed HCP infrastructure over other state-of-the-arts. In particular, the mAP reaches 84.2{\%} by HCP only and 90.3{\%} after the fusion with our complementary result in [47] based on hand-crafted features on the VOC2012 dataset, which significantly outperforms the state-of-the-arts with a large margin of more than 7{\%}.},
archivePrefix = {arXiv},
arxivId = {1406.5726},
author = {Wei, Yunchao and Xia, Wei and Lin, Min and Huang, Junshi and Ni, Bingbing and Dong, Jian and Zhao, Yao and Yan, Shuicheng},
doi = {10.1109/TPAMI.2015.2491929},
eprint = {1406.5726},
file = {:home/mario/Documents/Mendeley/2016/Wei et al. - 2016 - HCP A Flexible CNN Framework for Multi-Label Image Classification.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {9},
pages = {1901--1907},
title = {{HCP: A Flexible CNN Framework for Multi-Label Image Classification}},
url = {http://arxiv.org/abs/1406.5726 https://ieeexplore.ieee.org/document/7305792/},
volume = {38},
year = {2016}
}
@inproceedings{Liu2017_MAT,
address = {Melbourne, Australia},
author = {Liu, Chang and Sun, Fuchun and Wang, Changhu and Wang, Feng and Yuille, Alan},
booktitle = {26th International Joint Conference on Artificial Intelligence (IJCAI)},
doi = {10.24963/ijcai.2017/563},
file = {:home/mario/Documents/Mendeley/2017/Liu et al. - 2017 - MAT A Multimodal Attentive Translator for Image Captioning.pdf:pdf},
isbn = {9780999241103},
month = {aug},
pages = {4033--4039},
publisher = {International Joint Conferences on Artificial Intelligence Organization},
title = {{MAT: A Multimodal Attentive Translator for Image Captioning}},
url = {https://www.ijcai.org/proceedings/2017/563},
year = {2017}
}
@article{Wang2018,
abstract = {Image captioning is a challenging task that combines the field of computer vision and natural language processing. A variety of approaches have been proposed to achieve the goal of automatically describing an image, and recurrent neural network (RNN) or long-short term memory (LSTM) based models dominate this field. However, RNNs or LSTMs cannot be calculated in parallel and ignore the underlying hierarchical structure of a sentence. In this paper , we propose a framework that only employs convolu-tional neural networks (CNNs) to generate captions. Owing to parallel computing, our basic model is around 3× faster than NIC (an LSTM-based model) during training time, while also providing better results. We conduct extensive experiments on MSCOCO and investigate the influence of the model width and depth. Compared with LSTM-based models that apply similar attention mechanisms, our proposed models achieves comparable scores of BLEU-1,2,3,4 and METEOR, and higher scores of CIDEr. We also test our model on the paragraph annotation dataset [22], and get higher CIDEr score compared with hierarchical LSTMs.},
archivePrefix = {arXiv},
arxivId = {1805.09019v1},
author = {Wang, Qingzhong and Chan, Antoni B},
eprint = {1805.09019v1},
file = {:home/mario/Documents/Mendeley/2018/Wang, Chan - 2018 - CNNCNN Convolutional Decoders for Image Captioning.pdf:pdf},
journal = {CoRR},
title = {{CNN+CNN: Convolutional Decoders for Image Captioning}},
url = {https://arxiv.org/pdf/1805.09019.pdf},
year = {2018}
}
@inproceedings{Verma2014,
abstract = {Building bilateral semantic associations between images and texts is among the fundamental problems in computer vision. In this paper, we study two complementary cross-modal prediction tasks: (i) predicting text(s) given an image ("Im2Text"), and (ii) predicting image(s) given a piece of text ("Text2Im"). We make no assumption on the specific form of text; i.e., it could be either a set of labels, phrases, or even captions. We pose both these tasks in a retrieval framework. For Im2Text, given a query image, our goal is to retrieve a ranked list of semantically relevant texts from an independent textcorpus (i.e., texts with no corresponding images). Similarly, for Text2Im, given a query text, we aim to retrieve a ranked list of semantically relevant images from a collection of unannotated images (i.e., images without any associated textual meta-data). We propose a novel Structural SVM based unified formulation for these two tasks. For both visual and textual data, two types of representations are investigated. These are based on: (1) unimodal probability distributions over topics learned using latent Dirichlet allocation, and (2) explicitly learned multi-modal correlations using canonical correlation analysis. Extensive experiments on three popular datasets (two medium and one web-scale) demonstrate that our framework gives promising results compared to existing models under various settings, thus confirming its efficacy for both the tasks.},
author = {Verma, Yashaswi and Jawahar, C. V.},
booktitle = {Proceedings of the British Machine Vision Conference 2014},
doi = {10.5244/C.28.97},
file = {:home/mario/Documents/Mendeley/2014/Verma, Jawahar - 2014 - Im2Text and Text2Im Associating Images and Texts for Cross-Modal Retrieval.pdf:pdf},
isbn = {1-901725-52-9},
publisher = {British Machine Vision Association},
title = {{Im2Text and Text2Im: Associating Images and Texts for Cross-Modal Retrieval}},
url = {http://www.bmva.org/bmvc/2014/papers/paper089/index.html},
year = {2014}
}
@inproceedings{Dai2017_CL,
abstract = {Image captioning, a popular topic in computer vision, has achieved substantial progress in recent years. However, the distinctiveness of natural descriptions is often overlooked in previous work. It is closely related to the quality of captions, as distinctive captions are more likely to describe images with their unique aspects. In this work, we propose a new learning method, Contrastive Learning (CL), for image captioning. Specifically, via two constraints formulated on top of a reference model, the proposed method can encourage distinctiveness, while maintaining the overall quality of the generated captions. We tested our method on two challenging datasets, where it improves the baseline model by significant margins. We also showed in our studies that the proposed method is generic and can be used for models with various structures.},
address = {Long Beach, USA},
author = {Dai, Bo and Lin, Dahua},
booktitle = {31st Conference on Neural Information Processing Systems (NIPS 2017)},
file = {:home/mario/Documents/Mendeley/2017/Dai, Lin - 2017 - Contrastive Learning for Image Captioning.pdf:pdf},
title = {{Contrastive Learning for Image Captioning}},
url = {https://papers.nips.cc/paper/6691-contrastive-learning-for-image-captioning.pdf},
year = {2017}
}
@article{Yang2019,
abstract = {Automatically generating a natural sentence describing the content of an image has been extensively researched in artificial intelligence recently, and it bridges the gap between computer vision and natural language processing communities. Most of existing captioning frameworks rely heavily on the visual content, while rarely being aware of the sentimental information. In this paper, we introduce the affective concepts to enhance the emotion expressibility of text descriptions. We achieve this goal by composing appropriate emotional concepts to sentences, which is calculated from large-scale visual and textual repositories by learning both content and linguistic modules. We extract visual and textual representations respectively, followed by combining the latent codes of the two components into a low-dimensional subspace. After that, we decode the combined latent representations and finally generate the affective image captions. We evaluate our method on the SentiCap dataset, which was established with sentimental adjective noun pairs, and evaluate the emotional descriptions with several qualitative and human inception metrics. The experimental results demonstrate the capability of our method for analyzing the latent emotion of an image and providing the affective description which caters to human cognition.},
author = {Yang, Jufeng and Sun, Yan and Liang, Jie and Ren, Bo and Lai, Shang Hong},
doi = {10.1016/j.neucom.2018.03.078},
file = {:home/mario/Documents/Mendeley/2019/Yang et al. - 2019 - Image captioning by incorporating affective concepts learned from both visual and textual components.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Affective concepts,Emotion recognition,Image captioning},
pages = {56--68},
publisher = {Elsevier B.V.},
title = {{Image captioning by incorporating affective concepts learned from both visual and textual components}},
url = {https://doi.org/10.1016/j.neucom.2018.03.078},
volume = {328},
year = {2019}
}
@article{Ratnaparkhi2000,
address = {Washington, USA},
archivePrefix = {arXiv},
arxivId = {arXiv:cs/0006028v1},
author = {Ratnaparkhi, Adwait},
eprint = {0006028v1},
file = {:home/mario/Documents/Mendeley/2000/Ratnaparkhi - 2000 - Trainable Methods for Surface Natural Language Generation.pdf:pdf},
journal = {6th Applied Natural Language Processing Conference (ANLP)},
pages = {194--201},
primaryClass = {arXiv:cs},
title = {{Trainable Methods for Surface Natural Language Generation}},
url = {https://www.aclweb.org/anthology/papers/A/A00/A00-2026/},
year = {2000}
}
@inproceedings{Mathews2016,
abstract = {The recent progress on image recognition and language mod-eling is making automatic description of image content a reality. However, stylized, non-factual aspects of the written description are missing from the current systems. One such style is descriptions with emotions, which is commonplace in everyday communication, and influences decision-making and interpersonal relationships. We design a system to describe an image with emotions, and present a model that automatically generates captions with positive or negative sentiments. We propose a novel switching recurrent neural network with word-level regularization, which is able to produce emotional image captions using only 2000+ training sentences containing sentiments. We evaluate the captions with different automatic and crowd-sourcing metrics. Our model compares favourably in common quality metrics for image captioning. In 84.6{\%} of cases the generated positive captions were judged as being at least as descriptive as the factual captions. Of these positive captions 88{\%} were confirmed by the crowd-sourced workers as having the appropriate sentiment.},
address = {Phoneix, Arizona},
archivePrefix = {arXiv},
arxivId = {1510.01431v2},
author = {Mathews, Alexander and Xie, Lexing and He, Xuming},
booktitle = {Proceedings of the 30th AAAI Conference on Artificial Intelligence},
eprint = {1510.01431v2},
file = {:home/mario/Documents/Mendeley/2016/Mathews, Xie, He - 2016 - SentiCap Generating Image Descriptions with Sentiments.pdf:pdf},
pages = {3574--3580},
title = {{SentiCap: Generating Image Descriptions with Sentiments}},
url = {https://arxiv.org/pdf/1510.01431.pdf},
year = {2016}
}
@article{Hodosh2013a,
abstract = {The ability to associate images with natural language sentences that describe what is depicted in them is a hallmark of image understanding, and a prerequisite for applications such as sentence-based image search. In analogy to image search, we propose to frame sentence-based image annotation as the task of ranking a given pool of captions. We introduce a new benchmark collection for sentence-based image description and search, consisting of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. We introduce a number of systems that perform quite well on this task, even though they are only based on features that can be obtained with minimal supervision. Our results clearly indicate the importance of training on multiple captions per image, and of capturing syntactic (word order-based) and semantic features of these captions. We also perform an in-depth comparison of human and automatic evaluation metrics for this task, and propose strategies for collecting human judgments cheaply and on a very large scale, allowing us to augment our collection with additional relevance judgments of which captions describe which image. Our analysis shows that metrics that consider the ranked list of results for each query image or sentence are significantly more robust than metrics that are based on a single response per query. Moreover, our study suggests that the evaluation of ranking-based image description systems may be fully automated.},
author = {Hodosh, Micah and Young, Peter and Hockenmaier, Julia},
doi = {10.1613/jair.3994},
file = {:home/mario/Documents/Mendeley/2013/Hodosh, Young, Hockenmaier - 2013 - Framing Image Description as a Ranking Task Data, Models and Evaluation Metrics.pdf:pdf},
isbn = {9781577357384},
issn = {1076-9757},
journal = {Journal of Artificial Intelligence Research},
month = {aug},
pages = {853--899},
title = {{Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics}},
url = {https://jair.org/index.php/jair/article/view/10833},
volume = {47},
year = {2013}
}
@inproceedings{Elliott2014,
abstract = {Image description is a new natural language generation task, where the aim is to generate a human-like description of an image. The evaluation of computer-generated text is a notoriously difficult problem, however , the quality of image descriptions has typically been measured using unigram BLEU and human judgements. The focus of this paper is to determine the correlation of automatic measures with human judgements for this task. We estimate the correlation of unigram and Smoothed BLEU, TER, ROUGE-SU4, and Meteor against human judgements on two data sets. The main finding is that unigram BLEU has a weak correlation, and Meteor has the strongest correlation with human judgements.},
address = {Baltimore, USA},
author = {Elliott, Desmond and Keller, Frank},
booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics},
file = {:home/mario/Documents/Mendeley/2014/Elliott, Keller - 2014 - Comparing Automatic Evaluation Measures for Image Description.pdf:pdf},
pages = {452--457},
publisher = {Association for Computational Linguistics},
title = {{Comparing Automatic Evaluation Measures for Image Description}},
url = {http://www.aclweb.org/anthology/P14-2074},
year = {2014}
}
@inproceedings{Ren2016,
address = {New York, New York, USA},
author = {Ren, Zhou and Jin, Hailin and Lin, Zhe and Fang, Chen and Yuille, Alan},
booktitle = {Proceedings of the 2016 ACM on Multimedia Conference - MM '16},
doi = {10.1145/2964284.2967212},
isbn = {9781450336031},
keywords = {gaussian embedding,image classification,text-based image retrieval,visual-semantic embedding},
pages = {207--211},
publisher = {ACM Press},
title = {{Joint Image-Text Representation by Gaussian Visual-Semantic Embedding}},
url = {http://dl.acm.org/citation.cfm?doid=2964284.2967212},
year = {2016}
}
@inproceedings{Niu2017,
author = {Niu, Zhenxing and Zhou, Mo and Wang, Le and Gao, Xinbo and Hua, Gang},
booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2017.208},
file = {:home/mario/Documents/Mendeley/2017/Niu et al. - 2017 - Hierarchical Multimodal LSTM for Dense Visual-Semantic Embedding.pdf:pdf},
isbn = {978-1-5386-1032-9},
month = {oct},
pages = {1899--1907},
publisher = {IEEE},
title = {{Hierarchical Multimodal LSTM for Dense Visual-Semantic Embedding}},
url = {http://ieeexplore.ieee.org/document/8237470/},
year = {2017}
}
@inproceedings{Jia2014,
address = {New York, New York, USA},
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
booktitle = {Proceedings of the ACM International Conference on Multimedia},
doi = {10.1145/2647868.2654889},
file = {:home/mario/Documents/Mendeley/2014/Jia et al. - 2014 - Caffe Convolutional Architecture for Fast Feature Embedding.pdf:pdf},
isbn = {9781450330633},
keywords = {computer vision,machine learning,neural networks,open source,parallel computation},
pages = {675--678},
publisher = {ACM Press},
title = {{Caffe: Convolutional Architecture for Fast Feature Embedding}},
url = {http://dl.acm.org/citation.cfm?doid=2647868.2654889},
year = {2014}
}
@inproceedings{Ferraro2015,
abstract = {Integrating vision and language has long been a dream in work on artificial intelligence (AI). In the past two years, we have witnessed an explosion of work that brings together vision and language from images to videos and beyond. The available corpora have played a crucial role in advancing this area of research. In this paper, we propose a set of quality met-rics for evaluating and analyzing the vision {\&} language datasets and categorize them accordingly. Our analyses show that the most recent datasets have been using more complex language and more abstract concepts, however, there are different strengths and weaknesses in each.},
address = {Lisbon, Portugal},
author = {Ferraro, Francis and Mostafazadeh, Nasrin and Huang, Ting-Hao and Vanderwende, Lucy and Devlin, Jacob and Galley, Michel and Mitchell, Margaret},
booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
file = {:home/mario/Documents/Mendeley/2015/Ferraro et al. - 2015 - A Survey of Current Datasets for Vision and Language Research.pdf:pdf},
pages = {207--2013},
publisher = {Association for Computational Linguistics},
title = {{A Survey of Current Datasets for Vision and Language Research}},
url = {http://www.flickr.com},
year = {2015}
}
@inproceedings{Yang2017,
abstract = {Dense captioning is a newly emerging computer vision topic for understanding images with dense language descriptions. The goal is to densely detect visual concepts (e.g., objects, object parts, and interactions between them) from images, labeling each with a short descriptive phrase. We identify two key challenges of dense captioning that need to be properly addressed when tackling the problem. First, dense visual concept annotations in each image are associated with highly overlapping target regions, making accurate localization of each visual concept challenging. Second, the large amount of visual concepts makes it hard to recognize each of them by appearance alone. We propose a new model pipeline based on two novel ideas, joint inference and context fusion, to alleviate these two challenges. We design our model architecture in a methodical manner and thoroughly evaluate the variations in architecture. Our final model, compact and efficient, achieves state-of-the-art accuracy on Visual Genome [23] for dense captioning with a relative gain of 73{\%} compared to the previous best algorithm. Qualitative experiments also reveal the semantic capabilities of our model in dense captioning.},
address = {Honolulu, USA},
author = {Yang, Linjie and Tang, Kevin and Yang, Jianchao and Li, Li-Jia},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.214},
file = {:home/mario/Documents/Mendeley/2017/Yang et al. - 2017 - Dense Captioning with Joint Inference and Visual Context.pdf:pdf},
isbn = {978-1-5386-0457-1},
month = {jul},
pages = {1978--1987},
publisher = {IEEE},
title = {{Dense Captioning with Joint Inference and Visual Context}},
url = {http://ieeexplore.ieee.org/document/8099697/},
year = {2017}
}
@inproceedings{Anderson2018_SemiSup,
abstract = {Image captioning models are becoming increasingly successful at describing the content of images in restricted domains. However, if these models are to function in the wild - for example, as assistants for people with impaired vision - a much larger number and variety of visual concepts must be understood. To address this problem, we teach image captioning models new visual concepts from labeled images and object detection datasets. Since image labels and object classes can be interpreted as partial captions, we formulate this problem as learning from partially-specified sequence data. We then propose a novel algorithm for training sequence models, such as recurrent neural networks, on partially-specified sequences which we represent using finite state automata. In the context of image captioning, our method lifts the restriction that previously required image captioning models to be trained on paired image-sentence corpora only, or otherwise required specialized model architectures to take advantage of alternative data modalities. Applying our approach to an existing neural captioning model, we achieve state of the art results on the novel object captioning task using the COCO dataset. We further show that we can train a captioning model to describe new visual concepts from the Open Images dataset while maintaining competitive COCO evaluation scores.},
address = {Montreal},
archivePrefix = {arXiv},
arxivId = {1806.06004},
author = {Anderson, Peter and Gould, Stephen and Johnson, Mark},
booktitle = {32nd Conference on Neural Information Processing Systems (NeurIPS 2018)},
eprint = {1806.06004},
file = {:home/mario/Documents/Mendeley/2018/Anderson, Gould, Johnson - 2018 - Partially-Supervised Image Captioning.pdf:pdf},
title = {{Partially-Supervised Image Captioning}},
url = {http://arxiv.org/abs/1806.06004},
year = {2018}
}
@inproceedings{Soh2016,
abstract = {Automatic image caption generation brings together recent advances in natural language processing and computer vision. This work implements a generative CNN-LSTM model that beats human baselines by 2.7 BLEU-4 points and is close to matching (3.8 CIDEr points lower) the current state of the art. Experiments on the MSCOCO dataset set shows that it generates sensible and accurate captions in a majority of cases, and hyperparameter tuning using dropout and number of LSTM layers allow us to alleviate the effects of overfitting. We also demonstrate that semantically-close emitted words (e.g. 'plate' and 'bowl') move the LSTM hidden state in similar ways despite differing previous contexts, and that diver-gences in hidden state occur only upon emission of semantically-distant words (e.g. 'vase' and 'food'). This gives semantic meaning to the interaction between learned word embeddings and the LSTM hidden states. To our knowledge, this is a novel contribution to the literature.},
author = {Soh, Moses},
booktitle = {Thirtieth Conference on Neural Information Processing Systems (NIPS 2016)},
file = {:home/mario/Documents/Mendeley/2016/Soh - 2016 - Learning CNN-LSTM Architectures for Image Caption Generation.pdf:pdf},
title = {{Learning CNN-LSTM Architectures for Image Caption Generation}},
url = {https://cs224d.stanford.edu/reports/msoh.pdf},
year = {2016}
}
@article{He2019,
abstract = {Recently, attribute has demonstrated its effectiveness in guiding image captioning system. However, most attributes based image captioning methods treat the attributes prediction task as a separate task and rely on a standalone stage to obtain the attributes for the given image, e.g., a pre-trained network like Fully Convolutional Neural Network (FCN) is usually adopted. Inherently, they ignore the correlation between the attribute prediction task and image representation extraction task, and at the same time increases the complexity of the image captioning system. In this paper, we aim to couple the attributes prediction stage and image representation extraction stage tightly and propose a novel and efficient image captioning framework called Visual-Densely Semantic Attention Network(VD-SAN). In particular, the whole captioning system consists of shared convolutional layers from Dense Convolutional Network (DenseNet), which are further split into a semantic attributes prediction branch and an image feature extraction branch, two semantic attention models, and a long short-term memory networks (LSTM) for caption generation. To evaluate the proposed architecture, we construct Flickr30K-ATT and MS-COCO-ATT datasets based on the original popular image caption datasets Flickr30K and MS COCO respectively, and each image from Flickr30K-ATT or MS-COCO-ATT is annotated with an attribute list in addition to the corresponding caption. Empirical results demonstrate that our captioning system can achieve significant improvements over state-of-the-art approaches.},
author = {He, Xinwei and Yang, Yang and Shi, Baoguang and Bai, Xiang},
doi = {10.1016/j.neucom.2018.02.106},
file = {:home/mario/Documents/Mendeley/2019/He et al. - 2019 - VD-SAN Visual-Densely Semantic Attention Network for Image Caption Generation.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Convolutional neural network,Image caption,Long short-term memory networks,Semantic attributes},
pages = {48--55},
publisher = {Elsevier B.V.},
title = {{VD-SAN: Visual-Densely Semantic Attention Network for Image Caption Generation}},
url = {https://doi.org/10.1016/j.neucom.2018.02.106},
volume = {328},
year = {2019}
}
@misc{Zheng2017,
abstract = {Matching images and sentences demands a fine understanding of both modalities. In this paper, we propose a new system to discriminatively embed the image and text to a shared visual-textual space. In this field, most existing works apply the ranking loss to pull the positive image / text pairs close and push the negative pairs apart from each other. However, directly deploying the ranking loss is hard for network learning, since it starts from the two heterogeneous features to build inter-modal relationship. To address this problem, we propose the instance loss which explicitly considers the intra-modal data distribution. It is based on an unsupervised assumption that each image / text group can be viewed as a class. So the network can learn the fine granularity from every image/text group. The experiment shows that the instance loss offers better weight initialization for the ranking loss, so that more discriminative embeddings can be learned. Besides, existing works usually apply the off-the-shelf features, i.e., word2vec and fixed visual feature. So in a minor contribution, this paper constructs an end-to-end dual-path convolutional network to learn the image and text representations. End-to-end learning allows the system to directly learn from the data and fully utilize the supervision. On two generic retrieval datasets (Flickr30k and MSCOCO), experiments demonstrate that our method yields competitive accuracy compared to state-of-the-art methods. Moreover, in language based person retrieval, we improve the state of the art by a large margin. The code has been made publicly available.},
archivePrefix = {arXiv},
arxivId = {1711.05535},
author = {Zheng, Zhedong and Zheng, Liang and Garrett, Michael and Yang, Yi and Shen, Yi-Dong},
eprint = {1711.05535},
file = {:home/mario/Documents/Mendeley/2017/Zheng et al. - 2017 - Dual-Path Convolutional Image-Text Embedding with Instance Loss.pdf:pdf},
month = {nov},
number = {8},
pages = {1--15},
title = {{Dual-Path Convolutional Image-Text Embedding with Instance Loss}},
url = {http://arxiv.org/abs/1711.05535 https://www.groundai.com/project/dual-path-convolutional-image-text-embedding/},
volume = {14},
year = {2017}
}
@inproceedings{Yang2011,
abstract = {We propose a sentence generation strategy that describes images by predicting the most likely nouns, verbs, scenes and prepositions that make up the core sentence structure. The input are initial noisy estimates of the objects and scenes detected in the image using state of the art trained detectors. As predicting actions from still images directly is unreliable, we use a language model trained from the English Gi- gaword corpus to obtain their estimates; to- gether with probabilities of co-located nouns, scenes and prepositions. We use these esti- mates as parameters on a HMM that models the sentence generation process, with hidden nodes as sentence components and image de- tections as the emissions. Experimental re- sults show that our strategy of combining vi- sion and language produces readable and de- scriptive sentences compared to naive strate- gies that use vision alone.},
author = {Yang, Yezhou and Teo, C.L. and {Daum{\'{e}} III}, H. and Aloimonos, Y.},
booktitle = {The 2011 Conference on Empirical Methods in Natural Language Processing},
file = {:home/mario/Documents/Mendeley/2011/Yang et al. - 2011 - Corpus-Guided Sentence Generation of Natural Images.pdf:pdf},
isbn = {978-1-937284-11-4},
issn = {0023-7205},
pages = {444--454},
title = {{Corpus-Guided Sentence Generation of Natural Images}},
url = {http://hal3.name/docs/daume11generation.pdf},
year = {2011}
}
