Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Donahue2013,
abstract = {Models based on deep convolutional networks have dom- inated recent image interpretation tasks; we investigate whether models which are also recurrent, or “temporally deep”, are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image de- scription and retrieval problems, and video narration chal- lenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averag- ing for sequential processing, recurrent convolutional mod- els are “doubly deep” in that they can be compositional in spatial and temporal “layers”. Such models may have advantages when target concepts are complex and/or train- ing data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the net- work state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural lan- guage text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual rep- resentations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized. 1. Introduction Recognition and description of images and videos is a fundamental challenge of computer vision. Dramatic Visual},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.4389v3},
author = {Donahue, Jeff and Hendricks, Lisa Anne and Guadarrama, Sergio and Rohrbach, Marcus and Venugopalan, Subhashini and Darrell, Trevor and Saenko, Kate},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2015.7298878},
eprint = {arXiv:1411.4389v3},
file = {:home/mario/Documents/Mendeley/2015/Donahue et al. - 2015 - Long-term recurrent convolutional networks for visual recognition and description.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {9781467369640},
month = {jun},
number = {3},
pages = {2625--2634},
pmid = {23641138},
publisher = {IEEE},
title = {{Long-term recurrent convolutional networks for visual recognition and description}},
url = {http://ieeexplore.ieee.org/document/7298878/},
volume = {38},
year = {2015}
}
@inproceedings{Kiros2014,
abstract = {Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* - "blue" + "red" is near images of red cars. Sample captions generated for 800 images are made available for comparison.},
archivePrefix = {arXiv},
arxivId = {1411.2539},
author = {Kiros, Ryan and Salakhutdinov, Ruslan and Zemel, Richard S.},
booktitle = {Advances in Neural Information Processing Systems Deep Learning Workshop},
eprint = {1411.2539},
file = {:home/mario/Documents/Mendeley/2014/Kiros, Salakhutdinov, Zemel - 2014 - Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models.pdf:pdf},
title = {{Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models}},
url = {http://arxiv.org/abs/1411.2539},
year = {2014}
}
@inproceedings{Yang2011a,
abstract = {We propose a sentence generation strategy that describes images by predicting the most likely nouns, verbs, scenes and prepositions that make up the core sentence structure. The input are initial noisy estimates of the objects and scenes detected in the image using state of the art trained detectors. As predicting actions from still images directly is unreliable, we use a language model trained from the English Gi- gaword corpus to obtain their estimates; to- gether with probabilities of co-located nouns, scenes and prepositions. We use these esti- mates as parameters on a HMM that models the sentence generation process, with hidden nodes as sentence components and image de- tections as the emissions. Experimental re- sults show that our strategy of combining vi- sion and language produces readable and de- scriptive sentences compared to naive strate- gies that use vision alone.},
author = {Yang, Yezhou and Teo, C.L. and {Daum{\'{e}} III}, H. and Aloimonos, Y.},
booktitle = {The 2011 Conference on Empirical Methods in Natural Language Processing},
file = {:home/mario/Documents/Mendeley/2011/Yang et al. - 2011 - Corpus-Guided Sentence Generation of Natural Images.pdf:pdf},
isbn = {978-1-937284-11-4},
issn = {0023-7205},
pages = {444--454},
title = {{Corpus-Guided Sentence Generation of Natural Images}},
url = {http://hal3.name/docs/daume11generation.pdf},
year = {2011}
}
@inproceedings{Li2011,
abstract = {Studying natural language, and especially how people describe the world around them can help us better understand the visual world. In turn, it can also help us in the quest to generate natural language that describes this world in a human manner. We present a simple yet effec- tive approach to automatically compose im- age descriptions given computer vision based inputs and using web-scale n-grams. Unlike most previous work that summarizes or re- trieves pre-existing text relevant to an image, our method composes sentences entirely from scratch. Experimental results indicate that it is viable to generate simple textual descriptions that are pertinent to the specific content of an image, while permitting creativity in the de- scription – making for more human-like anno- tations than previous approaches.},
author = {Li, Siming and Kulkarni, Girish and Berg, TL and Berg, AC and Choi, Yejin},
booktitle = {Proceedings of the Fifteenth Conference on Computational Natural Language Learning},
file = {:home/mario/Documents/Mendeley/2011/Li et al. - 2011 - Composing simple image descriptions using web-scale n-grams.pdf:pdf},
isbn = {978-1-932432-92-3},
number = {June},
pages = {220--228},
title = {{Composing simple image descriptions using web-scale n-grams}},
url = {http://dl.acm.org/citation.cfm?id=2018962},
year = {2011}
}
@article{Kuznetsova2012,
abstract = {We present a holistic data-driven approach to image description generation, exploit- ing the vast amount of (noisy) parallel im- age data and associated natural language descriptions available on the web. More specifically, given a query image, we re- trieve existing human-composed phrases used to describe visually similar images, then selectively combine those phrases to generate a novel description for the query image. We cast the generation pro- cess as constraint optimization problems, collectively incorporating multiple inter- connected aspects of language composition for content planning, surface realization and discourse structure. Evaluation by hu- man annotators indicates that our final system generates more semantically cor- rect and linguistically appealing descrip- tions than two nontrivial baselines.},
author = {Kuznetsova, Polina and Ordonez, Vicente and Berg, Ac},
file = {:home/mario/Documents/Mendeley/2012/Kuznetsova, Ordonez, Berg - 2012 - Collective generation of natural image descriptions.pdf:pdf},
isbn = {9781937284244},
journal = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics},
number = {July},
pages = {359--368},
title = {{Collective generation of natural image descriptions}},
url = {http://dl.acm.org/citation.cfm?id=2390575},
volume = {1},
year = {2012}
}
@inproceedings{Lebret2015,
abstract = {Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutional Neural Network) and phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on caption syntax statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results in two popular datasets for the task: Flickr30k and the recently proposed Microsoft COCO.},
archivePrefix = {arXiv},
arxivId = {1502.03671},
author = {Lebret, R{\'{e}}mi and Pinheiro, Pedro O. and Collobert, Ronan},
booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
eprint = {1502.03671},
file = {:home/mario/Documents/Mendeley/2015/Lebret, Pinheiro, Collobert - 2015 - Phrase-based Image Captioning.pdf:pdf},
month = {feb},
title = {{Phrase-based Image Captioning}},
url = {http://arxiv.org/abs/1502.03671},
year = {2015}
}
@inproceedings{Xu2015,
abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
address = {Lille, France},
archivePrefix = {arXiv},
arxivId = {1502.03044},
author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
doi = {10.1016/j.exger.2013.05.060},
eprint = {1502.03044},
file = {:home/mario/Documents/Mendeley/2015/Xu et al. - 2015 - Show, Attend and Tell Neural Image Caption Generation with Visual Attention.pdf:pdf},
isbn = {0531-5565},
issn = {05315565},
pages = {2048--2057},
pmid = {23770107},
title = {{Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}},
url = {http://proceedings.mlr.press/v37/xuc15.pdf},
year = {2015}
}
@inproceedings{Pu2016,
abstract = {A novel variational autoencoder is developed to model images, as well as associated labels or captions. The Deep Generative Deconvolutional Network (DGDN) is used as a decoder of the latent image features, and a deep Convolutional Neural Network (CNN) is used as an image encoder; the CNN is used to approximate a distribution for the latent DGDN features/code. The latent code is also linked to generative models for labels (Bayesian support vector machine) or captions (recurrent neural network). When predicting a label/caption for a new image at test, averaging is performed across the distribution of latent codes; this is computationally efficient as a consequence of the learned CNN-based encoder. Since the framework is capable of modeling the image in the presence/absence of associated labels/captions, a new semi-supervised setting is manifested for CNN learning with images; the framework even allows unsupervised CNN learning, based on images alone.},
address = {Barcelona, Spain},
archivePrefix = {arXiv},
arxivId = {arXiv:1609.08976v1},
author = {Pu, Yunchen and Gan, Zhe and Henao, Ricardo and Yuan, Xin and Li, Chunyuan and Stevens, Andrew and Carin, Lawrence},
booktitle = {NIPS'16 Proceedings of the 30th International Conference on Neural Information Processing Systems},
eprint = {arXiv:1609.08976v1},
file = {:home/mario/Documents/Mendeley/2016/Pu et al. - 2016 - Variational Autoencoder for Deep Learning of Images , Labels and Captions.pdf:pdf},
pages = {2360--2368},
title = {{Variational Autoencoder for Deep Learning of Images , Labels and Captions}},
year = {2016}
}
@article{Zheng2017,
abstract = {Matching images and sentences demands a fine understanding of both modalities. In this paper, we propose a new system to discriminatively embed the image and text to a shared visual-textual space. In this field, most existing works apply the ranking loss to pull the positive image / text pairs close and push the negative pairs apart from each other. However, directly deploying the ranking loss is hard for network learning, since it starts from the two heterogeneous features to build inter-modal relationship. To address this problem, we propose the instance loss which explicitly considers the intra-modal data distribution. It is based on an unsupervised assumption that each image / text group can be viewed as a class. So the network can learn the fine granularity from every image/text group. The experiment shows that the instance loss offers better weight initialization for the ranking loss, so that more discriminative embeddings can be learned. Besides, existing works usually apply the off-the-shelf features, i.e., word2vec and fixed visual feature. So in a minor contribution, this paper constructs an end-to-end dual-path convolutional network to learn the image and text representations. End-to-end learning allows the system to directly learn from the data and fully utilize the supervision. On two generic retrieval datasets (Flickr30k and MSCOCO), experiments demonstrate that our method yields competitive accuracy compared to state-of-the-art methods. Moreover, in language based person retrieval, we improve the state of the art by a large margin. The code has been made publicly available.},
archivePrefix = {arXiv},
arxivId = {1711.05535},
author = {Zheng, Zhedong and Zheng, Liang and Garrett, Michael and Yang, Yi and Shen, Yi-Dong},
eprint = {1711.05535},
file = {:home/mario/Documents/Mendeley/2017/Zheng et al. - 2017 - Dual-Path Convolutional Image-Text Embedding with Instance Loss.pdf:pdf},
number = {8},
pages = {1--15},
title = {{Dual-Path Convolutional Image-Text Embedding with Instance Loss}},
url = {http://arxiv.org/abs/1711.05535},
volume = {14},
year = {2017}
}
@inproceedings{Elliott2013,
abstract = {Describing the main event of an image in- volves identifying the objects depicted and predicting the relationships between them. Previous approaches have represented images as unstructured bags of regions, which makes it difficult to accurately predict meaningful relationships between regions. In this pa- per, we introduce visual dependency represen- tations to capture the relationships between the objects in an image, and hypothesize that this representation can improve image de- scription. We test this hypothesis using a new data set of region-annotated images, as- sociated with visual dependency representa- tions and gold-standard descriptions. We de- scribe two template-based description gener- ation models that operate over visual depen- dency representations. In an image descrip- tion task, we find that these models outper- form approaches that rely on object proxim- ity or corpus information to generate descrip- tions on both automatic measures and on hu- man judgements.},
author = {Elliott, Desmond and Keller, Frank},
booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
file = {:home/mario/Documents/Mendeley/2013/Elliott, Keller - 2013 - Image Description using Visual Dependency Representations.pdf:pdf},
isbn = {9781937284978},
issn = {00036951},
number = {October},
pages = {1292--1302},
title = {{Image Description using Visual Dependency Representations}},
year = {2013}
}
@misc{Hrga,
abstract = {As a problem that resides at the intersection of Computer Vision and Natural Language Processing, image captioning has witnessed a rapid progress in a very short time, from initial template-based models to the current ones, based on deep neural networks. This paper gives an overview of current issues and recent research on image captioning, with a special emphasis on models employing deep encoder-decoder architectures. We discuss the advantages and disadvantages of different approaches, along with reviewing some of the most commonly used datasets and evaluation metrics. We point out to some open questions and conclude with directions for future research.},
author = {Hrga, Ingrid},
file = {:home/mario/Documents/Mendeley/Unknown/Hrga - Unknown - Deep Image Captioning Models, Data and Evaluation.pdf:pdf},
keywords = {attention mechanism,deep,encoder-decoder framework,image captioning,neural networks},
title = {{Deep Image Captioning: Models, Data and Evaluation}},
url = {http://www.inf.uniri.hr/files/studiji/poslijediplomski/kvalifikacijski/Hrga{\_}Ingrid{\_}Kvalifikacijski{\_}rad.pdf}
}
@inproceedings{GilbertoMateosOrtiz2015,
abstract = {Given a (static) scene, a human can effort- lessly describe what is going on (who is do- ing what to whom, how, and why). The pro- cess requires knowledge about the world, how it is perceived, and described. In this paper we study the problem of interpreting and verbal- izing visual information using abstract scenes created from collections of clip art images. We propose a model inspired by machine trans- lation operating over a large parallel corpus of visual relations and linguistic descriptions. We demonstrate that this approach produces human-like scene descriptions which are both fluent and relevant, outperforming a num- ber of competitive alternatives based on tem- plates, sentence-based retrieval, and a multi- modal neural language model. 1},
address = {Stroudsburg, PA, USA},
author = {Mateos-Ortiz, Luis Gilberto and Wolff, Clemens and Lapata, Mirella},
booktitle = {Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies},
doi = {10.3115/v1/N15-1174},
file = {:home/mario/Documents/Mendeley/2015/Mateos-Ortiz, Wolff, Lapata - 2015 - Learning to Interpret and Describe Abstract Scenes.pdf:pdf},
pages = {1505--1515},
publisher = {Association for Computational Linguistics},
title = {{Learning to Interpret and Describe Abstract Scenes}},
url = {http://aclweb.org/anthology/N15-1174},
year = {2015}
}
@inproceedings{Fang2015,
abstract = {This paper presents a novel approach for automatically generating image descriptions: visual detectors, language models, and multimodal similarity models learnt directly from a dataset of image captions. We use multiple instance learning to train visual detectors for words that commonly occur in captions, including many different parts of speech such as nouns, verbs, and adjectives. The word detector outputs serve as conditional inputs to a maximum-entropy language model. The language model learns from a set of over 400,000 image descriptions to capture the statistics of word usage. We capture global semantics by re-ranking caption candidates using sentence-level features and a deep multimodal similarity model. Our system is state-of-the-art on the official Microsoft COCO benchmark, producing a BLEU-4 score of 29.1{\%}. When human judges compare the system captions to ones written by other people on our held-out test set, the system captions have equal or better quality 34{\%} of the time.},
archivePrefix = {arXiv},
arxivId = {1411.4952},
author = {Fang, Hao and Gupta, Saurabh and Iandola, Forrest and Srivastava, Rupesh K. and Deng, Li and Dollar, Piotr and Gao, Jianfeng and He, Xiaodong and Mitchell, Margaret and Platt, John C and Zitnick, C Lawrence and Zweig, Geoffrey},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2015.7298754},
eprint = {1411.4952},
file = {:home/mario/Documents/Mendeley/2015/Fang et al. - 2015 - From captions to visual concepts and back.pdf:pdf},
isbn = {978-1-4673-6964-0},
month = {jun},
pages = {1473--1482},
publisher = {IEEE},
title = {{From captions to visual concepts and back}},
url = {http://arxiv.org/abs/1411.4952 http://ieeexplore.ieee.org/document/7298754/},
year = {2015}
}
@incollection{Srivastava2018,
author = {Srivastava, Gargi and Srivastava, Rajeev},
booktitle = {Communications in Computer and Information Science},
doi = {10.1007/978-981-13-0023-3_8},
file = {:home/mario/Documents/Mendeley/2018/Srivastava, Srivastava - 2018 - A Survey on Automatic Image Captioning.pdf:pdf},
isbn = {9789811300226},
issn = {18650929},
keywords = {Computer vision,Image captioning,Scene analysis},
pages = {74--83},
title = {{A Survey on Automatic Image Captioning}},
url = {http://link.springer.com/10.1007/978-981-13-0023-3{\_}8},
volume = {834},
year = {2018}
}
@article{Ordonez2011,
abstract = {We develop and demonstrate automatic image description methods using a large captioned photo collection. One contribution is our technique for the automatic collection of this new dataset -- performing a huge number of Flickr queries and then filtering the noisy results down to 1 million images with associated visually relevant captions. Such a collection allows us to approach the extremely challenging problem of description generation using relatively simple non-parametric methods and produces surprisingly effective results. We also develop methods incorporating many state of the art, but fairly noisy, estimates of image content to produce even more pleasing results. Finally we introduce a new objective performance measure for image captioning.},
author = {Ordonez, V and Kulkarni, G and Berg, Tl},
file = {:home/mario/Documents/Mendeley/2011/Ordonez, Kulkarni, Berg - 2011 - Im2text Describing Images Using 1 Million Captioned Photographs.pdf:pdf},
isbn = {9781618395993},
journal = {Advances in Neural Information Processing Systems (NIPS)},
pages = {1143--1151},
title = {{Im2text: Describing Images Using 1 Million Captioned Photographs}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2011{\_}0671.pdf},
year = {2011}
}
@inproceedings{Mitchell2012,
abstract = {This paper introduces a novel generation system that composes humanlike descriptions of images from computer vision detections. By leveraging syntactically informed word co-occurrence statistics, the generator ﬁlters and constrains the noisy detections output from a vision system to generate syntactic trees that detail what the computer vision system sees. Results show that the generation system outperforms state-of-the-art systems, automatically generating some of the most natural image descriptions to date.},
author = {Mitchell, Margaret and Dodge, Jesse and Goyal, Amit and Yamaguchi, Kota and Stratos, Karl and Han, Xufeng and Mensch, Alyssa and Berg, Alexander C. and Berg, Tamara L. and {Daume III}, Hal},
booktitle = {Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics},
file = {:home/mario/Documents/Mendeley/2012/Mitchell et al. - 2012 - Midge Generating Image Descriptions From Computer Vision Detections.pdf:pdf},
isbn = {978-1-937284-19-0},
pages = {747--756},
title = {{Midge: Generating Image Descriptions From Computer Vision Detections}},
year = {2012}
}
@article{Tan2019,
author = {Tan, Ying Hua and Chan, Chee Seng},
doi = {10.1016/j.neucom.2018.12.026},
file = {:home/mario/Documents/Mendeley/2019/Tan, Chan - 2019 - Phrase-based image caption generator with hierarchical LSTM network.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
month = {mar},
pages = {86--100},
title = {{Phrase-based image caption generator with hierarchical LSTM network}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231218314802},
volume = {333},
year = {2019}
}
@inproceedings{Lin2015,
abstract = {This paper proposes a novel framework for generating lingual descriptions of indoor scenes. Whereas substantial efforts have been made to tackle this problem, previous approaches focusing primarily on generating a single sentence for each image, which is not sufficient for describing complex scenes. We attempt to go beyond this, by generating coherent descriptions with multiple sentences. Our approach is distinguished from conventional ones in several aspects: (1) a 3D visual parsing system that jointly infers objects, attributes, and relations; (2) a generative grammar learned automatically from training text; and (3) a text generation algorithm that takes into account the coherence among sentences. Experiments on the augmented NYU-v2 dataset show that our framework can generate natural descriptions with substantially higher ROGUE scores compared to those produced by the baseline.},
archivePrefix = {arXiv},
arxivId = {1503.00064},
author = {Lin, Dahua and Fidler, Sanja and Kong, Chen and Urtasun, Raquel},
booktitle = {Procedings of the British Machine Vision Conference 2015},
doi = {10.5244/C.29.93},
eprint = {1503.00064},
file = {:home/mario/Documents/Mendeley/2015/Lin et al. - 2015 - Generating Multi-sentence Natural Language Descriptions of Indoor Scenes.pdf:pdf},
isbn = {1-901725-53-7},
publisher = {British Machine Vision Association},
title = {{Generating Multi-sentence Natural Language Descriptions of Indoor Scenes}},
url = {http://arxiv.org/abs/1503.00064 http://www.bmva.org/bmvc/2015/papers/paper093/index.html},
year = {2015}
}
@article{Sharma2018,
abstract = {We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and filtering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image cap-tioning models and show that a model architecture based on Inception-ResNet-v2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.},
author = {Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
file = {:home/mario/Documents/Mendeley/2018/Sharma et al. - 2018 - Conceptual Captions A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning.pdf:pdf},
journal = {Annual Meeting of the Association for Computational Linguistics},
pages = {2556--2565},
title = {{Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning}},
url = {https://en.wikipedia.org/wiki/Alt},
year = {2018}
}
@inproceedings{Yatskar2015,
abstract = {This paper studies generation of descriptive sentences from densely annotated images. Previous work studied generation from automatically detected visual information but produced a limited class of sentences, hindered by currently unreliable recognition of activities and attributes. Instead, we collect human annotations of objects, parts, attributes and activities in images. These annotations allow us to build a significantly more comprehensive model of language generation and allow us to study what visual information is required to generate human-like descriptions. Experiments demonstrate high quality output and that activity annotations and relative spatial location of objects contribute most to producing high quality sentences.},
address = {Stroudsburg, PA, USA},
author = {Yatskar, Mark and Galley, Michel and Vanderwende, Lucy and Zettlemoyer, Luke},
booktitle = {Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014)},
doi = {10.3115/v1/S14-1015},
file = {:home/mario/Documents/Mendeley/2014/Yatskar et al. - 2014 - See No Evil, Say No Evil Description Generation from Densely Labeled Images.pdf:pdf},
pages = {110--120},
publisher = {Association for Computational Linguistics and Dublin City University},
title = {{See No Evil, Say No Evil: Description Generation from Densely Labeled Images}},
url = {http://aclweb.org/anthology/S14-1015},
year = {2014}
}
@article{Patterson2014,
abstract = {In this paper we present the first large-scale scene attribute database. First, we perform crowdsourced human studies to find a taxonomy of 102 discriminative attributes. We discover attributes related to materials, surface properties, lighting, affordances, and spatial layout. Next, we build the 'SUN attribute database' on top of the diverse SUN categorical database. We use crowdsourcing to annotate attributes for 14,340 images from 707 scene categories. We perform numerous experiments to study the interplay between scene attributes and scene categories. We train and evaluate attribute classifiers and then study the feasibility of attributes as an intermediate scene representation for scene classification, zero shot learning, automatic image captioning, semantic image search, and parsing natural images. We show that when used as features for these tasks, low dimensional scene attributes can compete with or improve on the state of the art performance. The experiments suggest that scene attributes are an effective low-dimensional feature for capturing high-level context and semantics in scenes. [ABSTRACT FROM AUTHOR]},
author = {Patterson, Genevieve and Xu, Chen and Su, Hang and Hays, James},
doi = {10.1007/s11263-013-0695-z},
file = {:home/mario/Documents/Mendeley/2014/Patterson et al. - 2014 - The SUN Attribute Database Beyond Categories for Deeper Scene Understanding.pdf:pdf},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
keywords = {Attributes,Crowdsourcing,Image captioning,Scene parsing,Scene understanding},
month = {may},
number = {1-2},
pages = {59--81},
title = {{The SUN Attribute Database: Beyond Categories for Deeper Scene Understanding}},
url = {http://link.springer.com/10.1007/s11263-013-0695-z},
volume = {108},
year = {2014}
}
@inproceedings{Yagcioglu2015,
abstract = {In this paper, we propose a novel query ex-pansion approach for improving transfer-based automatic image captioning. The core idea of our method is to translate the given visual query into a distributional se-mantics based form, which is generated by the average of the sentence vectors ex-tracted from the captions of images visu-ally similar to the input image. Using three image captioning benchmark datasets, we show that our approach provides more ac-curate results compared to the state-of-the-art data-driven methods in terms of both automatic metrics and subjective evalua-tion.},
address = {Stroudsburg, PA, USA},
author = {Yagcioglu, Semih and Erdem, Erkut and Erdem, Aykut and Cakici, Ruket},
booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics},
doi = {10.3115/v1/P15-2018},
file = {:home/mario/Documents/Mendeley/2015/Yagcioglu et al. - 2015 - A Distributed Representation Based Query Expansion Approach for Image Captioning.pdf:pdf},
pages = {106--111},
publisher = {Association for Computational Linguistics},
title = {{A Distributed Representation Based Query Expansion Approach for Image Captioning}},
url = {http://aclweb.org/anthology/P15-2018},
year = {2015}
}
@article{Bai2018,
abstract = {Image captioning means automatically generating a caption for an image. As a recently emerged research area, it is attracting more and more attention. To achieve the goal of image captioning, semantic information of images needs to be captured and expressed in natural languages. Connecting both research communities of computer vision and natural language processing, image captioning is a quite challenging task. Various approaches have been proposed to solve this problem. In this paper, we present a survey on advances in image captioning research. Based on the technique adopted, we classify image captioning approaches into different categories. Representative methods in each category are summarized, and their strengths and limitations are talked about. In this paper, we first discuss methods used in early work which are mainly retrieval and template based. Then, we focus our main attention on neural network based methods, which give state of the art results. Neural network based methods are further divided into subcategories based on the specific framework they use. Each subcategory of neural network based methods are discussed in detail. After that, state of the art methods are compared on benchmark datasets. Following that, discussions on future research directions are presented.},
author = {Bai, Shuang and An, Shan},
doi = {10.1016/j.neucom.2018.05.080},
file = {:home/mario/Documents/Mendeley/2018/Bai, An - 2018 - A survey on automatic image caption generation.pdf:pdf},
isbn = {9789811300226},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Attention mechanism,Deep neural networks,Encoder–decoder framework,Image captioning,Multimodal embedding,Sentence template},
pages = {291--304},
publisher = {Elsevier B.V.},
title = {{A survey on automatic image caption generation}},
volume = {311},
year = {2018}
}
@inproceedings{Ushiku2015,
author = {Ushiku, Yoshitaka and Yamaguchi, Masataka and Mukuta, Yusuke and Harada, Tatsuya},
booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.306},
isbn = {978-1-4673-8391-2},
month = {dec},
pages = {2668--2676},
publisher = {IEEE},
title = {{Common Subspace for Model and Similarity: Phrase Learning for Caption Generation from Images}},
url = {http://ieeexplore.ieee.org/document/7410663/},
year = {2015}
}
@inproceedings{Devlin2015,
abstract = {Two recent approaches have achieved state-of-the-art results in image captioning. The first uses a pipelined process where a set of candidate words is generated by a convolutional neural network (CNN) trained on images, and then a maximum entropy (ME) language model is used to arrange these words into a coherent sentence. The second uses the penultimate activation layer of the CNN as input to a recurrent neural network (RNN) that then generates the caption sequence. In this paper, we compare the merits of these different language modeling approaches for the first time by using the same state-of-the-art CNN as input. We examine issues in the different approaches, including linguistic irregularities, caption repetition, and data set overlap. By combining key aspects of the ME and RNN methods, we achieve a new record performance over previously published results on the benchmark COCO dataset. However, the gains we see in BLEU do not translate to human judgments.},
address = {Stroudsburg, PA, USA},
archivePrefix = {arXiv},
arxivId = {1505.01809},
author = {Devlin, Jacob and Cheng, Hao and Fang, Hao and Gupta, Saurabh and Deng, Li and He, Xiaodong and Zweig, Geoffrey and Mitchell, Margaret},
booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
doi = {10.3115/v1/P15-2017},
eprint = {1505.01809},
file = {:home/mario/Documents/Mendeley/2015/Devlin et al. - 2015 - Language Models for Image Captioning The Quirks and What Works.pdf:pdf},
pages = {100--105},
publisher = {Association for Computational Linguistics},
title = {{Language Models for Image Captioning: The Quirks and What Works}},
url = {http://arxiv.org/abs/1505.01809 http://aclweb.org/anthology/P15-2017},
year = {2015}
}
@inproceedings{Cui2018,
abstract = {Evaluation metrics for image captioning face two challenges. Firstly, commonly used metrics such as CIDEr, METEOR, ROUGE and BLEU often do not correlate well with human judgments. Secondly, each metric has well known blind spots to pathological caption constructions, and rule-based metrics lack provisions to repair such blind spots once identified. For example, the newly proposed SPICE correlates well with human judgments, but fails to capture the syntactic structure of a sentence. To address these two challenges, we propose a novel learning based discriminative evaluation metric that is directly trained to distinguish between human and machine-generated captions. In addition, we further propose a data augmentation scheme to explicitly incorporate pathological transformations as negative examples during training. The proposed metric is evaluated with three kinds of robustness tests and its correlation with human judgments. Extensive experiments show that the proposed data augmentation scheme not only makes our metric more robust toward several pathological transformations, but also improves its correlation with human judgments. Our metric outperforms other metrics on both caption level human correlation in Flickr 8k and system level human correlation in COCO. The proposed approach could be served as a learning based evaluation metric that is complementary to existing rule-based metrics.},
archivePrefix = {arXiv},
arxivId = {1806.06422},
author = {Cui, Yin and Yang, Guandao and Veit, Andreas and Huang, Xun and Belongie, Serge},
booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00608},
eprint = {1806.06422},
file = {:home/mario/Documents/Mendeley/2018/Cui et al. - 2018 - Learning to Evaluate Image Captioning.pdf:pdf},
isbn = {978-1-5386-6420-9},
issn = {0265-203X},
month = {jun},
pages = {5804--5812},
publisher = {IEEE},
title = {{Learning to Evaluate Image Captioning}},
url = {http://arxiv.org/abs/1806.06422 https://ieeexplore.ieee.org/document/8578706/},
year = {2018}
}
@inproceedings{Mason2015,
abstract = {We present a nonparametric density esti-mation technique for image caption gener-ation. Data-driven matching methods have shown to be effective for a variety of com-plex problems in Computer Vision. These methods reduce an inference problem for an unknown image to finding an exist-ing labeled image which is semantically similar. However, related approaches for image caption generation (Ordonez et al., 2011; Kuznetsova et al., 2012) are ham-pered by noisy estimations of visual con-tent and poor alignment between images and human-written captions. Our work addresses this challenge by estimating a word frequency representation of the vi-sual content of a query image. This al-lows us to cast caption generation as an extractive summarization problem. Our model strongly outperforms two state-of-the-art caption extraction systems accord-ing to human judgments of caption rele-vance.},
address = {Stroudsburg, PA, USA},
author = {Mason, Rebecca and Charniak, Eugene},
booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics},
doi = {10.3115/v1/P14-2097},
file = {:home/mario/Documents/Mendeley/2014/Mason, Charniak - 2014 - Nonparametric Method for Data-driven Image Captioning.pdf:pdf},
pages = {592--598},
publisher = {Association for Computational Linguistics},
title = {{Nonparametric Method for Data-driven Image Captioning}},
url = {http://aclweb.org/anthology/P14-2097},
year = {2014}
}
@inproceedings{Hodosh2015,
abstract = {The ability to associate images with natural language sentences that describe what is depicted in them is a hallmark of image understanding, and a prerequisite for applications such as sentence-based image search. In analogy to image search, we propose to frame sentence-based image annotation as the task of ranking a given pool of captions. We introduce a new benchmark collection for sentence-based image description and search, consisting of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. We introduce a number of systems that perform quite well on this task, even though they are only based on features that can be obtained with minimal supervision. Our results clearly indicate the importance of training on multiple captions per image, and of capturing syntactic (word order-based) and semantic features of these captions. We also perform an in-depth comparison of human and automatic evaluation metrics for this task, and propose strategies for collecting human judgments cheaply and on a very large scale, allowing us to augment our collection with additional relevance judgments of which captions describe which image. Our analysis shows that metrics that consider the ranked list of results for each query image or sentence are significantly more robust than metrics that are based on a single response per query. Moreover, our study suggests that the evaluation of ranking-based image description systems may be fully automated.},
author = {Hodosh, Micah and Young, Peter and Hockenmaier, Julia},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
doi = {10.1613/jair.3994},
file = {:home/mario/Documents/Mendeley/2015/Hodosh, Young, Hockenmaier - 2015 - Framing image description as a ranking task Data, models and evaluation metrics.pdf:pdf},
isbn = {9781577357384},
issn = {10450823},
pages = {4188--4192},
title = {{Framing image description as a ranking task: Data, models and evaluation metrics}},
volume = {2015-Janua},
year = {2015}
}
@inproceedings{Gupta2012a,
abstract = {In this paper, we address the problem of automatically generating human-like descriptions for unseen images, given a collection of images and their corresponding human-generated descriptions. Previous attempts for this task mostly rely on visual clues and corpus statistics, but do not take much advantage of the semantic information inherent in the available image descriptions. Here, we present a generic method which benefits from all these three sources (i.e. visual clues, corpus statistics and available descriptions) simultaneously, and is capable of constructing novel descriptions. Our approach works on syntactically and linguistically motivated phrases extracted from the human descriptions. Experimental evaluations demonstrate that our formulation mostly generates lucid and semantically correct descriptions, and significantly outperforms the previous methods on automatic evaluation metrics. One of the significant advantages of our approach is that we can generate multiple interesting descriptions for an image. Unlike any previous work, we also test the applicability of our method on a large dataset containing complex images with rich descriptions.},
address = {Toronto, Ontario, Canada},
author = {Gupta, Ankush and Verma, Yashaswi and Jawahar, C.V.},
booktitle = {Proceedings of the Twenty-Sixth (AAAI) Conference on Artificial Intelligence},
file = {:home/mario/Documents/Mendeley/2012/Gupta, Verma, Jawahar - 2012 - Choosing Linguistics over Vision to Describe Images.pdf:pdf},
pages = {606--612},
publisher = {AAAI Press},
title = {{Choosing linguistics over vision to describe images}},
year = {2012}
}
@article{Bernardi2017,
abstract = {Automatic description generation from natural images is a challenging problem that has recently received a large amount of interest from the computer vision and natural language processing communities. In this survey, we classify the existing approaches based on how they conceptualize this problem, viz., models that cast description as either generation problem or as a retrieval problem over a visual or multimodal representational space. We provide a detailed review of existing models, highlighting their advantages and disadvantages. Moreover, we give an overview of the benchmark image datasets and the evaluation measures that have been developed to assess the quality of machine-generated image descriptions. Finally we extrapolate future directions in the area of automatic image description generation.},
archivePrefix = {arXiv},
arxivId = {1601.03896},
author = {Bernardi, Raffaella and Cakici, Ruket and Elliott, Desmond and Erdem, Aykut and Erdem, Erkut and Ikizler-Cinbis, Nazli and Keller, Frank and Muscat, Adrian and Plank, Barbara},
doi = {10.1613/jair.4900},
eprint = {1601.03896},
file = {:home/mario/Documents/Mendeley/2017/Bernardi et al. - 2017 - Automatic description generation from images A survey of models, datasets, and evaluation measures.pdf:pdf},
isbn = {9780999241103},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
pages = {4970--4974},
pmid = {11329188},
title = {{Automatic description generation from images: A survey of models, datasets, and evaluation measures}},
volume = {55},
year = {2017}
}
@inproceedings{Jia2015,
address = {Santiago, Chile},
author = {Jia, Xu and Gavves, Efstratios and Fernando, Basura and Tuytelaars, Tinne},
booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.277},
file = {:home/mario/Documents/Mendeley/2015/Jia et al. - 2015 - Guiding the Long-Short Term Memory Model for Image Caption Generation.pdf:pdf},
isbn = {978-1-4673-8391-2},
month = {dec},
pages = {2407--2415},
publisher = {IEEE},
title = {{Guiding the Long-Short Term Memory Model for Image Caption Generation}},
url = {http://ieeexplore.ieee.org/document/7410634/},
year = {2015}
}
@article{Zhao2019,
abstract = {Deep convolution neural networks connected with the recurrent neural networks are potent models that have achieved excellent performance on image caption task. Although many methods based on the neural network can generate fluent and complete sentences, the image feature vectors extracted by the convolution neural network can only retain a few significant features of the original image, which will lose a lot of useful image information. Moreover, RNNs have a gradient vanishing problem with the growth of RNNs time step, and the generation of sentences will lack the guidance of previous information. In this paper, we introduce a multimodal fusion method for generating descriptions to explain the content of images. Our model consists of four sub-networks: a convolutional neural network for image feature extraction, a ATTssd model for image attributes extraction, a language CNN model CNNm for sentence modeling and a recurrent network (e.g., GRU, LSTM, etc.) for word prediction. Compared with existing methods which predict next word based on one previous word and hidden state, our model uses image attributes information to enhance the image representation and handles all the previous words to modeling the long-term dependencies of history words. The methods are evaluated on the Flickr8k, Flickr30k and MSCOCO datasets. We prove that our model combined with ATTssd and CNNm can significantly enhance the performance, and achieve the competitive results.},
author = {Zhao, Dexin and Chang, Zhi and Guo, Shutao},
doi = {10.1016/j.neucom.2018.11.004},
file = {:home/mario/Documents/Mendeley/2019/Zhao, Chang, Guo - 2019 - A multimodal fusion approach for image captioning.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Deep neural network,Image caption,Multimodal fusion},
pages = {476--485},
publisher = {Elsevier B.V.},
title = {{A multimodal fusion approach for image captioning}},
url = {https://doi.org/10.1016/j.neucom.2018.11.004},
volume = {329},
year = {2019}
}
@inproceedings{Jing2018,
address = {Melbourne, Australia},
author = {Jing, Baoyu and Xie, Pengtao and Xing, Eric P},
booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
doi = {10.18653/v1/P18-1240},
file = {:home/mario/Documents/Mendeley/2018/Jing, Xie, Xing - 2018 - On the Automatic Generation of Medical Imaging Reports.pdf:pdf},
pages = {2577--2586},
title = {{On the Automatic Generation of Medical Imaging Reports}},
year = {2018}
}
@article{Yuan2019,
abstract = {It is a big challenge of computer vision to make machine automatically describe the content of an image with a natural language sentence. Previous works have made great progress on this task, but they only use the global or local image feature, which may lose some important subtle or global information of an image. In this paper, we propose a model with 3-gated model which fuses the global and local image features together for the task of image caption generation. The model mainly has three gated structures. (1) Gate for the global image feature, which can adaptively decide when and how much the global image feature should be imported into the sentence generator. (2) The gated recurrent neural network (RNN) is used as the sentence generator. (3) The gated feedback method for stacking RNN is employed to increase the capability of nonlinearity fitting. More specially, the global and local image features are combined together in this paper, which makes full use of the image information. The global image feature is controlled by the first gate and the local image feature is selected by the attention mechanism. With the latter two gates, the relationship between image and text can be well explored, which improves the performance of the language part as well as the multi-modal embedding part. Experimental results show that our proposed method outperforms the state-of-the-art for image caption generation.},
author = {Yuan, Aihong and Li, Xuelong and Lu, Xiaoqiang},
doi = {10.1016/j.neucom.2018.10.059},
file = {:home/mario/Documents/Mendeley/2019/Yuan, Li, Lu - 2019 - 3G structure for image caption generation.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Convolutional neural network,Deep learning,Image caption generation,Multi-modal learning,Recurrent neural network},
pages = {17--28},
publisher = {Elsevier B.V.},
title = {{3G structure for image caption generation}},
url = {https://doi.org/10.1016/j.neucom.2018.10.059},
volume = {330},
year = {2019}
}
@inproceedings{Vinyals2015,
abstract = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1411.4555},
author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2015.7298935},
eprint = {1411.4555},
file = {:home/mario/Documents/Mendeley/2015/Vinyals et al. - 2015 - Show and tell A neural image caption generator.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {10636919},
month = {jun},
pages = {3156--3164},
pmid = {903},
publisher = {IEEE},
title = {{Show and tell: A neural image caption generator}},
url = {http://ieeexplore.ieee.org/document/7298935/},
volume = {07-12-June},
year = {2015}
}
@article{Gupta2012,
abstract = {In this paper, we address the problem of automatically generating human-like descriptions for unseen images, given a collection of images and their corresponding human-generated descriptions. Previous attempts for this task mostly rely on visual clues and corpus statis- tics, but do not take much advantage of the semantic in- formation inherent in the available image descriptions. Here, we present a generic method which benefits from all these three sources (i.e. visual clues, corpus statis- tics and available descriptions) simultaneously, and is capable of constructing novel descriptions. Our ap- proach works on syntactically and linguistically moti- vated phrases extracted from the human descriptions. Experimental evaluations demonstrate that our formu- lation mostly generates lucid and semantically correct descriptions, and significantly outperforms the previous methods on automatic evaluation metrics. One of the significant advantages of our approach is that we can generate multiple interesting descriptions for an image. Unlike any previous work, we also test the applicabil- ity of our method on a large dataset containing complex images with rich descriptions.},
author = {Gupta, Ankush and Verma, Yashaswi and Jawahar, C. V.},
file = {:home/mario/Documents/Mendeley/2012/Gupta, Verma, Jawahar - 2012 - Choosing Linguistics over Vision to Describe Images.pdf:pdf},
isbn = {9781577355687},
journal = {Proceedings of the 26th AAAI Conference on Artificial Intelligence (AAAI'12)},
keywords = {Knowledge-Based Information Systems (Main Track)},
pages = {606--612},
title = {{Choosing Linguistics over Vision to Describe Images}},
url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/viewFile/5039/5245},
year = {2012}
}
@article{Kuznetsova2018,
abstract = {We present a new tree based approach to composing expressive image descriptions that makes use of naturally occuring web images with captions. We investigate two related tasks: image caption generalization and generation , where the former is an optional sub-task of the latter. The high-level idea of our approach is to harvest expressive phrases (as tree fragments) from existing image descriptions, then to compose a new description by selectively combining the extracted (and optionally pruned) tree fragments. Key algorithmic components are tree composition and compression , both integrating tree structure with sequence structure. Our proposed system attains significantly better performance than previous approaches for both image caption generalization and generation . In addition, our work is the first to show the empirical benefit of automatically generalized captions for composing natural image descriptions.},
author = {Kuznetsova, Polina and Ordonez, Vicente and Berg, Tamara L. and Choi, Yejin},
doi = {10.1162/tacl_a_00188},
file = {:home/mario/Documents/Mendeley/2018/Kuznetsova et al. - 2018 - TreeTalk Composition and Compression of Trees for Image Descriptions.pdf:pdf},
journal = {Transactions of the Association for Computational Linguistics},
pages = {351--362},
title = {{TreeTalk : Composition and Compression of Trees for Image Descriptions}},
volume = {2},
year = {2018}
}
@inproceedings{Elliott2015,
abstract = {The Visual Dependency Representation (VDR) is an explicit model of the spa-tial relationships between objects in an im-age. In this paper we present an approach to training a VDR Parsing Model without the extensive human supervision used in previous work. Our approach is to find the objects mentioned in a given descrip-tion using a state-of-the-art object detec-tor, and to use successful detections to pro-duce training data. The description of an unseen image is produced by first predict-ing its VDR over automatically detected objects, and then generating the text with a template-based generation model using the predicted VDR. The performance of our approach is comparable to a state-of-the-art multimodal deep neural network in images depicting actions.},
address = {Stroudsburg, PA, USA},
author = {Elliott, Desmond and de Vries, Arjen},
booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
doi = {10.3115/v1/P15-1005},
file = {:home/mario/Documents/Mendeley/2015/Elliott, de Vries - 2015 - Describing Images using Inferred Visual Dependency Representations.pdf:pdf},
pages = {42--52},
publisher = {Association for Computational Linguistics},
title = {{Describing Images using Inferred Visual Dependency Representations}},
url = {http://aclweb.org/anthology/P15-1005},
year = {2015}
}
@inproceedings{Mao2014,
abstract = {In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions. It directly models the probability distribution of generating a word given previous words and an image. Image captions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K, Flickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In addition, we apply the m-RNN model to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval. The project page of this work is: www.stat.ucla.edu/{\~{}}junhua.mao/m-RNN.html .},
archivePrefix = {arXiv},
arxivId = {1412.6632},
author = {Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Huang, Zhiheng and Yuille, Alan},
booktitle = {2015 International Conference on Learning Representations},
eprint = {1412.6632},
file = {:home/mario/Documents/Mendeley/2015/Mao et al. - 2015 - Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN).pdf:pdf},
number = {2014},
pages = {1--17},
title = {{Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)}},
url = {http://arxiv.org/abs/1412.6632},
volume = {1090},
year = {2015}
}
@inproceedings{Johnson,
abstract = {We introduce the dense captioning task, which requires a computer vision system to both localize and describe salient regions in images in natural language. The dense captioning task generalizes object detection when the descriptions consist of a single word, and Image Captioning when one predicted region covers the full image. To address the localization and description task jointly we propose a Fully Convolutional Localization Network (FCLN) architecture that processes an image with a single, efficient forward pass, requires no external regions proposals, and can be trained end-to-end with a single round of optimization. The architecture is composed of a Convolutional Network, a novel dense localization layer, and Recurrent Neural Network language model that generates the label sequences. We evaluate our network on the Visual Genome dataset, which comprises 94,000 images and 4,100,000 region-grounded captions. We observe both speed and accuracy improvements over baselines based on current state of the art approaches in both generation and retrieval settings.},
address = {Las Vegas, NV, USA},
author = {Johnson, Justin and Karpathy, Andrej and Fei-Fei, Li},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.494},
file = {:home/mario/Documents/Mendeley/2016/Johnson, Karpathy, Fei-Fei - 2016 - DenseCap Fully Convolutional Localization Networks for Dense Captioning.pdf:pdf},
isbn = {978-1-4673-8851-1},
month = {jun},
pages = {4565--4574},
publisher = {IEEE},
title = {{DenseCap: Fully Convolutional Localization Networks for Dense Captioning}},
url = {http://ieeexplore.ieee.org/document/7780863/},
year = {2016}
}
@inproceedings{Zhou2016,
abstract = {Attention mechanisms have attracted considerable interest in image captioning due to its powerful performance. However, existing methods use only visual content as attention and whether textual context can improve attention in image captioning remains unsolved. To explore this problem, we propose a novel attention mechanism, called $\backslash$textit{\{}text-conditional attention{\}}, which allows the caption generator to focus on certain image features given previously generated text. To obtain text-related image features for our attention model, we adopt the guiding Long Short-Term Memory (gLSTM) captioning architecture with CNN fine-tuning. Our proposed method allows joint learning of the image embedding, text embedding, text-conditional attention and language model with one network architecture in an end-to-end manner. We perform extensive experiments on the MS-COCO dataset. The experimental results show that our method outperforms state-of-the-art captioning methods on various quantitative metrics as well as in human evaluation, which supports the use of our text-conditional attention in image captioning.},
address = {New York, New York, USA},
archivePrefix = {arXiv},
arxivId = {1606.04621},
author = {Zhou, Luowei and Xu, Chenliang and Koch, Parker and Corso, Jason J.},
booktitle = {Proceedings of the on Thematic Workshops of ACM Multimedia 2017 - Thematic Workshops '17},
doi = {10.1145/3126686.3126717},
eprint = {1606.04621},
file = {:home/mario/Documents/Mendeley/2017/Zhou et al. - 2017 - Watch What You Just Said Image Captioning with Text-Conditional Attention.pdf:pdf},
isbn = {9781450354165},
keywords = {-  Computing methodologies  -{\textgreater}  Natural language g,Computer vision representations,Neural networks,image captioning,lstm,multi-modal embedding,neural},
month = {jun},
pages = {305--313},
publisher = {ACM Press},
title = {{Watch What You Just Said: Image Captioning with Text-Conditional Attention}},
url = {http://arxiv.org/abs/1606.04621 http://dl.acm.org/citation.cfm?doid=3126686.3126717},
year = {2017}
}
@article{Yang2019,
abstract = {Automatically generating a natural sentence describing the content of an image has been extensively researched in artificial intelligence recently, and it bridges the gap between computer vision and natural language processing communities. Most of existing captioning frameworks rely heavily on the visual content, while rarely being aware of the sentimental information. In this paper, we introduce the affective concepts to enhance the emotion expressibility of text descriptions. We achieve this goal by composing appropriate emotional concepts to sentences, which is calculated from large-scale visual and textual repositories by learning both content and linguistic modules. We extract visual and textual representations respectively, followed by combining the latent codes of the two components into a low-dimensional subspace. After that, we decode the combined latent representations and finally generate the affective image captions. We evaluate our method on the SentiCap dataset, which was established with sentimental adjective noun pairs, and evaluate the emotional descriptions with several qualitative and human inception metrics. The experimental results demonstrate the capability of our method for analyzing the latent emotion of an image and providing the affective description which caters to human cognition.},
author = {Yang, Jufeng and Sun, Yan and Liang, Jie and Ren, Bo and Lai, Shang Hong},
doi = {10.1016/j.neucom.2018.03.078},
file = {:home/mario/Documents/Mendeley/2019/Yang et al. - 2019 - Image captioning by incorporating affective concepts learned from both visual and textual components.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Affective concepts,Emotion recognition,Image captioning},
pages = {56--68},
publisher = {Elsevier B.V.},
title = {{Image captioning by incorporating affective concepts learned from both visual and textual components}},
url = {https://doi.org/10.1016/j.neucom.2018.03.078},
volume = {328},
year = {2019}
}
@inproceedings{Chen2015,
abstract = {In this paper we explore the bi-directional mapping be-tween images and their sentence-based descriptions. We propose learning this mapping using a recurrent neural net-work. Unlike previous approaches that map both sentences and images to a common embedding, we enable the gener-ation of novel sentences given an image. Using the same model, we can also reconstruct the visual features associ-ated with an image given its visual description. We use a novel recurrent visual memory that automatically learns to remember long-term visual concepts to aid in both sentence generation and visual feature reconstruction. We evaluate our approach on several tasks. These include sentence gen-eration, sentence retrieval and image retrieval. State-of-the-art results are shown for the task of generating novel im-age descriptions. When compared to human generated cap-tions, our automatically generated captions are preferred by humans over 19.8{\%} of the time. Results are better than or comparable to state-of-the-art results on the image and sentence retrieval tasks for methods using similar visual features.},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.5654v1},
author = {Chen, Xinlei and Zitnick, C Lawrence},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2015.7298856},
eprint = {arXiv:1411.5654v1},
file = {:home/mario/Documents/Mendeley/2015/Chen, Zitnick - 2015 - Mind's eye A recurrent visual representation for image caption generation.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {10636919},
month = {jun},
pages = {2422--2431},
publisher = {IEEE},
title = {{Mind's eye: A recurrent visual representation for image caption generation}},
url = {http://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2015/html/Chen{\_}Minds{\_}Eye{\_}A{\_}2015{\_}CVPR{\_}paper.html http://ieeexplore.ieee.org/document/7298856/},
year = {2015}
}
@techreport{Hossain2018,
abstract = {Generating a description of an image is called image captioning. Image captioning requires to recognize the important objects, their attributes and their relationships in an image. It also needs to generate syntactically and semantically correct sentences. Deep learning-based techniques are capable of handling the complexities and challenges of image captioning. In this survey paper, we aim to present a comprehensive review of existing deep learning-based image captioning techniques. We discuss the foundation of the techniques to analyze their performances, strengths and limitations. We also discuss the datasets and the evaluation metrics popularly used in deep learning based automatic image captioning.},
archivePrefix = {arXiv},
arxivId = {1810.04020v2},
author = {Hossain, Zakir and Sohel, Ferdous and Shiratuddin, Mohd Fairuz and Laga, Hamid},
eprint = {1810.04020v2},
file = {:home/mario/Documents/Mendeley/2018/Hossain et al. - 2018 - A Comprehensive Survey of Deep Learning for Image Captioning.pdf:pdf},
keywords = {Additional Key Words and Phrases: Image Captioning,CCS Concepts: • Computing methodologies → Machine,CNN,Computer Vision,Deep Learning,LSTM,Natural Language Processing,Neural networks,Reinforcement learning,Supervised learning,Unsupervised learning},
title = {{A Comprehensive Survey of Deep Learning for Image Captioning}},
year = {2018}
}
@incollection{Lin2014,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
archivePrefix = {arXiv},
arxivId = {1405.0312},
author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C. Lawrence},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-10602-1_48},
eprint = {1405.0312},
file = {:home/mario/Documents/Mendeley/2014/Lin et al. - 2014 - Microsoft COCO Common Objects in Context.pdf:pdf},
number = {PART 5},
pages = {740--755},
title = {{Microsoft COCO: Common Objects in Context}},
url = {https://arxiv.org/pdf/1405.0312.pdf http://link.springer.com/10.1007/978-3-319-10602-1{\_}48},
volume = {8693 LNCS},
year = {2014}
}
@article{Plummer2017,
abstract = {The Flickr30k dataset has become a standard benchmark for sentence-based image description. This paper presents Flickr30k Entities, which augments the 158k captions from Flickr30k with 244k coreference chains, linking mentions of the same entities across different captions for the same image, and associating them with 276k manually annotated bounding boxes. Such annotations are essential for continued progress in automatic image description and grounded language understanding. They enable us to define a new benchmark for localization of textual entity mentions in an image. We present a strong baseline for this task that combines an image-text embedding, detectors for common objects, a color classifier, and a bias towards selecting larger objects. While our baseline rivals in accuracy more complex state-of-the-art models, we show that its gains cannot be easily parlayed into improvements on such tasks as image-sentence retrieval, thus underlining the limitations of current methods and the need for further research.},
archivePrefix = {arXiv},
arxivId = {1505.04870},
author = {Plummer, Bryan A. and Wang, Liwei and Cervantes, Chris M. and Caicedo, Juan C. and Hockenmaier, Julia and Lazebnik, Svetlana},
doi = {10.1007/s11263-016-0965-7},
eprint = {1505.04870},
file = {:home/mario/Documents/Mendeley/2017/Plummer et al. - 2017 - Flickr30k Entities Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models.pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Computer vision,Crowdsourcing,Datasets,Language,Region phrase correspondence},
number = {1},
pages = {74--93},
title = {{Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models}},
volume = {123},
year = {2017}
}
@article{Xu2015a,
abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
archivePrefix = {arXiv},
arxivId = {1502.03044},
author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
doi = {10.1016/j.exger.2013.05.060},
eprint = {1502.03044},
file = {:home/mario/magomar@gmail.com/UOC/TFM/Bibliograf{\'{i}}a/Attention/1502.03044.pdf:pdf},
isbn = {0531-5565},
issn = {05315565},
pmid = {23770107},
title = {{Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}},
url = {http://arxiv.org/abs/1502.03044},
year = {2015}
}
@inproceedings{Lebret2014,
abstract = {Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutional Neural Network) and phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on caption syntax statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results on the recently release Microsoft COCO dataset.},
archivePrefix = {arXiv},
arxivId = {1412.8419},
author = {Lebret, Remi and Pinheiro, Pedro O. and Collobert, Ronan},
booktitle = {Proceedings of the 2015 International Conference on Learning Representations},
eprint = {1412.8419},
file = {:home/mario/Documents/Mendeley/2015/Lebret, Pinheiro, Collobert - 2015 - Simple Image Description Generator via a Linear Phrase-Based Approach.pdf:pdf},
title = {{Simple Image Description Generator via a Linear Phrase-Based Approach}},
url = {http://arxiv.org/abs/1412.8419},
year = {2015}
}
@article{Tanti2017,
abstract = {When a recurrent neural network language model is used for caption generation, the image information can be fed to the neural network either by directly incorporating it in the RNN -- conditioning the language model by `injecting' image features -- or in a layer following the RNN -- conditioning the language model by `merging' image features. While both options are attested in the literature, there is as yet no systematic comparison between the two. In this paper we empirically show that it is not especially detrimental to performance whether one architecture is used or another. The merge architecture does have practical advantages, as conditioning by merging allows the RNN's hidden state vector to shrink in size by up to four times. Our results suggest that the visual and linguistic modalities for caption generation need not be jointly encoded by the RNN as that yields large, memory-intensive models with few tangible advantages in performance; rather, the multimodal integration should be delayed to a subsequent stage.},
archivePrefix = {arXiv},
arxivId = {1703.09137},
author = {Tanti, Marc and Gatt, Albert and Camilleri, Kenneth P},
doi = {10.1017/S1351324918000098},
eprint = {1703.09137},
file = {:home/mario/Documents/Mendeley/2017/Tanti, Gatt, Camilleri - 2017 - Where to put the Image in an Image Caption Generator.pdf:pdf},
journal = {Natural Language Engineering},
month = {mar},
number = {special 3},
pages = {1--28},
title = {{Where to put the Image in an Image Caption Generator}},
url = {http://arxiv.org/abs/1703.09137 http://dx.doi.org/10.1017/S1351324918000098},
volume = {24},
year = {2017}
}
@article{Young2014,
abstract = {We propose to use the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show to be at least as beneficial as distributional similarities for two tasks that require semantic inference. To compute these denotational similarities, we construct a denotation graph, i.e. a subsump- tion hierarchy over constituents and their de- notations, based on a large corpus of 30K im- ages and 150K descriptive captions.},
author = {Young, M. H. Peter and Lai, Alice and Hockenmaier, J.},
file = {:home/mario/Documents/Mendeley/2014/Young, Lai, Hockenmaier - 2014 - From image descriptions to visual denotations New similarity metrics for semantic inference over event.pdf:pdf},
isbn = {1-58113-905-5},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics},
pages = {67--78},
title = {{From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions}},
volume = {2},
year = {2014}
}
@article{Karpathy2017,
abstract = {Convolutional neural networks (CNNs) have been extensively applied for image recognition problems giving state-of-the-art results on recognition, detection, segmentation and retrieval. In this work we propose and evaluate several deep neural network architectures to combine image information across a video over longer time periods than previously attempted. We propose two methods capable of handling full length videos. The first method explores various convolutional temporal feature pooling architectures, examining the various design choices which need to be made when adapting a CNN for this task. The second proposed method explicitly models the video as an ordered sequence of frames. For this purpose we employ a recurrent neural network that uses Long Short-Term Memory (LSTM) cells which are connected to the output of the underlying CNN. Our best networks exhibit significant performance improvements over previously published results on the Sports 1 million dataset (73.1{\%} vs. 60.9{\%}) and the UCF-101 datasets with (88.6{\%} vs. 88.0{\%}) and without additional optical flow information (82.6{\%} vs. 72.8{\%}).},
archivePrefix = {arXiv},
arxivId = {1503.08909},
author = {Karpathy, Andrej and Fei-Fei, Li},
doi = {10.1109/TPAMI.2016.2598339},
eprint = {1503.08909},
file = {:home/mario/Documents/Mendeley/2017/Karpathy, Fei-Fei - 2017 - Deep Visual-Semantic Alignments for Generating Image Descriptions.pdf:pdf},
isbn = {9781467369640},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Image captioning,deep neural networks,language model,recurrent neural network,visual-semantic embeddings},
month = {apr},
number = {4},
pages = {664--676},
pmid = {16873662},
title = {{Deep Visual-Semantic Alignments for Generating Image Descriptions}},
url = {http://arxiv.org/abs/1503.08909 http://ieeexplore.ieee.org/document/7534740/},
volume = {39},
year = {2017}
}
@article{He2019,
abstract = {Recently, attribute has demonstrated its effectiveness in guiding image captioning system. However, most attributes based image captioning methods treat the attributes prediction task as a separate task and rely on a standalone stage to obtain the attributes for the given image, e.g., a pre-trained network like Fully Convolutional Neural Network (FCN) is usually adopted. Inherently, they ignore the correlation between the attribute prediction task and image representation extraction task, and at the same time increases the complexity of the image captioning system. In this paper, we aim to couple the attributes prediction stage and image representation extraction stage tightly and propose a novel and efficient image captioning framework called Visual-Densely Semantic Attention Network(VD-SAN). In particular, the whole captioning system consists of shared convolutional layers from Dense Convolutional Network (DenseNet), which are further split into a semantic attributes prediction branch and an image feature extraction branch, two semantic attention models, and a long short-term memory networks (LSTM) for caption generation. To evaluate the proposed architecture, we construct Flickr30K-ATT and MS-COCO-ATT datasets based on the original popular image caption datasets Flickr30K and MS COCO respectively, and each image from Flickr30K-ATT or MS-COCO-ATT is annotated with an attribute list in addition to the corresponding caption. Empirical results demonstrate that our captioning system can achieve significant improvements over state-of-the-art approaches.},
author = {He, Xinwei and Yang, Yang and Shi, Baoguang and Bai, Xiang},
doi = {10.1016/j.neucom.2018.02.106},
file = {:home/mario/Documents/Mendeley/2019/He et al. - 2019 - VD-SAN Visual-Densely Semantic Attention Network for Image Caption Generation.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Convolutional neural network,Image caption,Long short-term memory networks,Semantic attributes},
pages = {48--55},
publisher = {Elsevier B.V.},
title = {{VD-SAN: Visual-Densely Semantic Attention Network for Image Caption Generation}},
url = {https://doi.org/10.1016/j.neucom.2018.02.106},
volume = {328},
year = {2019}
}
